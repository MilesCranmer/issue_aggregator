{
  "data":
  {
    "repository":
    {
      "discussion":
      {
        "number": 373,
        "title": "Custom objective questions",
        "body": "EDIT: Resolved. I figured out the reason for the difference.\r\nIn my Julia helper's code, length(prediction) was 100 (correct, because I am trying to divide by the number of points, was 100).\r\nWhile in my model's objective function, length(prediction) was 1.\r\nSo, in the model's objective function, I just changed length(prediction) to sizeof(prediction)[1].\r\n\r\nHi,\r\n\r\nWhen I run the following objective function:\r\n```\r\nobjective = \"\"\"\r\nimport Pkg\r\nPkg.add(\"Roots\")\r\nPkg.add(\"ForwardDiff\")\r\nPkg.add(\"QuadGK\")\r\n\r\nusing Roots\r\nusing ForwardDiff\r\nusing QuadGK\r\nfunction eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    MAX_THRESHOLD_ALLOWED = 10000\r\n    \r\n\r\n    prediction, flag = eval_tree_array(tree, dataset.X, options)\r\n    !flag && return L(Inf)\r\n    \r\n    num_points = length(prediction)\r\n    \r\n    for i in 1:num_points\r\n        cur_expr_val = prediction[i]\r\n        if (cur_expr_val > MAX_THRESHOLD_ALLOWED)\r\n            return L(Inf)\r\n        end\r\n        if (cur_expr_val < 0)\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY POSITIVE INFINITY VALUES\r\n    # OR NEGATIVE INFINITY VALUES ON THE DOMAIN X=[0, 1].\r\n    \r\n    \r\n    # NOTE: reshape([x], 1, 1) should give same dimensions\r\n    # as reshape([x;], 1, 1).\r\n    # f(x) = x -> 1/(eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))\r\n    f(x) = 1 / ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])\r\n    \r\n    # fp(x) = ForwardDiff.derivative(f,float(x))\r\n    fp(x) = ( -1 / ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])^2 * (eval_diff_tree_array(tree, reshape([Float32(x)], 1, 1), options, 1))[2][1])\r\n    \r\n    global found_a_recip_root = true\r\n    global recip_roots = -3\r\n    try\r\n        global recip_roots = find_zero((f, fp), 0.3, Roots.Newton(), maxevals=100)\r\n        #println(\"recip_roots is\", recip_roots)\r\n    catch\r\n        global found_a_recip_root = false\r\n        # \"Convergence Failed\"\r\n    end\r\n    \r\n    # If the function f(x) represented by the tree \"tree\"\r\n    # has a value f(x) close to positive infinity or negative infinity\r\n    # when x is in [0, 1], then return L(Inf).\r\n    if found_a_recip_root\r\n        if (recip_roots >= 0 && recip_roots <= 1)\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY NEGATIVE VALUES OR ZERO VALUES\r\n    # ON THE DOMAIN X=[0, 1].\r\n    f(x) = ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])\r\n    \r\n    fp(x) = ((eval_diff_tree_array(tree, reshape([Float32(x)], 1, 1), options, 1))[2][1])\r\n    \r\n    global found_a_root = true\r\n    global roots = -3\r\n    try\r\n        global roots = find_zero((f, fp), 0.3, Roots.Newton(), maxevals=100)\r\n        #println(\"roots is\", roots)\r\n    catch\r\n        global found_a_root = false\r\n        # \"Convergence Failed\"\r\n    end\r\n    \r\n    if found_a_root\r\n        if (roots >= 0 && roots <= 1)\r\n            println(\"found a root so returned L(Inf) and string rep of the tree was\")\r\n            println(string_tree(tree, options))\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    global integral = 1\r\n    err = -3\r\n    try\r\n        my_tuple = quadgk(x -> (eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1], 0, 1, rtol=0.01)\r\n        global integral = my_tuple[1]\r\n        global err = my_tuple[2]\r\n    catch\r\n        return L(Inf)\r\n    end\r\n    \r\n    \r\n    \r\n    norm_constant = integral\r\n    if (integral <= 0)\r\n        println(\"integral was <=0\")\r\n        return L(Inf)\r\n    end\r\n    \r\n    if (isinf(integral))\r\n        #println(\"integral was Inf\")\r\n        return L(Inf)\r\n    end\r\n    \r\n    actual_probs = (prediction) / norm_constant\r\n    \r\n    # length(actual_probs) equals length(prediction)\r\n    \r\n    # We really care about the EXPECTED VALUE OF L, where L = -1 * sum(log.(actual_probs)) / (length(prediction))\r\n    # given that dataset.X is generated by the true probability distribution.\r\n    # E(L) is really equal to the definite integral from x=0 to x=1 of ln(guessed_f(x)) * f_true(x) dx.\r\n    # As num_points goes to +infinity, the variance of L becomes 0.\r\n    \r\n    final_loss = exp(-1 * sum(log.(actual_probs)) / (length(prediction)))\r\n    #println(\"final_loss is \", final_loss)\r\n    return final_loss\r\nend\r\n\"\"\"\r\n```\r\n\r\nand then I define the PySR model as:\r\n```\r\nmodel = PySRRegressor(\r\n    niterations=20,  # < Increase me for better results\r\n    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\r\n    full_objective=objective,\r\n    enable_autodiff=True,\r\n    early_stop_condition = \"f(loss, complexity) = (loss < 0.1)\"\r\n)\r\n```\r\n\r\nI define the dataset in Python as:\r\n```\r\nimport numpy as np\r\nX = np.random.rand(100, 1)\r\nprint(X.shape)\r\nnum_points = X.shape[0] # num_points is 100\r\ny = 1000 * np.ones(num_points)\r\n```\r\n\r\nThen I run\r\n```\r\nmodel.fit(X, y)\r\n```\r\n(I do all this in a Python shell inside the MacBook terminal),\r\nthe final value of models has a equations_ of 9 equations (so 9 rows).\r\nThe model.equations_[8, :] (so the last equation found by the model \"model\") is:\r\ncomplexity                                                      17\r\nloss                                                       0.00174\r\nscore                                                     1.872243\r\nequation         '(((1.4335294 / (x0 + 1.6759317)) + 0.6227007) / (0.21702437 - ((-1.312221 - (-0.2316349 / x0)) - x0)))\r\nsympy_format     (0.6227007 + 1.4335294/(x0 + 1.6759317))/(x0 +...\r\nlambda_format    PySRFunction(X=>(0.6227007 + 1.4335294/(x0 + 1...\r\nName: 8, dtype: object\r\n\r\nHowever, when I run this exact equation using a Julia helper object (\"jl\"), I get a infinity value for the loss.\r\nSpecifically, I use the \"jl\" julia helper and I run:\r\n```\r\njl.eval(\"\"\"\r\n# YOU CAN USE THIS CODE BLOCK TO TEST OUT THE VALUE OF THE CUSTOM OBJECTIVE FUNCTION\r\n\r\nfunction eval_loss_real(tree, dataset, options)\r\n    MAX_THRESHOLD_ALLOWED = 10000\r\n    \r\n\r\n    prediction, flag = eval_tree_array(tree, dataset, options)\r\n    !flag && return L(Inf)\r\n    \r\n    num_points = length(prediction)\r\n    \r\n    for i in 1:num_points\r\n        cur_expr_val = prediction[i]\r\n        if (cur_expr_val > MAX_THRESHOLD_ALLOWED)\r\n            println(\"prediction was greater than MAX_THRESHOLD_ALLOWED\")\r\n            return (Inf)\r\n        end\r\n        if (cur_expr_val < 0)\r\n            println(\"prediction was less than 0\")\r\n            return (Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY POSITIVE INFINITY VALUES\r\n    # OR NEGATIVE INFINITY VALUES ON THE DOMAIN X=[0, 1].\r\n    \r\n    \r\n    # NOTE: reshape([x], 1, 1) should give same dimensions\r\n    # as reshape([x;], 1, 1).\r\n    # f(x) = x -> 1/(eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))\r\n    f(x) = 1 / ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])\r\n    \r\n    # fp(x) = ForwardDiff.derivative(f,float(x))\r\n    fp(x) = ( -1 / ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])^2 * (eval_diff_tree_array(tree, reshape([Float32(x)], 1, 1), options, 1))[2][1])\r\n    \r\n    global found_a_recip_root = true\r\n    global recip_roots = -3\r\n    try\r\n        global recip_roots = find_zero((f, fp), 0.3, Roots.Newton(), maxevals=100)\r\n        #println(\"recip_roots is\", recip_roots)\r\n    catch\r\n        global found_a_recip_root = false\r\n        # \"Convergence Failed\"\r\n    end\r\n    \r\n    # If the function f(x) represented by the tree \"tree\"\r\n    # has a value f(x) close to positive infinity or negative infinity\r\n    # when x is in [0, 1], then return L(Inf).\r\n    if found_a_recip_root\r\n        if (recip_roots >= 0 && recip_roots <= 1)\r\n            return (Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY NEGATIVE VALUES OR ZERO VALUES\r\n    # ON THE DOMAIN X=[0, 1].\r\n    f(x) = ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])\r\n    \r\n    fp(x) = ((eval_diff_tree_array(tree, reshape([Float32(x)], 1, 1), options, 1))[2][1])\r\n    \r\n    global found_a_root = true\r\n    global roots = -3\r\n    try\r\n        global roots = find_zero((f, fp), 0.3, Roots.Newton(), maxevals=100)\r\n        #println(\"roots is\", roots)\r\n    catch\r\n        global found_a_root = false\r\n        # \"Convergence Failed\"\r\n    end\r\n    \r\n    if found_a_root\r\n        if (roots >= 0 && roots <= 1)\r\n            println(\"found a root so returned L(Inf) and string rep of the tree was\")\r\n            println(string_tree(tree, options))\r\n            return (Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    global integral = 1\r\n    err = -3\r\n    try\r\n        my_tuple = quadgk(x -> (eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1], 0, 1, rtol=0.01)\r\n        global integral = my_tuple[1]\r\n        global err = my_tuple[2]\r\n    catch\r\n        return (Inf)\r\n    end\r\n    \r\n    \r\n    \r\n    norm_constant = integral\r\n    if (integral <= 0)\r\n        println(\"integral was <=0\")\r\n        return (Inf)\r\n    end\r\n    \r\n    if (isinf(integral))\r\n        println(\"integral was Inf\")\r\n        return (Inf)\r\n    end\r\n    \r\n    actual_probs = (prediction) / norm_constant\r\n    \r\n    # length(actual_probs) equals length(prediction)\r\n    \r\n    # We really care about the EXPECTED VALUE OF L, where L = -1 * sum(log.(actual_probs)) / (length(prediction))\r\n    # given that dataset.X is generated by the true probability distribution.\r\n    # E(L) is really equal to the definite integral from x=0 to x=1 of ln(guessed_f(x)) * f_true(x) dx.\r\n    # As num_points goes to +infinity, the variance of L becomes 0.\r\n    \r\n    final_loss = exp(-1 * sum(log.(actual_probs)) / (length(prediction)))\r\n    #println(\"final_loss is \", final_loss)\r\n    return final_loss\r\nend\r\n\r\noptions = SymbolicRegression.Options(\r\n    binary_operators=[+, *, /, -, ^],\r\n    unary_operators=[cos, sin],\r\n    enable_autodiff=true\r\n)\r\n\r\nnum_points = 100\r\n\r\nx = rand(Beta(3, 2), num_points)\r\nx = reshape(x, 1, num_points)\r\nx = Float32.(x)\r\n\r\nx1 = Node(;feature=1)\r\n\r\n#tree = -0.828202055*(x1*x1) + x1 - 0.14140837\r\n\r\n# tree = x1 + x1\r\n# tree = x1\r\n\r\n# Final_loss was 0.67114264 (when I allowed it to not return Infinity, and used a norm_constant of 1).\r\n#tree = -0.094332316551433*x1 + 1.7470982*x1/(1.67670228876569*x1 - 0.282760834105208)\r\n\r\n# On model.equations_, the loss was 0.001739\r\ntree = 0.21702437*(0.6227007 + 1.4335294/(x1 + 1.6759317))/(x1 + 1.52924537 - 0.2316349/x1)\r\n\r\ntree = Node{Float32}(tree)\r\n\r\nprintln(string_tree(tree, options))\r\n\r\nmy_loss = eval_loss_real(tree, x, options)\r\nprintln(\"FINAL LOSS was \", my_loss)\r\n\"\"\")\r\n\r\n```\r\nThe output of running this jl.eval() was:\r\n```\r\nWARNING: Method definition (::Main.var\"#f#52\"{tree, options})(Any) in module Main at none:26 overwritten at none:69.\r\nWARNING: Method definition (::Main.var\"#fp#53\"{tree, options})(Any) in module Main at none:29 overwritten at none:71.\r\nWARNING: Method definition (::Main.var\"#f#56\"{tree, options})(Any) in module Main at none:169 overwritten at none:196.\r\nWARNING: Method definition (::Main.var\"#fp#57\"{tree, options})(Any) in module Main at none:172 overwritten at none:198.\r\n((0.21702437 * (0.6227007 + (1.4335294 / (x1 + 1.6759317)))) / ((x1 + 1.5292454) - (0.2316349 / x1)))\r\nprediction was less than 0\r\nFINAL LOSS was Inf\r\n```\r\n\r\nAnd it's not just this one time that I ran model.fit(X, y) that I noticed a difference between the model's reported loss of the model's last-found function and the usually-more correct outputted value of passing the model's last-found function into a Julia objective function manually and manually running the jl.eval() of that Julia code.\r\n\r\nAnother question: What would \"WARNING: Method definition (::Main.var\"#f#52\"{tree, options})(Any) in module Main at none:26 overwritten at none:69.\" mean, what are its negative consequences, and how do I change my code to avoid this warning message?\r\n",
        "comments":
        {
          "nodes":
          [
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "You can get the number of rows from `dataset.n`. Alternatively using `size(dataset.X, 2)`. You don’t want to use `length` because it will count all elements if there are multiple features.",
              "createdAt": "2023-07-08T21:28:48Z"
            },
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "With so much Julia customization, it might be easier to work on this directly from Julia? i.e., with SymbolicRegression.jl?",
              "createdAt": "2023-07-08T22:28:24Z"
            }
          ],
          "pageInfo":
          {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wNy0wOFQyMzoyODoyNCswMTowMM4AYY6t"
          }
        }
      }
    }
  }
}