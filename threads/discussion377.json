{
  "data":
  {
    "repository":
    {
      "discussion":
      {
        "number": 377,
        "title": "Why direct run of Julia code gives different output than loss value reported by model.equations_",
        "body": "A direct run of the objective function by the Julia Helper gave this answer:\r\n```\r\nMatrix{Float64}\r\n(1, 100)\r\nVector{Float64}\r\n(100,)\r\nmy_loss0.9999999999999999\r\n```\r\nwhen given this code:\r\n```\r\njl.eval(\"\"\"\r\n\r\nfunction process_eval_tree(tree, x, options, take_recip)\r\n    #println(\"RUN EVAL x=\", x)\r\n\r\n    #x_float = Float32(x)\r\n    #println(\"string_tree(tree, options) is\", string_tree(tree, options))\r\n    func_val, func_complete = (eval_tree_array(tree, reshape([x], 1, 1), options))\r\n    #println(\"func_val is \", func_val)\r\n    #println(\"func_complete is \", func_complete)\r\n    \r\n    if !func_complete\r\n       return error(\"Evaluation of tree failed.\")\r\n    end\r\n    \r\n    if take_recip && func_val[1] == 0\r\n        return Inf\r\n    end\r\n    \r\n    if take_recip\r\n        return (1/func_val[1])\r\n    end\r\n    return func_val[1]\r\nend\r\n\r\nfunction process_eval_diff_tree(tree, x, options, take_recip)\r\n    #x_float = Float32(x)\r\n    \r\n    #println(\"RUN DERIV x=\", x)\r\n    \r\n    deriv_val, gradient, deriv_complete = (eval_diff_tree_array(tree, reshape([x], 1, 1), options, 1))\r\n    if !deriv_complete\r\n       return error(\"Evaluation of tree's derivative failed.\")\r\n    end\r\n    \r\n    #println(\"typeof and size of gradient are\")\r\n    #println(typeof(gradient))\r\n    #println(size(gradient))\r\n    \r\n    if isinf(gradient[1])\r\n        return Inf\r\n    end\r\n    \r\n    if take_recip\r\n        recip_func_val = process_eval_tree(tree, x, options, take_recip)\r\n        #println(\"recip func_val is\", recip_func_val)\r\n        #println(\"gradient[1] is\", gradient[1])\r\n        if isinf(recip_func_val)\r\n            return Inf\r\n        end\r\n        return ( -1 * (recip_func_val * recip_func_val) * (gradient[1]))\r\n    end\r\n    \r\n    return gradient[1]\r\nend\r\n\r\nfunction eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    MAX_THRESHOLD_ALLOWED = 10000\r\n\r\n    x_init_1 = Float64(0.99)\r\n    x_init_2 = Float64(0.01)\r\n    \r\n    MAX_EVALS = 30\r\n    \r\n    tree = Node{Float64}(tree)\r\n    dataset_X = Float64.(dataset.X)\r\n    \r\n    # size(dataset.X) is (100, 1)\r\n    # println(\"size(x) is \", size(dataset.X))\r\n\r\n    prediction, flag = eval_tree_array(tree, dataset_X, options)\r\n    !flag && return L(Inf)\r\n    \r\n    # size(prediction) is (100,)\r\n    #println(\"size(prediction) is \", size(prediction))\r\n    \r\n    # num_points is 100\r\n    num_points = size(prediction)[1]\r\n    \r\n    # println(\"num_points is \", num_points)\r\n    \r\n    for i in 1:num_points\r\n        cur_expr_val = prediction[i]\r\n        if (cur_expr_val > MAX_THRESHOLD_ALLOWED)\r\n            return L(Inf)\r\n        end\r\n        if (cur_expr_val < 0)\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY POSITIVE INFINITY VALUES\r\n    # OR NEGATIVE INFINITY VALUES ON THE DOMAIN X=[0, 1].\r\n    \r\n    \r\n    # NOTE: reshape([x], 1, 1) should give same dimensions\r\n    # as reshape([x;], 1, 1).\r\n    # f(x) = x -> 1/(eval_tree_array(tree, reshape([x], 1, 1), options))\r\n    f_recip(x) = process_eval_tree(tree, x, options, true)\r\n\r\n    fp_recip(x) = process_eval_diff_tree(tree, x, options, true)\r\n   \r\n    global found_a_recip_root = false\r\n    global recip_roots = -3\r\n    global recip_roots_2 = -4\r\n    try\r\n        #global recip_roots = find_zero((f_recip, fp_recip), Float64(0.3), Roots.Newton(), maxevals=10)\r\n        global recip_roots = find_zero(f_recip, x_init_1, Order1(), maxevals=MAX_EVALS)\r\n        global found_a_recip_root = true\r\n    catch\r\n        my_var = 1\r\n    end\r\n    \r\n    try\r\n        #global recip_roots_2 = find_zero((f_recip, fp_recip), Float64(0.01), Roots.Newton(), maxevals=10)\r\n        global recip_roots_2 = find_zero(f_recip, x_init_2, Order1(), maxevals=MAX_EVALS)\r\n        global found_a_recip_root = true\r\n    catch\r\n        my_var = 1\r\n    end\r\n\r\n    \r\n    # If the function f(x) represented by the tree \"tree\"\r\n    # has a value f(x) close to positive infinity or negative infinity\r\n    # when x is in [0, 1], then return L(Inf).\r\n    if found_a_recip_root\r\n        if (recip_roots >= 0 && recip_roots <= 1)\r\n            return L(Inf)\r\n        end\r\n        if (recip_roots_2 >= 0 && recip_roots_2 <= 1)\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY NEGATIVE VALUES OR ZERO VALUES\r\n    # ON THE DOMAIN X=[0, 1].\r\n    f(x) = process_eval_tree(tree, x, options, false)\r\n    \r\n    fp(x) = process_eval_diff_tree(tree, x, options, false)\r\n    \r\n    global found_a_root = false\r\n    global roots = -5\r\n    global roots_2 = -6\r\n    try\r\n        #global roots = find_zero((f, fp), Float64(0.3), Roots.Newton(), maxevals=100)\r\n        global roots = find_zero(f, x_init_1, Order1(), maxevals=MAX_EVALS)\r\n        global found_a_root = true\r\n        #println(\"roots is\", roots)\r\n    catch\r\n        my_var = 1\r\n        # \"Convergence Failed\"\r\n    end\r\n\r\n    try\r\n        #global roots = find_zero((f, fp), Float64(0.3), Roots.Newton(), maxevals=100)\r\n        global roots_2 = find_zero(f, x_init_2, Order1(), maxevals=MAX_EVALS)\r\n        global found_a_root = true\r\n        #println(\"roots is\", roots)\r\n    catch\r\n        my_var = 1\r\n        # \"Convergence Failed\"\r\n    end\r\n    \r\n    if found_a_root\r\n        if (roots >= 0 && roots <= 1)\r\n            #println(\"found a root so returned L(Inf) and string rep of the tree was\")\r\n            #println(string_tree(tree, options))\r\n            return L(Inf)\r\n        end\r\n        if (roots_2 >= 0 && roots_2 <= 1)\r\n            #println(\"found a root so returned L(Inf) and string rep of the tree was\")\r\n            #println(string_tree(tree, options))\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    global integral = 1\r\n    err = -3\r\n    try\r\n        my_tuple = quadgk(x -> begin\r\n       output, completion = (eval_tree_array(tree, reshape([x], 1, 1), options))\r\n       if !completion\r\n           error(\"Integration failed.\")\r\n       end\r\n       output[1]\r\n    end, Float64(0), Float64(1), rtol=0.1)\r\n        global integral = my_tuple[1]\r\n        global err = my_tuple[2]\r\n    catch\r\n        return L(Inf)\r\n    end\r\n    \r\n    \r\n    \r\n    norm_constant = integral\r\n    \r\n    if (integral <= 0)\r\n        #println(\"integral was <=0\")\r\n        return L(Inf)\r\n    end\r\n    \r\n    if (isinf(integral))\r\n        #println(\"integral was Inf\")\r\n        return L(Inf)\r\n    end\r\n    \r\n    actual_probs = (prediction) / norm_constant\r\n    \r\n    # length(actual_probs) equals length(prediction)\r\n    \r\n    # We really care about the EXPECTED VALUE OF L, where L = -1 * sum(log.(actual_probs)) / (length(prediction))\r\n    # given that dataset.X is generated by the true probability distribution.\r\n    # E(L) is really equal to the definite integral from x=0 to x=1 of ln(guessed_f(x)) * f_true(x) dx.\r\n    # As num_points goes to +infinity, the variance of L becomes 0.\r\n    \r\n    # println(\"num_points is \", num_points)\r\n    \r\n    final_loss = exp(-1 * sum(log.(actual_probs)) / (num_points))\r\n    if final_loss < 0.1\r\n        println(\"string rep of tree is \", string_tree(tree, options))\r\n        println(\"norm_constant is \", norm_constant)\r\n        println(\"final_loss is \", final_loss)\r\n    end\r\n    return final_loss\r\nend\r\n\r\noptions = SymbolicRegression.Options(\r\n    binary_operators=[+, *, /, -, ^],\r\n    unary_operators=[cos, sin],\r\n    enable_autodiff=true\r\n)\r\n\r\nnum_points = 100\r\n\r\nX = rand(Beta(3, 2), num_points)\r\nX = reshape(X, 1, num_points)\r\n# X = Float64.(X)\r\n#X = Float32.(X)\r\nprintln(typeof(X))\r\nprintln(size(X))\r\n\r\ny = 1000. * ones(num_points)\r\n# y = Float64.(y)\r\n#y = Float32.(y)\r\nprintln(typeof(y))\r\nprintln(size(y))\r\n\r\n\r\nmy_dataset = Dataset(X, y)\r\n\r\nx1 = Node(;feature=1)\r\n\r\ntree = ((x1 + x1) / (0.19358382 * x1))\r\n\r\nmy_loss = eval_loss(tree, my_dataset, options)\r\nprintln(\"my_loss\", my_loss)\r\n\"\"\")\r\n\r\n```\r\n\r\nHowever, the exact same code (this time, I made sure to copy the entire objective function inside the variable named \"objective\", when run by model.fit(X, y), results in a model.equations_ that reports a loss value of 0.084261 for the function \"((x0 + x0) / (0.19358382 * x0))\r\n\".\r\n\r\nmodel.equations_ does not report the lowest-found loss value after calling eval_loss multiple times, right?\r\n\r\n```\r\nfrom pysr import PySRRegressor\r\n\r\n\r\nobjective = \"\"\"\r\nimport Pkg\r\nPkg.add(\"Roots\")\r\nPkg.add(\"ForwardDiff\")\r\nPkg.add(\"QuadGK\")\r\n\r\nusing Roots\r\nusing ForwardDiff\r\nusing QuadGK\r\n\r\nfunction process_eval_tree(tree, x, options, take_recip)\r\n    #println(\"RUN EVAL x=\", x)\r\n\r\n    #x_float = Float32(x)\r\n    #println(\"string_tree(tree, options) is\", string_tree(tree, options))\r\n    func_val, func_complete = (eval_tree_array(tree, reshape([x], 1, 1), options))\r\n    #println(\"func_val is \", func_val)\r\n    #println(\"func_complete is \", func_complete)\r\n    \r\n    if !func_complete\r\n       return error(\"Evaluation of tree failed.\")\r\n    end\r\n    \r\n    if take_recip && func_val[1] == 0\r\n        return Inf\r\n    end\r\n    \r\n    if take_recip\r\n        return (1/func_val[1])\r\n    end\r\n    return func_val[1]\r\nend\r\n\r\nfunction process_eval_diff_tree(tree, x, options, take_recip)\r\n    #x_float = Float32(x)\r\n    \r\n    #println(\"RUN DERIV x=\", x)\r\n    \r\n    deriv_val, gradient, deriv_complete = (eval_diff_tree_array(tree, reshape([x], 1, 1), options, 1))\r\n    if !deriv_complete\r\n       return error(\"Evaluation of tree's derivative failed.\")\r\n    end\r\n    \r\n    #println(\"typeof and size of gradient are\")\r\n    #println(typeof(gradient))\r\n    #println(size(gradient))\r\n    \r\n    if isinf(gradient[1])\r\n        return Inf\r\n    end\r\n    \r\n    if take_recip\r\n        recip_func_val = process_eval_tree(tree, x, options, take_recip)\r\n        #println(\"recip func_val is\", recip_func_val)\r\n        #println(\"gradient[1] is\", gradient[1])\r\n        if isinf(recip_func_val)\r\n            return Inf\r\n        end\r\n        return ( -1 * (recip_func_val * recip_func_val) * (gradient[1]))\r\n    end\r\n    \r\n    return gradient[1]\r\nend\r\n\r\nfunction eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    MAX_THRESHOLD_ALLOWED = 10000\r\n\r\n    x_init_1 = Float64(0.99)\r\n    x_init_2 = Float64(0.01)\r\n    \r\n    MAX_EVALS = 30\r\n    \r\n    tree = Node{Float64}(tree)\r\n    dataset_X = Float64.(dataset.X)\r\n    \r\n    # size(dataset.X) is (100, 1)\r\n    # println(\"size(x) is \", size(dataset.X))\r\n\r\n    prediction, flag = eval_tree_array(tree, dataset_X, options)\r\n    !flag && return L(Inf)\r\n    \r\n    # size(prediction) is (100,)\r\n    #println(\"size(prediction) is \", size(prediction))\r\n    \r\n    # num_points is 100\r\n    num_points = size(prediction)[1]\r\n    \r\n    # println(\"num_points is \", num_points)\r\n    \r\n    for i in 1:num_points\r\n        cur_expr_val = prediction[i]\r\n        if (cur_expr_val > MAX_THRESHOLD_ALLOWED)\r\n            return L(Inf)\r\n        end\r\n        if (cur_expr_val < 0)\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY POSITIVE INFINITY VALUES\r\n    # OR NEGATIVE INFINITY VALUES ON THE DOMAIN X=[0, 1].\r\n    \r\n    \r\n    # NOTE: reshape([x], 1, 1) should give same dimensions\r\n    # as reshape([x;], 1, 1).\r\n    # f(x) = x -> 1/(eval_tree_array(tree, reshape([x], 1, 1), options))\r\n    f_recip(x) = process_eval_tree(tree, x, options, true)\r\n\r\n    fp_recip(x) = process_eval_diff_tree(tree, x, options, true)\r\n   \r\n    global found_a_recip_root = false\r\n    global recip_roots = -3\r\n    global recip_roots_2 = -4\r\n    try\r\n        #global recip_roots = find_zero((f_recip, fp_recip), Float64(0.3), Roots.Newton(), maxevals=10)\r\n        global recip_roots = find_zero(f_recip, x_init_1, Order1(), maxevals=MAX_EVALS)\r\n        global found_a_recip_root = true\r\n    catch\r\n        my_var = 1\r\n    end\r\n    \r\n    try\r\n        #global recip_roots_2 = find_zero((f_recip, fp_recip), Float64(0.01), Roots.Newton(), maxevals=10)\r\n        global recip_roots_2 = find_zero(f_recip, x_init_2, Order1(), maxevals=MAX_EVALS)\r\n        global found_a_recip_root = true\r\n    catch\r\n        my_var = 1\r\n    end\r\n\r\n    \r\n    # If the function f(x) represented by the tree \"tree\"\r\n    # has a value f(x) close to positive infinity or negative infinity\r\n    # when x is in [0, 1], then return L(Inf).\r\n    if found_a_recip_root\r\n        if (recip_roots >= 0 && recip_roots <= 1)\r\n            return L(Inf)\r\n        end\r\n        if (recip_roots_2 >= 0 && recip_roots_2 <= 1)\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY NEGATIVE VALUES OR ZERO VALUES\r\n    # ON THE DOMAIN X=[0, 1].\r\n    f(x) = process_eval_tree(tree, x, options, false)\r\n    \r\n    fp(x) = process_eval_diff_tree(tree, x, options, false)\r\n    \r\n    global found_a_root = false\r\n    global roots = -5\r\n    global roots_2 = -6\r\n    try\r\n        #global roots = find_zero((f, fp), Float64(0.3), Roots.Newton(), maxevals=100)\r\n        global roots = find_zero(f, x_init_1, Order1(), maxevals=MAX_EVALS)\r\n        global found_a_root = true\r\n        #println(\"roots is\", roots)\r\n    catch\r\n        my_var = 1\r\n        # \"Convergence Failed\"\r\n    end\r\n\r\n    try\r\n        #global roots = find_zero((f, fp), Float64(0.3), Roots.Newton(), maxevals=100)\r\n        global roots_2 = find_zero(f, x_init_2, Order1(), maxevals=MAX_EVALS)\r\n        global found_a_root = true\r\n        #println(\"roots is\", roots)\r\n    catch\r\n        my_var = 1\r\n        # \"Convergence Failed\"\r\n    end\r\n    \r\n    if found_a_root\r\n        if (roots >= 0 && roots <= 1)\r\n            #println(\"found a root so returned L(Inf) and string rep of the tree was\")\r\n            #println(string_tree(tree, options))\r\n            return L(Inf)\r\n        end\r\n        if (roots_2 >= 0 && roots_2 <= 1)\r\n            #println(\"found a root so returned L(Inf) and string rep of the tree was\")\r\n            #println(string_tree(tree, options))\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    global integral = 1\r\n    err = -3\r\n    try\r\n        my_tuple = quadgk(x -> begin\r\n       output, completion = (eval_tree_array(tree, reshape([x], 1, 1), options))\r\n       if !completion\r\n           error(\"Integration failed.\")\r\n       end\r\n       output[1]\r\n    end, Float64(0), Float64(1), rtol=0.1)\r\n        global integral = my_tuple[1]\r\n        global err = my_tuple[2]\r\n    catch\r\n        return L(Inf)\r\n    end\r\n    \r\n    \r\n    \r\n    norm_constant = integral\r\n    \r\n    if (integral <= 0)\r\n        #println(\"integral was <=0\")\r\n        return L(Inf)\r\n    end\r\n    \r\n    if (isinf(integral))\r\n        #println(\"integral was Inf\")\r\n        return L(Inf)\r\n    end\r\n    \r\n    actual_probs = (prediction) / norm_constant\r\n    \r\n    # length(actual_probs) equals length(prediction)\r\n    \r\n    # We really care about the EXPECTED VALUE OF L, where L = -1 * sum(log.(actual_probs)) / (length(prediction))\r\n    # given that dataset.X is generated by the true probability distribution.\r\n    # E(L) is really equal to the definite integral from x=0 to x=1 of ln(guessed_f(x)) * f_true(x) dx.\r\n    # As num_points goes to +infinity, the variance of L becomes 0.\r\n    \r\n    # println(\"num_points is \", num_points)\r\n    \r\n    final_loss = exp(-1 * sum(log.(actual_probs)) / (num_points))\r\n    if final_loss < 0.1\r\n        println(\"string rep of tree is \", string_tree(tree, options))\r\n        println(\"norm_constant is \", norm_constant)\r\n        println(\"final_loss is \", final_loss)\r\n    end\r\n    return final_loss\r\nend\r\n\"\"\"\r\n\r\n\r\nmodel = PySRRegressor(\r\n    niterations=20,  # < Increase me for better results\r\n    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\r\n    full_objective=objective,\r\n    enable_autodiff=True,\r\n    early_stop_condition = \"f(loss, complexity) = (loss < 0.1)\",\r\n    progress=True\r\n)\r\n\r\nimport numpy as np\r\nnum_points = 100\r\n# (1, 100) versus (100, 1)\r\n# I think the intended shape was (1, 100), right?\r\nX = np.random.beta(3, 2, (num_points, 1))\r\n\r\nprint(X.shape)\r\n#num_points = X.shape[0] # num_points is 100\r\n\r\ny = 1000 * np.ones(num_points)\r\n\r\nprint(X)\r\n\r\nmodel.fit(X, y)\r\n\r\nprint(model.equations_)\r\n```\r\nWhy would this difference in output happen between the loss value printed by the Julia helper \"jl\" object and the loss value reported by model.equations_ ?\r\n\r\nFeel free to use https://www.diffchecker.com/ so you can quickly understand the very small differences between the Julia helper code and the objective function used by the model \"model\".\r\n\r\n\r\n",
        "comments":
        {
          "nodes":
          [
            {
              "author":
              {
                "login": "unary-code"
              },
              "body": "Another example:\r\n\r\nmodel.equations_.iloc[5, :] is:\r\n```\r\ni= 5 th equation found\r\ncomplexity          11\r\nloss          0.260501\r\nscore         0.584707\r\nName: 5, dtype: object\r\n(((x0 / 0.2732506) - x0) + (0.2732506 - (x0 - 0.11180633)))\r\nPySRFunction(X=>1.65964429721289*x0 + 0.38505693)\r\n```\r\n\r\nOutput of the Julia Helper object jl's code is:\r\n```\r\nMatrix{Float32}\r\n(1, 100)\r\nVector{Float32}\r\n(100,)\r\nmy_loss 0.87469167\r\n```\r\nwhere the Julia Helper object jl's code (using the Float32 function 1.65964429721289*x0 + 0.38505693) is:\r\n```\r\noptions = SymbolicRegression.Options(\r\n    binary_operators=[+, *, /, -, ^],\r\n    unary_operators=[cos, sin],\r\n    enable_autodiff=true\r\n)\r\n\r\nnum_points = 100\r\n\r\nX = rand(Beta(3, 2), num_points)\r\nX = reshape(X, 1, num_points)\r\n# X = Float64.(X)\r\nX = Float32.(X)\r\nprintln(typeof(X))\r\nprintln(size(X))\r\n\r\ny = 1000. * ones(num_points)\r\n# y = Float64.(y)\r\ny = Float32.(y)\r\nprintln(typeof(y))\r\nprintln(size(y))\r\n\r\n\r\nmy_dataset = Dataset(X, y)\r\n\r\nx1 = Node(;feature=1)\r\n\r\n#tree = ((x1 + x1) / (0.19358382 * x1))\r\n\r\n#tree = (-69.8075844683988*x1 - 0.6917331)/(-1*x1 - 0.93741227) + 0.021739371\r\ntree = 1.65964429721289*x1 + 0.38505693\r\ntree = Node{Float32}(tree)\r\n\r\nmy_loss = eval_loss(tree, my_dataset, options)\r\nprintln(\"my_loss\", my_loss)\r\n\r\n```\r\n\r\nEdit: My Julia Helper object also prints out a accurate loss value of:\r\n```\r\nMatrix{Float32}\r\n(1, 100)\r\nVector{Float32}\r\n(100,)\r\nmy_loss0.89749175\r\n```\r\nwhen I use\r\n```\r\ntree = (((x1 / 0.2732506) - x1) + (0.2732506 - (x1 - 0.11180633)))\r\n```\r\ninstead of the simplified format\r\n```\r\ntree = 1.65964429721289*x1 + 0.38505693\r\n```",
              "createdAt": "2023-07-10T22:51:20Z"
            },
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "Hi @unary-code,\r\n\r\nCould you summarize the question/issue? I don't quite follow.\r\n\r\nThanks,\r\nMiles",
              "createdAt": "2023-07-11T00:02:54Z"
            },
            {
              "author":
              {
                "login": "unary-code"
              },
              "body": "> Oh. It’s because you aren’t passing the loss function to the Julia version. So it’s just doing MSE.\r\n\r\nCould you explain more? You mean to the Julia helper object \"jl\" ?",
              "createdAt": "2023-07-11T01:11:00Z"
            },
            {
              "author":
              {
                "login": "unary-code"
              },
              "body": "Interesting:\r\n\r\nI believe that when I ran:\r\n```\r\njl.eval(\"\"\"\r\n    options = SymbolicRegression.Options(;\r\n    binary_operators=[+, *, /, -, ^],\r\n    unary_operators=[cos, sin],\r\n    enable_autodiff=true\r\n    )\r\n    x1 = Node(;feature=1)\r\n\r\n    tree = (((x1 - (x1 * x1)) / (((-0.37249994 + 0.40882915) / x1) - (0.0046278937 * x1))) + (0.0046278937 * 0.28049845))\r\n\r\n    err = -3\r\n    integral = 1\r\n    err = -3\r\n    try\r\n        my_tuple = quadgk(x -> begin\r\n       output, completion = (eval_tree_array(tree, reshape([x], 1, 1), options))\r\n       if !completion\r\n           return L(Inf)\r\n       end\r\n       output[1]\r\n    end, Float64(0), Float64(1), rtol=0.001)\r\n        integral = my_tuple[1]\r\n         err = my_tuple[2]\r\n    catch\r\n        return L(Inf)\r\n    end\r\n\r\n    println(\"integral is\", integral)\r\n    println(\"err is\", err)\r\n    \"\"\")\r\n```\r\n\r\nI got this error:\r\n```\r\n┌ Warning: Assignment to `integral` in soft scope is ambiguous because a global variable by the same name exists: `integral` will be treated as a new local. Disambiguate by using `local integral` to suppress this warning or `global integral` to assign to the existing global variable.\r\n└ @ none:22\r\n┌ Warning: Assignment to `err` in soft scope is ambiguous because a global variable by the same name exists: `err` will be treated as a new local. Disambiguate by using `local err` to suppress this warning or `global err` to assign to the existing global variable.\r\n└ @ none:23\r\n\r\n```\r\n\r\nand this was printed out:\r\n```\r\nintegral is1\r\nerr is-3\r\n```\r\n\r\nHowever, I changed \"global integral = my_tuple[1]\" to \"integral = my_tuple[1]\" in my model's objective function eval_loss,\r\nand now the model actually found a equation that numerically is close to the true function 12*x^2 (1-x).\r\nSo perhaps this was the issue?\r\n\r\nMy thoughts for why the model's loss value was inaccurate:\r\nI know it is because the value of the variable \"integral\" was inaccurate.\r\n\r\nSince the PySR package creates multiple threads which call eval_loss at the same time, the value of the variable \"integral\" that was being used to divide \"predictions\" by, was actually coming from a different call to eval_loss with a different parameter tree from a different integral.\r\n\r\nWhen I did \"global integral = my_tuple[1]\", this actually changed the value of integral for a different call to eval_loss function for a different parameter \"tree\".\r\n\r\n\r\nQuestion: Is there any way to make the call to model.fit(X, y) only create one thread  instead of multiple threads? A.K.A. is there a way to make the call to model.fit(X, y) run completely sequentially, instead of parallel?\r\n\r\nQuestion: I did \"progress=False\" when I constructed the model, but when I ran model.fit(X, y), it still showed the progress bar. Any reason this happens?\r\n",
              "createdAt": "2023-07-11T02:08:21Z"
            }
          ],
          "pageInfo":
          {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wNy0xMVQwMzowODoyMSswMTowMM4AYdL0"
          }
        }
      }
    }
  }
}