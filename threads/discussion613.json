{
  "data":
  {
    "repository":
    {
      "discussion":
      {
        "number": 613,
        "title": "Help with custom loss function",
        "body": "Summary:\r\nI am trying to create a loss function that incorporates multiple variables that are not used in X for Pysr. I'm not sure if this is possible. Essentially I am trying to achieve multi-objective optimization through a custom loss function. \r\n\r\nBackground:\r\nI have several Gaussian optics equations with multiple variables to choose an intraocular lens for surgery.\r\nI have a training database with _measured_ truths AFTER surgery:\r\n\r\n- Actual_P: lens inserted\r\n- Actual_S: resulting refraction following surgery\r\n- Actual_P and Actual_S have a complex relationship that is based on Gaussian optics, and relies on the final lens position, _e_\r\n\r\nI also have variables that can be measured BEFORE surgery\r\n\r\n- a,k,d,L,w,o and g, these will be used to predict e.\r\n\r\nBack_calculated_e is an variable that can be **back calculated** based on gaussian optics: \r\n\r\n- Back_calculated_e = f1(a,k,Actual_P,Actual_S), this is used for training data (y)\r\n\r\n**Pysr is used to Predict e based on pre-operative variables:**\r\n\r\n- Predicted_e = f2(a,k,d,L,w,o) with y This is the Pysr equation\r\n\r\nFollowing this we can verify how good our Predicted_E is by using it to compare:\r\n- **Predicted_P to Actual_P,              Predicted P=f3(a,k,Actual_S,Predicted_e)**\r\n- **Predicted_S to Actual_S,             Predicted S=f4(a,k,Actual_P,Predicted_e)**\r\n \r\nWhy not just use Back_Calculated_e as y and use MSE as a loss function? This is what I'm currently doing with Back_Calculated e as the y, with a,k,d,L,w,o as X. \r\n\r\nHowever, as there are measurement errors of baseline variables (_a_ in particular), this makes Back_Calculated_e unreliable as _a_ increases, and so it is much better to rely on absolute truths for loss function, e.g. an average of MSE between [Predicted_P and Actual_P], and [Predicted_S and Actual_S]. \r\n\r\n**How can I feed in Actual_S and Actual_P to a custom loss function although they can are not included in the x variables?**\r\nActual_S and Actual_P are just two arrays that I can read from an Excelsheet. \r\n\r\nThank you very much in advance, or if anyone has any ideas of how this could be achieved?\r\nFailing this:\r\n1) Is it possible to feed in Actual_P (more important than Actual_S) as a weight then make that the custom loss from Predicted_e?\r\n2) I will run a separate Pysr model that does not use _a_ to predict e, when _a_ is large. However, it seems to be a suboptimal solution.\r\n\r\n**Gaussian Optics equations if of interest:** \r\nv is a constant (12).  \r\n\r\nEquation to calculate P from e, based on a chosen S: \r\n_Predicted_P=(1336 / (a - e)) - (1336 / (1336 / ((1000 / (1000 / s - v)) + k) - e))_ (function 3)\r\n\r\nEquation to calculate S from E, based on a chosen P:\r\n_Predicted_S = 1000*(a*e*k*p - 1336*a*k - 1336*a*p - e^2*k*p + 1336*e*p + 1784896)/(a*e*k*p*v - 1000*a*e*p - 1336*a*k*v - 1336*a*p*v + 1336000*a - e^2*k*p*v + 1000*e^2*p + 1336*e*p*v + 1784896*v)_ (function 4)\r\n\r\nCannot get this to work...\r\n\r\n```julia\r\nfunction custom_loss(tree, data)\r\n  a, k, d, L, w, o, g = extract_variables(tree) #how to extract these variables?\r\n\r\n  # Extract actual P and S from the data structure\r\n  actual_p = ?#how to feed through\r\n  actual_s = ?#how to feed through\r\n\r\n  # Predicted e from the symbolic expression (tree)\r\n  predicted_e = evaluate(tree, [a, k, d, L, w, o, g])\r\n\r\n  # Calculate predicted P and S using preexisting functions\r\n  predicted_p = calculate_predicted_p(predicted_e, actual_s)\r\n  predicted_s = calculate_predicted_s(predicted_e, actual_p)\r\n\r\n  # Combine errors using weighted MSE\r\n  weight_p = 0.5  # Adjust weight for P prediction\r\n  weight_s = 0.5  # Adjust weight for S prediction\r\n  return weight_p * mean((predicted_p - actual_p) .^ 2) + weight_s * mean((predicted_s - actual_s) .^ 2)\r\nend\r\n```\r\n\r\nCurrent Code:\r\n\r\n```python\r\nimport numpy as np\r\nfrom pysr import PySRRegressor\r\nimport pandas as pd\r\nfrom pandas import ExcelFile\r\ndf = pd.read_excel(\"path.xlsx\", sheet_name=\"DataTrain\")\r\nx = df[['a', 'k','d', 'L', 'w','g','o']].to_numpy() #'AVAL',\r\ny = df[['e']].to_numpy()\r\n\r\nmodel = PySRRegressor(\r\n    elementwise_loss = \"L1DistLoss()\",\r\n    model_selection=\"accuracy\",  \r\n    niterations=1000000, #1000000\r\n    #ncycles_per_iteration=1500,\r\n    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\r\n    unary_operators=[\r\n        \"cos\",\r\n       \"tan\",\r\n        \"exp\",\r\n        \"sin\",\r\n        \"sqrt\",\r\n        \"inv\",\r\n        \"square\",\r\n\t\"log\"\r\n    ],\r\n    maxsize=100,\r\n    warm_start=True,\r\n    populations=18,\r\n    population_size=300,\r\n    fraction_replaced_hof = 0.1,\r\n    parsimony = 0.01,    \r\n    bumper=True,\r\n    nested_constraints={\r\n        \"sin\": {\"sin\": 0, \"cos\": 0, \"tan\": 0}, \r\n        \"cos\": {\"sin\": 0, \"cos\": 0, \"tan\": 0},\r\n        \"tan\": {\"sin\": 0, \"cos\": 0, \"tan\": 0},\r\n        \"exp\": {\"exp\": 0, \"log\": 1},\r\n\t\"log\": {\"exp\": 1, \"log\": 0},\r\n        \"square\": {\"square\": 2, \"sqrt\": 4},\r\n        \"sqrt\": {\"square\": 4, \"sqrt\": 2},\r\n    }\r\n)\r\nmodel.fit(x, y,variable_names=[\"a\",\"k\",\"d\",\"L\",\"w\",\"g\",\"o\"])",
        "comments":
        {
          "nodes":
          [
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "Happy to help. I am a bit confused about one thing in your question:\r\n\r\n> Predicted_P to Actual_P, Predicted P=f(a,k,Actual_S,Predicted_e)\r\n> Predicted_S to Actual_S, Predicted S=f(a,k,Actual_P,Predicted_e)\r\n\r\nDoes this mean you want to find the same `f` for both, but just with the variable (S or P) swapped? If not, could you rewrite your question with a different symbol for functions that should be different? Thanks!",
              "createdAt": "2024-05-02T17:08:25Z"
            },
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "> How can I feed in Actual_S and Actual_P to a custom loss function although they can are not included in the x variables?\r\n\r\nWhat I normally do here is just feed in the variables as additional columns of `X`, but then zero them out within the custom loss function. For example:\r\n\r\n```julia\r\nfunction my_loss_function(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    X = copy(dataset.X)\r\n    y = copy(dataset.y)  # Don't need to copy if you aren't modifying; but just a safer habit\r\n\r\n    # Ordering (depends how you pass to .fit)\r\n    # 'a' - 1; 'k' - 2; 'd' - 3; 'L' - 4; 'w' - 5; 'g' - 6;'o' - 7.\r\n    # Thus, say that 'P' is 8 and 'S' is 9\r\n    X_without_P_and_S = vcat(\r\n        X[1:7, :],  # The actual data\r\n        X[8:9, :] .* 0,  # Pass zeroed version\r\n    )\r\n    # Need to pass the the full shape,\r\n    # as the genetic algorithm will still sometimes use features 8 and 9!\r\n    # Thus, we simply hide that information from it.\r\n\r\n    prediction, complete = eval_tree_array(tree, X_without_P_and_S, options)\r\n    if !complete\r\n        return L(Inf)\r\n    end\r\n\r\n    mse = sum(i -> (prediction[i] - y[i])^2, eachindex(y)) / length(y)\r\n\r\n    # Do something with X[8, :] and X[9, :] ? \r\n\r\n    return loss\r\nend\r\n```\r\n\r\nHopefully this helps you get started! And pass this entire thing as a string to the `loss_function` parameter.",
              "createdAt": "2024-05-02T17:17:20Z"
            }
          ],
          "pageInfo":
          {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNS0wMlQxODoxNzoyMCswMTowMM4AjeEG"
          }
        }
      }
    }
  }
}