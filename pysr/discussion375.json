{
  "data": {
    "repository": {
      "discussion": {
        "number": 375,
        "title": "Custom objective questions",
        "body": "I can not find a reason for the difference between the output of objective function using Julia helper and output of objective function using model.\r\n\r\nThe math function isPySRFunction(X=>11.9731872446037).\r\nthe model.equations_.iloc[3, :] is\r\n```\r\ncomplexity                                                      15\r\nloss                                                       0.08352\r\nscore                                                     0.217383\r\nequation         '(((((((0.3079866 / 0.026352234) + 0.9724522) - 0.3079866) + -0.18929175) + x0) + -0.18929175) - x0)\r\nsympy_format                                      11.9731872446037\r\nlambda_format                    PySRFunction(X=>11.9731872446037)\r\nName: 3, dtype: object\r\n\r\n```\r\n\r\nThe model's reported loss is 0.08352. This is incorrect, given that the math function is 11.9731872 (I don't want the model to return this value).\r\n\r\nWhile when I run the objective function using julia helper object, I get a loss of 1.0000000015930166. This is correct.\r\n\r\nConcrete yes/no question you can answer:\r\nDoes the model call my custom objective function eval_loss multiple times, or just one time, in order to find its reported value of loss?\r\n\r\nI can provide code, but the julia helper object ran the same code as the model's objective function.\r\nSpecifically, the julia helper object ran this code (and returned a final loss of 1.000000000001593):\r\n```\r\njl.eval(\"\"\"\r\n# YOU USE THIS CODE BLOCK TO TEST OUT THE VALUE OF THE CUSTOM OBJECTIVE FUNCTION\r\n\r\nusing Distributions\r\nusing SymbolicRegression\r\n\r\n\r\nfunction eval_loss_real(tree, dataset, options)\r\n    MAX_THRESHOLD_ALLOWED = 10000\r\n    \r\n\r\n    prediction, flag = eval_tree_array(tree, dataset, options)\r\n    !flag && return L(Inf)\r\n    \r\n    \r\n    \r\n    num_points = length(prediction)\r\n    println(\"num_points is \", num_points)\r\n    num_points = size(prediction)[1]\r\n    println(\"num_points is \", num_points)\r\n    \r\n    \r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY POSITIVE INFINITY VALUES\r\n    # OR NEGATIVE INFINITY VALUES ON THE DOMAIN X=[0, 1].\r\n    \r\n    \r\n    # NOTE: reshape([x], 1, 1) should give same dimensions\r\n    # as reshape([x;], 1, 1).\r\n    # f(x) = x -> 1/(eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))\r\n    f_recip(x) = 1 / ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])\r\n    \r\n    # fp(x) = ForwardDiff.derivative(f,float(x))\r\n    fp_recip(x) = ( -1 / ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])^2 * (eval_diff_tree_array(tree, reshape([Float32(x)], 1, 1), options, 1))[2][1])\r\n    \r\n    # 0.26047713\r\n    #println(((eval_tree_array(tree, reshape([Float32(0.01)], 1, 1), options))[1][1])^2)\r\n    #println(fp_recip(0.01))\r\n    \r\n    global found_a_recip_root = true\r\n    global recip_roots = -3\r\n    global recip_roots_2 = -4\r\n    try\r\n        global recip_roots = find_zero((f_recip, fp_recip), 0.3, Roots.Newton(), maxevals=100)\r\n        global recip_roots_2 = find_zero((f_recip, fp_recip), 0.01, Roots.Newton(), maxevals=100)\r\n        println(\"recip_roots is\", recip_roots)\r\n        println(\"recip_roots_2 is\", recip_roots_2)\r\n    catch\r\n        global found_a_recip_root = false\r\n        # \"Convergence Failed\"\r\n    end\r\n    \r\n    #println(\"recip_roots is\", recip_roots)\r\n    \r\n    \r\n    # If the function f(x) represented by the tree \"tree\"\r\n    # has a value f(x) close to positive infinity or negative infinity\r\n    # when x is in [0, 1], then return L(Inf).\r\n    if found_a_recip_root\r\n        if (recip_roots >= 0 && recip_roots <= 1)\r\n            return (Inf)\r\n        end\r\n    end\r\n    \r\n    if found_a_recip_root\r\n        if (recip_roots_2 >= 0 && recip_roots_2 <= 1)\r\n            return (Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    # BELOW SECTION IS TO CHECK FOR IF THE GUESSED FUNCTION TAKES ON ANY NEGATIVE VALUES OR ZERO VALUES\r\n    # ON THE DOMAIN X=[0, 1].\r\n    f(x) = ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])\r\n    \r\n    fp(x) = ((eval_diff_tree_array(tree, reshape([Float32(x)], 1, 1), options, 1))[2][1])\r\n    \r\n    global found_a_root = true\r\n    global roots = -3\r\n    try\r\n        global roots = find_zero((f, fp), 0.3, Roots.Newton(), maxevals=100)\r\n        #println(\"roots is\", roots)\r\n    catch\r\n        global found_a_root = false\r\n        # \"Convergence Failed\"\r\n    end\r\n    \r\n    println(\"roots is\", roots)\r\n    \r\n    if found_a_root\r\n        if (roots >= 0 && roots <= 1)\r\n            println(\"found a root so returned L(Inf) and string rep of the tree was\")\r\n            println(string_tree(tree, options))\r\n            return (Inf)\r\n        end\r\n    end\r\n    \r\n    \r\n    \r\n    global integral = 1\r\n    err = -3\r\n    try\r\n        my_tuple = quadgk(x -> (eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1], 0, 1, rtol=0.01)\r\n        global integral = my_tuple[1]\r\n        global err = my_tuple[2]\r\n    catch\r\n        println(\"A error when calling quadgk\")\r\n        return (Inf)\r\n    end\r\n    \r\n    \r\n    \r\n    norm_constant = integral\r\n    println(\"norm_constant is\", norm_constant)\r\n    if (integral <= 0)\r\n        println(\"integral was <=0\")\r\n        return (Inf)\r\n    end\r\n    \r\n    if (isinf(integral))\r\n        println(\"integral was Inf\")\r\n        return (Inf)\r\n    end\r\n    \r\n    actual_probs = (prediction) / norm_constant\r\n    \r\n    # length(actual_probs) equals length(prediction)\r\n    \r\n    # We really care about the EXPECTED VALUE OF L, where L = -1 * sum(log.(actual_probs)) / (length(prediction))\r\n    # given that dataset.X is generated by the true probability distribution.\r\n    # E(L) is really equal to the definite integral from x=0 to x=1 of ln(guessed_f(x)) * f_true(x) dx.\r\n    # As num_points goes to +infinity, the variance of L becomes 0.\r\n    \r\n    \r\n    final_loss = exp(-1 * sum(log.(actual_probs)) / (num_points))\r\n    #println(\"final_loss is \", final_loss)\r\n    return final_loss\r\nend\r\n\r\noptions = SymbolicRegression.Options(\r\n    binary_operators=[+, *, /, -, ^],\r\n    unary_operators=[cos, sin],\r\n    enable_autodiff=true\r\n)\r\n\r\nnum_points = 100\r\n\r\nx = rand(Beta(3, 2), num_points)\r\nx = reshape(x, 1, num_points)\r\nx = Float32.(x)\r\n\r\nx1 = Node(;feature=1)\r\n\r\ntree = Node(;val=Float32(11.9731872446037))\r\ntree = Node{Float32}(tree)\r\n\r\nprintln(string_tree(tree, options))\r\n\r\nmy_loss = eval_loss_real(tree, x, options)\r\nprintln(\"FINAL LOSS was \", my_loss)\r\n\r\n\r\n\"\"\")\r\n```\r\n\r\n",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "> Does the model call my custom objective function eval_loss multiple times, or just one time, in order to find its reported value of loss?\r\n\r\nOnly one time per expression. ",
              "createdAt": "2023-07-08T22:22:01Z"
            },
            {
              "author": {
                "login": "unary-code"
              },
              "body": "> Maybe try running this directly from Julia (`Pkg.add(\"SymbolicRegression\"); using SymbolicRegression`)? Also, you might want to pass it a `Dataset` object rather than an `Array` as is being done now, so as to ensure absolute consistency in both cases?\r\n\r\nI already did run\r\n```\r\njl.eval(\"\"\"\r\nPkg.add(\"SymbolicRegression\")\r\nusing SymbolicRegression\r\n\"\"\")\r\n```\r\nbefore I ran my objective function using the julia helper \"jl\".\r\n\r\nThat's a good point: Now I'll try changing the objective function's inputs to be a dataset so it is consistent with the model's objective function. That was the only difference between my Julia helper's objective function and the model's objective function.\r\n\r\n",
              "createdAt": "2023-07-08T22:35:11Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wNy0wOFQyMzozNToxMSswMTowMM4AYY6_"
          }
        }
      }
    }
  }
}