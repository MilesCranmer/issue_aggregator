{
  "data": {
    "repository": {
      "discussion": {
        "number": 469,
        "title": "How can I re-discover the Fast Inverse Square Root function?",
        "body": "Hey everyone! \r\n\r\nSuper excited to start learning this package and adding it to my toolkit, I'm trying to learn the buttons and knobs of this package by re-discovering the famous fast inverse square root function from Quake. The function seems to not be discovering it well, or really finding much that resembles the function. \r\n\r\nThe reason I'm tackling this one in particular is because I vaguely remember during a talk on the package that Miles may have mentioned it's possible to do this, so first and foremost please correct me if I'm hallucinating, but secondly let me know if the code to do this is already out there somewhere!\r\n\r\n\r\nHere's my code so far\r\n\r\n```Julia\r\nusing SymbolicRegression\r\nusing MLJ\r\n\r\nX = (; input=Float32.(rand(10:100, 100)))\r\ny = 1 ./ sqrt.(X.input)\r\n\r\nshiftright(a::Float32) = reinterpret(Float32, reinterpret(Int32, a) >> 1)\r\nshiftleft(a::Float32) = reinterpret(Float32, reinterpret(Int32, a) << 1)\r\n\r\nmodel = SRRegressor(\r\n    binary_operators=[-, *, +],\r\n    unary_operators=[shiftright, shiftleft],\r\n    niterations=1000,\r\n)\r\nmach = machine(model, X, y)\r\nfit!(mach)\r\n\r\n\r\nusing Plots\r\nplotly()\r\n\r\n# testing various outputs\r\nc6(x) = ((shiftright(x) * -3.2614e+17) - -0.3356)\r\nc17(input) = (0.39054 - ((input * 0.38849) * ((0.041157 - (input * (((input - 0.38849) * -2.6506e-06) - -0.00055827))) * 0.6586)))\r\nc14(input) = ((shiftright(shiftleft(shiftleft(input * Float32(-1.4716e+17))) + Float32(7.9136e+34)) - (input * Float32(0.0048464))) - Float32(-0.30194))\r\ns = scatter(X.input, y, label=\"data\")\r\nscatter!(X.input, c6.(X.input), label=\"6\")\r\nscatter!(X.input, c14.(X.input), label=\"14\")\r\nscatter!(X.input, c17.(X.input), label=\"17\")\r\n```\r\n\r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/9f1ab577-0444-4deb-ae27-6a20384eefde)\r\n\r\n\r\nSo we're certainly getting reasonable answers! Just not the Famous 6-8 operation answer. I wanted to ask you all for ideas and tips on how to better explore the function space here, and if I may be doing something wrong or missing some critical aspects of the search. \r\n\r\nThanks!\r\nWill. ",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "Boxylmer"
              },
              "body": "Oh, and if anyone would like to verify the FISR in Julia!\r\n\r\n```Julia\r\n\r\nfunction fisr(x::Float32)\r\n    x2 = x * Float32(0.5);\r\n    i  = reinterpret(Int32, x)\r\n    i  = 0x5f3759df - (i >> 1) \r\n    y  = reinterpret(Float32, i)\r\n    y  = y * (Float32(1.5) - (x2 * y * y))\r\nend\r\ninputs = Float32.(range(1, 100, 1000))\r\nout_approx = fisr.(inputs)\r\nout_actual = 1 ./ sqrt.(inputs)\r\n\r\ns = plot(inputs, out_actual)\r\nplot!(inputs, out_approx)\r\n```\r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/a87fec5b-348c-44a4-a373-3c63e3643f10)\r\n",
              "createdAt": "2023-11-17T04:48:29Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Hey @Boxylmer,\r\n\r\nYou indeed are not hallucinating, I did that search maybe 1-2 years and occasionally mention it in talks. I'm happy to hear you are interested in this type of thing, it would be cool to have a reproducible example in the docs!\r\n\r\nI'm not sure if I'll be able to find the original script as it was 2 laptops ago... but I will dig around.\r\n\r\nYou have the overall right idea with the operators if I recall correctly, but be sure to look at the tuning page: https://astroautomata.com/PySR/tuning/. Also probably need to tweak complexity of each operator (based on number FLOPS – I think I measured those with https://github.com/triscale-innov/GFlops.jl), etc., which will encourage it to use the bitshift operator. Might also want to throw more compute at the problem on a multi-node cluster.\r\n\r\nIt might have also been that the Newton iteration steps (the `y  = y * (Float32(1.5) - (x2 * y * y))` – which can be repeated btw) can be just incorporated into the loss function, because it's a standard method (derivable). Maybe that's what I did originally. If not you will probably need to use a larger maxsize to represent the whole algorithm in a single binary tree without shared connections.\r\n\r\n---\r\n\r\nI'm trying to also remember if I tweaked any part of the code related to the constants... (since optimizing the constants for this problem is a bit tricky as it's related to the bits, rather than a continuous value). I don't think I hacked anything, but it could have just been I disabled constant optimization (a parameter), or switched to NelderMead (another parameter), or something else. Sorry I'm not more helpful here... But yeah I'd be eager to help get this example working again!",
              "createdAt": "2023-11-17T04:54:47Z"
            },
            {
              "author": {
                "login": "Boxylmer"
              },
              "body": "Hey Miles! Thanks for the detailed and fast response.\r\n\r\nOh yeah, I originally gave it way too much freedom and made an arbitrary bit shift, but quickly realized it never used it, and settled on using the shift-by-one methods above. The docs on PySR were super helpful! I should have checked over there as well rather than just the Julia docs (which I think you actually mentioned too: that PySR was more documented)\r\n\r\nThe complexity trick is super clever! Just assign the average time that computation takes as the complexity. I don't have too much experience with SLURM clusters, but I want to get into it, and I have a few laptops sitting around that I could use to set it up eventually! I found that the pool of equations stagnated after a bit of time (or at least the hall of fame did). Is this something I should actually just leave for a bit more time? The data has zero noise, so I figured it was just not finding the magic numbers.\r\n\r\nOn that note, magic numbers for bit shifting is tricky. The number is massive and is represented in hexadecimal form. It seems unlikely that optim would be able to find it unless we search for it in log space instead, but I'm not immediately sure how to set that up and it also feels like cheating.\r\n\r\nI'm also willing to put some more effort into it, as I think finding magic tricks with algorithms in specific types could prove super useful for some microcontrollers, and I was hoping to make enough headway to write a short communication on it.\r\n\r\n\r\nThanks for the tips! ",
              "createdAt": "2023-11-17T05:25:37Z"
            },
            {
              "author": {
                "login": "Boxylmer"
              },
              "body": "@MilesCranmer checking in with some updates! I have the current script finding a lot of pretty decent approximations that almost look like piecewise functions. I'm stuck here, I believe, and am not sure how to get it to further find FISR. Here's some of the key steps I've taken\r\n\r\n1. I defined a number of \"atomic\" binary operations and trick operators that can be used\r\n2. I made sure FISR can be found using these operations (I've included the function and some code to prove this) \r\n3. I made SR aware of these operators and gave it what it needs to find the actual function. \r\n4. I hacked together a hexadecimal magic number search by mapping the Float32 domain onto the UInt32 domain with a sigmoid-like function so that the optimization would get somewhat close. I empirically found that `-5.2391f0` is the FISR magic number on that map. \r\n\r\n\r\nI was genuinely surprised that it didn't turn out able to find it, even with me messing with a few hyperparameters and letting it run overnight. I'm hoping you might be able to point me in the right direction as this is a really interesting problem that I'm hoping to use to find other FISR like tricks for microcontrollers. One hunch I'm getting is that there might be some way to punish non-smooth functions, or those piecewise looking results that you typically get from bit shifting. Telling it that there should only be N instances of certain operators feels like cheating, so I'm not at that point yet, haha. \r\n\r\n\r\nHere you can see that FISR is near-perfect, but two good functions I found with complexities of 6 and 8 respective look piecewise-like. \r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/fb79af5d-18a9-449d-b186-2e39f39b350f)\r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/bef18b87-8c41-4b13-87a5-e4003455f6de)\r\n\r\n\r\n```Julia\r\nusing MLJ, SymbolicRegression\r\n\r\nX = (; input=Float32.(range(0.1, 100, 10000)))\r\ny = 1 ./ sqrt.(X.input)\r\n\r\nshift_right(a::Float32) = reinterpret(Float32, reinterpret(Int32, a) >> 1)\r\nshift_left(a::Float32) = reinterpret(Float32, reinterpret(Int32, a) << 1)\r\n\r\nfunction scaled_sigmoid(x::Float32, k::Float32)\r\n    return 1 / (1 + exp(-k * x))\r\nend\r\nmap_float32_to_uint32(f::Float32) = floor(UInt32, ((scaled_sigmoid(f, 0.1f0)) * (2^32 - 2^7 - 1)))\r\nmagic_add(val::Float32, magic::Float32) = magic == 0f0 ? val : reinterpret(Float32, reinterpret(UInt32, val) + map_float32_to_uint32(magic)) \r\n\r\nmagic_inverse(val::Float32) = reinterpret(Float32, -reinterpret(Int32, val))\r\n\r\nfunction fisr(x::Float32, magicnum::Float32)\r\n    x2 = x * Float32(0.5)\r\n    i1 = magic_inverse(x)\r\n    i2 = shift_right(i1)\r\n    y  = magic_add(i2, magicnum)\r\n    y  = y * (Float32(1.5) - (x2 * y * y))\r\nend\r\n\r\n# see the mapping function, maybe this should just be a linear transformation? \r\nplot(range(-100, 100, 100), map_float32_to_uint32.(Float32.(range(-1000, 1000, 100))))\r\n\r\n\r\nmodel = SRRegressor(\r\n    binary_operators=[*, +, magic_add],\r\n    unary_operators=[shift_right, magic_inverse, neg],\r\n    complexity_of_operators=[shift_right=>0.5, magic_inverse=>0.5, magic_add=>0.5],\r\n    niterations=100,\r\n    ncycles_per_iteration=5000,\r\n    optimizer_nrestarts=4,\r\n    optimizer_algorithm=\"NelderMead\",\r\n)\r\nmach = machine(model, X, y)\r\nfit!(mach)\r\n\r\nusing Plots\r\nplotly()\r\n\r\n# testing various outputs\r\ntest_x = (; input=Float32.(range(0.05, 10, 1000)))\r\ntest_y = 1 ./ sqrt.(test_x.input)\r\n\r\n\r\nc6(input) = magic_inverse(magic_add(neg(shift_right(input * Float32(-4.1931e-20))), Float32(-2.3726)))\r\nc8(input) = magic_add(magic_inverse(shift_right(input) * Float32(0.88152)), Float32(-0.18826)) * Float32(8.441e-20)\r\nc19(input) = (magic_inverse((shift_right(shift_right(input)) * Float32(1.5938e+29)) * (Float32(1.5938e+29) + magic_add(magic_inverse(shift_right(shift_right(Float32(-0.54733) + input))), Float32(-0.54733)))) * magic_inverse(shift_right(shift_right(input * Float32(5.4562)))))\r\ns = plot()\r\nplot!(s, X.input, y, label=\"data\")\r\nscatter!(s, test_x.input, c8.(test_x.input), label=\"8\")\r\nscatter!(s, test_x.input, c6.(test_x.input), label=\"6\")\r\nscatter!(s, test_x.input, c19.(test_x.input), label=\"19\")\r\n\r\nscatter!(s, test_x.input, fisr.(test_x.input, -5.2391f0), label=\"FISR\")\r\nplot!(s, test_x.input, test_y, label=\"test_answers\")\r\n\r\n\r\n\r\n# what's the expected loss if we get the right answer with L2 loss? \r\ninputs = Float32.(range(1, 100, 1000))\r\nout_actual = 1 ./ sqrt.(inputs)\r\ns = plot(inputs, out_actual)\r\nscatter!(s, inputs, out_approx)\r\nsum((out_approx .- out_actual))^2/length(out_actual) # \"perfect\" loss\r\n                                # 0x5f3759df \r\nmap_float32_to_uint32(-5.2391f0) # magic numbers!\r\n\r\n\r\n```\r\n",
              "createdAt": "2023-11-17T23:51:20Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Did you try the tip about doing the Newton approximation by hand? i.e., something like this:\r\n\r\n```julia\r\nfunction loss_with_newton_approx(tree, dataset::Dataset{T,L}, options) where {T,L}\r\n    if tree.degree != 0\r\n        return L(10_000)  # Return large loss if not split-able\r\n    end\r\n    f = tree.l\r\n    f_output, f_flag = eval_tree_array(f, dataset.X, options)\r\n    if !f_flag\r\n        return L(Inf)\r\n    end\r\n    g = tree.r\r\n    g_output, g_flag = eval_tree_array(g, dataset.X, options)\r\n    if !g_flag\r\n        return L(Inf)\r\n    end\r\n    \r\n    # Now, we can compute the Newton approx:\r\n    y = f_output\r\n    @. y =  y * (T(1.5) - (g_output * y * y))\r\n    # Second step (or more!) is optional (can remove it from final evaluation on microcontroller)\r\n    @. y =  y * (T(1.5) - (g_output * y * y))\r\n\r\n    return sum(i -> (y[i] - dataset.y[i])^2, eachindex(y)) / length(y)\r\nend\r\n\r\nmodel = SRRegressor(; loss_function=loss_with_newton_approx, other_params...)\r\n```\r\n\r\n\r\nYou can derive that loss from Newton method to optimization (the `y=y*(1.5-x*y*y)`), from the original formula (without anything crazy), so maybe this is okay to use generally? \r\n\r\nJust keep in mind that the formulae it prints out are \"unsplit\" and you will need to print the equations by yourself in pieces (splitting at `tree.l, tree.r`)",
              "createdAt": "2023-11-18T00:40:00Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Other assorted tips/questions\r\n\r\n1. You should use a larger `maxsize`. Maybe just go up to 40. Otherwise it will be impossible to find it because the complexity is too large.\r\n2. I ran your code on my laptop and it only took 5 minutes to run. But you mentioned it took all night on your machine; do you know if you maybe don't have multithreading turned on or have an incompatible Julia install or something?\r\n3. When doing functional approximations I like to compute the ground truth with `BigFloat`, since `sqrt(::Float32)` might have its own small errors\r\n    - My favorite personal example is `log(1+erf(x))`, which I needed to compute for the weird likelihood in [this paper](https://www.pnas.org/doi/10.1073/pnas.2026053118#fig01). The default evaluation is awful for `x<-5`, and even the ones implemented in various packages are just simple Taylor series. The one SR found was significantly more accurate AND cheaper.\r\n4. Since you want to get to 0.0 loss, you could turn `parsimony=0.0` (doesn't punish complexity as much). Then you might want to turn up the adaptive parsimony with `adaptive_parsimony_scaling=100` or 1000 (adaptive way to favor diversity of complexities, rather than an absolute penalty term)\r\n5. You could try turning off optimization, with `should_optimize_constants=false`. Then it will use only the mutation operator to change constants, rather than NelderMead.",
              "createdAt": "2023-11-18T00:53:31Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Data generation I might go a bit larger dynamic range. Also you probably don't need that many samples:\r\n\r\n```julia\r\njulia> X = (; input=Float32.(10 .^ collect(range(-3, 3, 1000))))\r\n\r\njulia> y = @. Float32(1 / sqrt(big(X.input)));\r\n```\r\n\r\n(Which will also do the calculation with BigFloat)",
              "createdAt": "2023-11-18T00:57:11Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Lastly, for the actual loss, MSE might not be a great choice, as it might just ignore precision for very small values. You could try MSE in log space instead:\r\n\r\n```julia\r\nloss(prediction, target) = prediction <= 0 ? eltype(target)(Inf) : (log(prediction) - log(target))^2\r\n```\r\n",
              "createdAt": "2023-11-18T01:00:17Z"
            },
            {
              "author": {
                "login": "Boxylmer"
              },
              "body": "Wow! Thank you so much for the tips. I feel I'm learning quite a bit as a product of this problem. \r\n\r\nI should clarify, it took a few days prior as I turned up the iterations and cycles per iteration as I left to work on other problems, the current iterations I posted here was my \"debug run\" length. Sorry about that!\r\n\r\nOne thing I'm having a bit of a hard time understanding is the adaptive parsimony scaling.  This encourages diversity in complexity, but if parsimony=0, will this still do something? \r\n\r\nUsing the tips you've provided, other than the custom loss function to include loss with a step of newton's method (I wasn't able to get my head around the trees with the time I had this weekend), the code I have is below.\r\n\r\nThis approaches reasonable answers _much_ more quickly than before, and answers seem to be a bit more precise. Right now I think this can be attributed to using loss in the log domain. Overall, current approximations are great for most applications\r\n\r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/44195584-b8dd-49d7-9a92-6b9c5c273b20)\r\n\r\nBut we still aren't able to do this naively, I assume with the newtons step taken out of the search, it wouldn't have a problem. It's interesting, however, that we're still seeing \"jittery\" solutions every time. Have often do you try to incorporate a derivative term in the loss? I.e., total_loss = loss(value) + loss(derivative)? \r\n\r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/15a2d199-1b9f-4ec8-aecc-f8e5b4a89168)\r\n\r\nFinally for the sake of learning, what advantage does turning off optimization have (tip #5)? Is letting it learn through the genetic algorithm preferable if we're trying to navigate a parameter space that's highly nonlinear or unable to be navigated smoothly? \r\n\r\n```Julia\r\nusing MLJ, SymbolicRegression\r\nusing Plots\r\nplotly()\r\n\r\nX = (; input=Float32.(10 .^ collect(range(-3, 3, 1000))))\r\ny = @. Float32(1 / sqrt(big(X.input)));\r\n\r\nshift_right(a::Float32) = reinterpret(Float32, reinterpret(Int32, a) >> 1)\r\nshift_left(a::Float32) = reinterpret(Float32, reinterpret(Int32, a) << 1)\r\n\r\nfunction scaled_sigmoid(x::Float32, k::Float32)\r\n    return 1 / (1 + exp(-k * x))\r\nend\r\n\r\nmap_float32_to_uint32(f::Float32) = floor(UInt32, ((scaled_sigmoid(f, 0.1f0)) * (2^32 - 2^7 - 1)))\r\nmagic_add(val::Float32, magic::Float32) = magic == 0f0 ? val : reinterpret(Float32, reinterpret(UInt32, val) + map_float32_to_uint32(magic)) \r\n\r\nmagic_inverse(val::Float32) = reinterpret(Float32, -reinterpret(Int32, val))\r\n\r\nfunction fisr(x::Float32, magicnum::Float32)\r\n    x2 = x * Float32(0.5)\r\n    i1 = magic_inverse(x)\r\n    i2 = shift_right(i1)\r\n    y  = magic_add(i2, magicnum)\r\n    y  = y * (Float32(1.5) - (x2 * y * y))\r\nend\r\n\r\n# see the mapping function, maybe this should just be a linear transformation? \r\nplot(range(-100, 100, 100), map_float32_to_uint32.(Float32.(range(-1000, 1000, 100))))\r\n\r\nloss(prediction, target) = prediction <= 0 ? eltype(target)(Inf) : (log(prediction) - log(target))^2\r\n\r\nmodel = SRRegressor(\r\n    binary_operators=[*, +, magic_add],\r\n    unary_operators=[shift_right, magic_inverse, neg],\r\n    complexity_of_operators=[shift_right=>0.5, magic_inverse=>0.5, magic_add=>0.5],\r\n    niterations=100000,\r\n    ncycles_per_iteration=1000,\r\n    optimizer_nrestarts=4,\r\n    optimizer_algorithm=\"NelderMead\",\r\n    parsimony=0.0,\r\n    maxsize=50,\r\n    adaptive_parsimony_scaling=1000, # or 100, as miles mentioned. I feel I have an unclear meaning of what this does.\r\n    # should_optimize_constants=false, # more of a hail mary than something that will definitively improve things.\r\n    elementwise_loss=loss,\r\n)\r\nmach = machine(model, X, y)\r\nfit!(mach)\r\n# r = report(mach)\r\nfitted_model = fitted_params(mach)\r\nselected_equation = fitted_model.equations[9]\r\n\r\n# testing various outputs\r\nmach.model.selection_method = Returns(9)\r\n\r\ns = plot()\r\nscatter!(s, X.input, predict(mach, X), label=\"predict\")\r\nscatter!(s, X.input, fisr.(X.input, -5.2391f0), label=\"FISR\")\r\nplot!(s, X.input, y, label=\"data\", linewidth=4)\r\n```\r\n\r\n",
              "createdAt": "2023-11-20T06:43:53Z"
            },
            {
              "author": {
                "login": "Boxylmer"
              },
              "body": "I'm actually struggling to implement derivative information in the `loss_function` input. I've seen the trick proposed prior was to include derivative information as extras in the weights argument, but the [pull request for the associated issue](https://github.com/MilesCranmer/SymbolicRegression.jl/pull/249) didn't pass and the logs have expired. Is there a current best practice on this if I wanted to include said derivative information using the current main version? \r\n\r\n\r\n```Julia\r\nX = (; input=Float32.(10 .^ collect(range(-3, 3, 1000))))\r\ny = @. Float32(1 / sqrt(big(X.input)));\r\ndy = Float32.([Zygote.gradient(x -> 1/sqrt(x), val)[1] for val in big.(X.input)])\r\n```",
              "createdAt": "2023-11-24T01:25:48Z"
            },
            {
              "author": {
                "login": "Boxylmer"
              },
              "body": "Absolutely no need to apologize! I'm enjoying learning the library and figuring out the issue. I really appreciate the time you've taken this far to assist on the problem. \r\n\r\nYup! I use zygote to calculate the derivative for the bigfloat ground truth and planned on using finitediff.jl on the `eval_tree_array` function. The issue is getting that data available in the dataset using either MLJ or the native interface.\r\n\r\nTo clarify, I tried passing in a named tuple to get the data in as `(; dy=...)` on the main branch, but in this case you're just talking about using a single `Vector{Float32}` as a quick fix until that PR is merged? Or was there a way to do the named tuple that I may have missed? ",
              "createdAt": "2023-11-24T02:19:23Z"
            },
            {
              "author": {
                "login": "Boxylmer"
              },
              "body": "Alright, after a bit of effort I'm actually finding functions that, at first glance, appear to _outperform_ the original FISR function. This ended up happening by including the newton approximation in the loss function (as you first suggested) and without any derivative information. \r\n\r\nFor the sake of learning, I'm actually struggling to get the derivative terms incorporated into the loss function in a way that doesn't constantly end up with `NaN`s or `Inf`s everywhere, and left everything as-is except for the derivative loss being set to zero (so that by default the code should run correctly). \r\n\r\nTo show the results so far, I made two plots:\r\n- One that shows the log scale results of the outputs from equations that have losses on the order of the best loss (i.e., from the best loss to 10X the best loss) and had complexities lower than 12. For reference, without Newton's method, the FISR complexity would be 4 to 6. \r\n- Another that shows the deviation from the true answer along with FISR\r\n\r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/b92facc9-e97d-4a31-992e-ecbb9552b053)\r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/2b021b50-8468-460b-b629-fb458be4b240)\r\n\r\n___\r\n![image](https://github.com/MilesCranmer/PySR/assets/11844340/74bbdedb-9db6-4ef8-b059-8fea124a117e)\r\n\r\nAs a side note, I was able to observe the very interesting behavior of the loss suddenly \"clicking\", and going from ~0.1 to 1e-7 instantly at high complexity equations, then over time the algorithm would find ways to keep that order of loss while reducing the complexity bits and pieces at a time. I understand this is expected and possibly even routine for you, but it's exciting seeing it in action!\r\n\r\n\r\nThe hunch I'm beginning to get from this is that FISR has multiple \"fast\" solutions, and finding the exact FISR magic number is not something we should expect from this type of approach. The FISR solution deviates in a very similar visual way to that of the other solutions, and about how we would expect for a theoretical solution with a complexity of somewhere between 4 and 6. \r\n\r\n\r\nCode so far\r\n\r\n```Julia\r\nusing MLJ, SymbolicRegression\r\nusing Plots\r\nusing Zygote\r\nplotly()\r\n\r\nX = (; input=Float32.(10 .^ collect(range(-3, 3, 1000))))\r\ny = @. Float32(1 / sqrt(big(X.input)));\r\ndy = Float32.([Zygote.gradient(x -> 1/sqrt(x), val)[1] for val in big.(X.input)])\r\n\r\nshift_right(a::Float32) = reinterpret(Float32, reinterpret(Int32, a) >> 1)\r\nshift_left(a::Float32) = reinterpret(Float32, reinterpret(Int32, a) << 1)\r\n\r\nfunction scaled_sigmoid(x::Float32, k::Float32)\r\n    return 1 / (1 + exp(-k * x))\r\nend\r\n\r\nmap_float32_to_uint32(f::Float32) = floor(UInt32, ((scaled_sigmoid(f, 0.1f0)) * (2^32 - 2^7 - 1)))\r\nmagic_add(val::Float32, magic::Float32) = magic == 0f0 ? val : reinterpret(Float32, reinterpret(UInt32, val) + map_float32_to_uint32(magic)) \r\n\r\nmagic_inverse(val::Float32) = reinterpret(Float32, -reinterpret(Int32, val))\r\n\r\nfunction fisr(x::Float32, magicnum::Float32)\r\n    x2 = x * Float32(0.5)\r\n    i1 = magic_inverse(x)\r\n    i2 = shift_right(i1)\r\n    y  = magic_add(i2, magicnum)\r\n    y  = y * (Float32(1.5) - (x2 * y * y))\r\nend\r\n\r\n# see the mapping function, maybe this should just be a linear transformation? \r\nplot(range(-100, 100, 100), map_float32_to_uint32.(Float32.(range(-1000, 1000, 100))))\r\n\r\nvalue_loss(prediction, target) = prediction <= 0 ? eltype(target)(Inf) : (log(prediction) - log(target))^2\r\nfunction deriv_loss(prediction, target)\r\n    # scatter_loss = abs(log((abs(prediction)+1e-20) / (abs(target)+1e-20)))\r\n    # sign_loss = 10 * (sign(prediction) - sign(target))^2\r\n    # return (scatter_loss + sign_loss) / 100\r\n    return 0\r\nend\r\n\r\n\r\nfunction numerical_derivative(f, x::AbstractArray{T}, h=T(1e-10)) where T\r\n    return (f(x .+ h) .- f(x)) ./ h\r\nend\r\n\r\nfunction eval_with_newton(tree, x, options, iters=1)\r\n    pred_y, completed = eval_tree_array(tree, x, options)\r\n    for _ in 1:iters\r\n        pred_y .= pred_y .* (3/2 .- (pred_y .* pred_y .* @view x[1, :]) ./ 2)\r\n    end\r\n    return pred_y, completed\r\nend\r\n\r\nfunction loss_function(tree, dataset::Dataset{T,L}, options, idx) where {T,L}\r\n    y = dataset.y\r\n    dy = dataset.weights\r\n    total_loss::L = 0\r\n    # pred_y_improved = eval_with_newton(tree, dataset.X, options)\r\n    pred_y, completed = eval_with_newton(tree, dataset.X, options)\r\n    if !completed\r\n        return L(Inf) # grossly assume that if it ran on the dataset, then it will run on perturbations of those points (±ε of each point in the dataset for 1st deriv.)\r\n    end\r\n   \r\n    function eval_without_flag(x)\r\n        return eval_with_newton(tree, x, options)[1]\r\n    end\r\n\r\n    pred_dy = numerical_derivative(eval_without_flag, dataset.X)\r\n    for i in eachindex(y)\r\n        vl = value_loss(pred_y[i], y[i])\r\n        dl = deriv_loss(pred_dy[i], dy[i])\r\n        total_loss += (vl + dl)\r\n    end\r\n    norm_loss = total_loss / length(y)\r\n    return norm_loss\r\nend\r\n\r\n\r\nmodel = SRRegressor(\r\n    binary_operators=[*, +, magic_add],\r\n    unary_operators=[shift_right, magic_inverse, neg],\r\n    complexity_of_operators=[shift_right=>1, magic_inverse=>1, magic_add=>1],\r\n    niterations=1000,\r\n    ncycles_per_iteration=100,\r\n    optimizer_nrestarts=4,\r\n    optimizer_algorithm=\"NelderMead\",\r\n    parsimony=0.0,\r\n    maxsize=50,\r\n    adaptive_parsimony_scaling=1000, # or 100, as miles mentioned. I feel I have an unclear meaning of what this does.\r\n    # should_optimize_constants=false, # more of a hail mary than something that will definitively improve things.\r\n    loss_function=loss_function,\r\n    progress=true, # ??? when loss functions are provided it doesn't print by default anymore???\r\n)\r\n\r\nmach = machine(model, X, y, dy)\r\nfit!(mach)\r\nr = report(mach)\r\nfitted_model = fitted_params(mach)\r\n\r\n\r\n# testing various outputs\r\ns = plot()\r\nscatter!(s, X.input, log.(fisr.(X.input, -5.2391f0)), label=\"FISR\")\r\nds = Dataset(reshape(X.input, 1, :))\r\noptions = Options(; unary_operators=[shift_right, magic_inverse, neg], binary_operators=[*, +, magic_add])\r\nloss_cutoff = losses[end] * 100 # only show equations \"on the order of\" the best loss\r\ncomplexity_cutoff = 10 # and equations with less than this many operations\r\nfor (i, eq) in enumerate(fitted_model.equations)\r\n    if r.losses[i] <= loss_cutoff && r.complexities[i] < complexity_cutoff\r\n        complexity = r.complexities[i]\r\n        loss = r.losses[i]\r\n        x = X.input\r\n        pred_y = eval_with_newton(eq, ds.X, options)[1]\r\n        scatter!(s, x, log.(pred_y), label=string(complexity) * \"→ logloss=\" * string(round(log10(loss); digits=2)))\r\n    end\r\nend\r\nplot!(s, X.input, log.(y), label=\"data\", linewidth=4)\r\n\r\n\r\n# deviation curve\r\ns = plot()\r\nplot!(s, X.input, log.(fisr.(X.input, -5.2391f0)) .- log.(y), label=\"FISR\", linewidth=2)\r\nds = Dataset(reshape(X.input, 1, :))\r\noptions = Options(; unary_operators=[shift_right, magic_inverse, neg], binary_operators=[*, +, magic_add])\r\nloss_cutoff = losses[end] * 10 # only show equations \"on the order of\" the best loss\r\ncomplexity_cutoff = 12 # and equations with less than this many operations\r\nfor (i, eq) in enumerate(fitted_model.equations)\r\n    if r.losses[i] <= loss_cutoff && r.complexities[i] < complexity_cutoff\r\n        complexity = r.complexities[i]\r\n        loss = r.losses[i]\r\n        x = X.input\r\n        ypred = eval_with_newton(eq, ds.X, options)[1]\r\n        plot!(s, x, log.(ypred) .- log.(y), label=string(complexity) * \"→ logloss=\" * string(round(log10(loss); digits=2)))\r\n    end\r\nend\r\ndisplay(s)\r\nplot!(s, X.input, zeros(length(y)), label=\"data\", linewidth=4)\r\n\r\n\r\n\r\n\r\n\r\n# why is the equation of complexity 8 so good? it likely isn't being plotted correctly\r\nfunction unusual_equation(x)\r\n    v1 = magic_add(magic_inverse(shift_right(Float32(x))) * Float32(0.998277), Float32(-0.09932381)) * shift_right(Float32(0.3022318))\r\n    v2 = v1 .* (3/2 .- (v1 .* v1 .* x) ./ 2)\r\n    return v2\r\nend\r\nx = range(0.1, 10, 100)\r\ny_unusual = unusual_equation.(x)\r\ny_exact = 1 ./ sqrt.(big.(x))\r\ns = plot(y_exact, y_unusual .- y_exact) # yup! random bug. \r\n```",
              "createdAt": "2023-11-25T08:48:22Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "This is so awesome. Nice work!!\r\n\r\n> For the sake of learning, I'm actually struggling to get the derivative terms incorporated into the loss function in a way that doesn't constantly end up with `NaN`s or `Inf`s everywhere, and left everything as-is except for the derivative loss being set to zero (so that by default the code should run correctly).\r\n\r\nThe normal `eval_tree_array` has a return value `(output, flag)`. This is set up so that if it detects a `NaN` or `Inf` at any point when evaluating the expression, it will just immediately return with `flag == false` (some operators throw an error if you feed them an `Inf`, hence this check). That means the array you get when `flag == false` is basically just intermediate evaluation values, and shouldn't be used, as it doesn't reflect the full expression.\r\n\r\nLuckily, you can use the short-form version `tree(X, options)` as a sort of `eval_without_flag`, only that it will set all the array values to `NaN` if a flag is thrown. This is because `(::Node{T})(::AbstractArray{T}, ::Options)` is defined as a method for convenient use.\r\n\r\nSo, here is how I would modify this:\r\n\r\n```julia\r\n    # Inside loss function:\r\n    pred_dy = numerical_derivative(x -> tree(x, options), dataset.X)\r\n    if !all(isfinite, pred_dy)\r\n        return L(Inf)\r\n    end\r\n```\r\n\r\nDoes this make sense?\r\n\r\n---\r\n\r\nSide note: this makes me want to get https://github.com/MilesCranmer/SymbolicRegression.jl/pull/135 actually working (all the pieces are there; just need to get the mutations working appropriately). That way it would basically let you evolve the Newton approximation method as well, because it would be able to \"re-use\" variables further up the tree.",
              "createdAt": "2023-11-25T17:00:28Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Regarding learning the Newton's approximation from scratch (or another graph-like calculation), I've made some progress on https://github.com/SymbolicML/DynamicExpressions.jl/pull/56. Once all of that is done (will take a bit of work), it should be possible to use that to learn \"programs\" rather than just tree-like expressions. So it should be useful for your use-case.",
              "createdAt": "2023-11-25T21:23:16Z"
            },
            {
              "author": {
                "login": "Boxylmer"
              },
              "body": "> The normal eval_tree_array has a return value (output, flag)\r\n\r\nWhoops! I wish I had seen that. I was even using the function in your runtests.jl to generate random trees in the effort to benchmark the loss function's allocations. Thanks! I've rewritten those functions using the new method. I've also uploaded this as a more elegant writeup here: [here](https://github.com/Boxylmer/FISR-Discovery-with-SymbolicRegression.jl) instead of posting the code every update :)\r\n\r\nI'm getting much worse results when incorporating derivative information, even when playing around with the weight that the derivative itself has (i.e., C*deriv_loss + value_loss, where I mess with C) as well as tricks with the derivative loss function itself. We're already getting decent answers though without it, so maybe it's just unnecessary!\r\n\r\nFor generalizing the learning to graphs rather than just trees: That would be very exciting! This would be super useful for physical models, where similar terms tend to show up often, especially when units and unit conversions matter. \r\n",
              "createdAt": "2023-11-27T06:13:09Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0xMS0yN1QwNjoxMzowOSswMDowMM4AdSTA"
          }
        }
      }
    }
  }
}