{
  "data": {
    "repository": {
      "issue": {
        "number": 706,
        "title": "[BUG]: Memory leak(?) when using batching with large dataset (>450K items)",
        "body": "### What happened?\n\nIt seems like when letting PySR run forever after a while it gets an OOM error after a while, but only when using a large dataset. I can watch it steadily grow memory.\r\n\r\nI used the following setup in each of my tests changing the sample_size 10000 value to a specific value or commenting the three lines out to run the whole dataset:\r\n\r\n```python\r\nimport numpy as np\r\nimport json\r\nfrom pysr import PySRRegressor\r\n\r\nwith open('test2.json', 'r') as file:\r\n    data = json.load(file)\r\n\r\ndata_array = np.array(data)\r\n\r\nsample_size = min(10000, len(data_array))\r\nrandom_indices = np.random.choice(len(data_array), size=sample_size, replace=False)\r\ndata_array = data_array[random_indices]\r\n\r\n# Split the array into X and y\r\nX = data_array[:, :5]\r\ny = data_array[:, 5]\r\n\r\nmodel = PySRRegressor(\r\n    procs=14,\r\n    populations=32,\r\n    population_size=200,\r\n    ncycles_per_iteration=1000,\r\n    niterations=10000000,\r\n    binary_operators=[\"-\", \"+\", \"*\", \"/\", \"^\"],\r\n    unary_operators=[\r\n        \"square\",\r\n    ],\r\n    nested_constraints={\r\n        \"square\": {\"square\": 1 }\r\n    },\r\n    constraints={\r\n        \"^\": (-1, 8),\r\n    },\r\n    elementwise_loss=\"\"\"\r\nloss(prediction, target) = try \r\n    trunc(Int, prediction) == trunc(Int, target) ? 0 : (prediction - target)^2\r\ncatch\r\n    (prediction - target)^2\r\nend\r\n\"\"\",\r\n    maxsize=40,\r\n    maxdepth=10,\r\n    complexity_of_constants=2,\r\n    batching=True,\r\n    batch_size=200,\r\n    heap_size_hint_in_bytes=200000000,\r\n)\r\n\r\nmodel.fit(X, y)\r\n```\r\n\r\nMy dataset has 460K records which I know isn't advised, but it's for a niche problem. The memory issues appears to only happen when running on a dataset over a certain size.\r\n\r\nI saw a comment about heap_size_hint_in_bytes needing to be set and I've played with values for it, but it doesn't appear to change the behavior. I've set it to 1.2 GB and also 200MB for instance. I've tried smaller batch sizes like 50 and it doesn't appear to change the behavior either. None of the other settings appear to change things either. I've tried procs=8 and smaller populations and population_size, and smaller ncycles_per_iteration.\r\n\r\n100K random records then WSL starts at 3.85 GB. At 10 minutes 4GB. At 50 minutes 4.3 GB. At 1 hour 3.2 GB. At 1.5 hours 3 GBs. No issues.\r\n\r\n200K random records then WSL starts at ~4GB. At 20 minutes 4.2GB, At 1 hour 3.6GB. At 2 hours 15 minutes 3.5GB. No issues.\r\n\r\n300K random records then WSL starts at ~4.3GB.  At 20 minutes 5.3GB. Grew to 5.9 GBs then dropped to 5.2GB at 30 minutes. 1 hour 6GB. 1 hour 15 minutes 5.4GB. 1 hour 30 minutes 4.7GB. 1 hour 32 minutes 4.4GB. No issues.\r\n\r\n400k random records then WSL starts at ~4.2GB. 3 minutes 4.5GB. 8 hours 30 minutes 7.7GB.\r\n\r\n460K then WSL starts at ~4.8GB. 2 minutes 5.2GB. 30 minutes 9.6GB, 40 minutes 12.3GB. 55 minutes 14.5 GB. 1 hour 15.1GB. 1 hour 10 minutes 15.4GB. 1 hour 19 minutes 15.5 GB. 1 hour 26 minutes OOM. I ran this also using:\r\n\r\n```\r\nsample_size = min(460309, len(data_array))\r\n```\r\nJust to be sure and it failed at 1 hour 23 minutes so no difference.\r\n\r\nI've attached my test2.json file. \r\n[test2.json](https://github.com/user-attachments/files/16754363/test2.json)\r\n\r\n\n\n### Version\n\n0.19.4\n\n### Operating System\n\nLinux\n\n### Package Manager\n\npip\n\n### Interface\n\nIPython Terminal\n\n### Relevant log output\n\n_No response_\n\n### Extra Info\n\n_No response_",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "How much memory does your machine have? I fear this OOM error might be “real” in the sense that it is legitimately not enough memory for doing the calculations up to this maxsize and in this many processes (with this large a dataset) at once. Keep in mind that even with batching, it will still periodically evaluate and compute gradients on the entire dataset (although perhaps in the future we could aggregate over batches rather than all at once…). I think it may be trending towards more complex expressions which is why the memory usage increases later in training.\r\n\r\nAlthough I don’t want to rule out a memory leak issue, it could still be the case.\r\n\r\nMaybe you could try with fewer processes, but the same 200MB heap size limit, and see if the OOM still occurs?\r\n\r\nThanks for the detailed report!",
              "createdAt": "2024-08-26T22:39:48Z"
            },
            {
              "author": {
                "login": "sirisian"
              },
              "body": "> How much memory does your machine have?\r\n\r\nI have 32GB with around 16GB free when I'm testing. I have tried running this with procs=8 with the same behavior.\r\n\r\n> Maybe you could try with fewer processes, but the same 200MB heap size limit, and see if the OOM still occurs?\r\n\r\nI just ran it with procs=4, same settings as above with ```sample_size = min(460309, len(data_array))```. WSL starts at 4.8GB. 5 minutes 6GB. 15 minutes 7.7GB. 33 minutes 11.2GB. 40 minutes 12GB. 45 minutes 11.6GB. 50 minutes 12.8GB. 1 hour 14.3GB. 1 hour 10 minutes 15.4GB. 1 hour 20 minutes 15.6GB. And OOM at 01:20:10.\r\n\r\nThis seems to be independent of procs based on this testing.\r\n\r\n> I think it may be trending towards more complex expressions which is why the memory usage increases later in training.\r\n\r\nIs it continuously updating this memory with new expressions? If it could offload to storage feasibly that would be nice. (I have TBs of storage). The loss seems to slowly go down, so I'm interested in having this run forever.",
              "createdAt": "2024-08-27T01:19:11Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Thanks, that's interesting. Let me walk you through my thought process on debugging –\r\n\r\nSo a vector of length 460k with `precision=64` (default) is about 4 MB. For a maxsize of 40, this means that at worst, let's say an expression evaluation will have need to allocate 21 arrays of that size (one for each leaf), meaning 84 MB of memory usage per evaluation. Then, with 14 processes running concurrently, this is about 1.2 GB of memory being used for a single evaluation at every process at the same time.\r\n\r\nNow, one tricky part is that PySR can **also** use multithreading during evaluations, at this part of the code: https://github.com/MilesCranmer/SymbolicRegression.jl/blob/cd23a6e25c64d00565c3ae3905d06dc3c63033ed/src/SingleIteration.jl#L112.\r\n\r\n```julia\r\n    @threads_if !(options.deterministic) for j in 1:(pop.n)\r\n        if options.should_simplify\r\n            tree = pop.members[j].tree\r\n            tree = simplify_tree!(tree, options.operators)\r\n            if tree isa Node\r\n                tree = combine_operators(tree, options.operators)\r\n            end\r\n            pop.members[j].tree = tree\r\n        end\r\n        if options.should_optimize_constants && do_optimization[j]\r\n            # TODO: Might want to do full batch optimization here?\r\n            pop.members[j], array_num_evals[j] = optimize_constants(\r\n                dataset, pop.members[j], options\r\n            )\r\n        end\r\n    end\r\n```\r\n\r\n By default threading is enabled (unless you set the environment variable `PYTHON_JULIACALL_THREADS` and `JULIA_NUM_THREADS` to `1`, which means 1 thread per Julia process)\r\n\r\nthis means if each process has like 14 threads (which might be automatically set if you have 14 cores), at worst, assuming all the processes hit this part of the code at the same time, there is 16.8 GB of memory usage. That is the very worst case though. And since Julia uses garbage collection, memory is not deallocated immediately (unless you were to use `bumper=True` which uses bump allocation instead of garbage collection), so it could be that memory hangs around and looks similar to a memory leak. \r\n\r\nMaybe that could be something to try first. Does `bumper=True` help at all? Or do you still see the same behavior?\r\n\r\nIt's weird that using fewer processes doesn't help. Maybe you could also try `multithreading=False, procs=0` which should use serial mode? That should be easier on memory consumption.",
              "createdAt": "2024-08-27T11:56:04Z"
            },
            {
              "author": {
                "login": "sirisian"
              },
              "body": "> precision=64 (default)\r\n\r\nhttps://astroautomata.com/PySR/api/ This says default is 32.\r\n\r\n> Maybe you could also try multithreading=False, procs=0 which should use serial mode?\r\n\r\nI ran that test overnight and it indeed runs without issues. I ran it for over 10 hours and it never went over 2GB of memory. I'd probably need to play with settings though as the loss kind of stopped lowering it seemed. (It was above the levels I got when using procs=14 and such I assume due to less variation or something).\r\n\r\n>  Does bumper=True help at all?\r\n\r\nOOM at 5 hours and 51 minutes. That does appear to help.\r\n\r\nI'll try running it with lower procs and bumper to see what happens.\r\n",
              "createdAt": "2024-08-27T22:51:29Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Sorry you are totally right about the precision, oops!\r\n\r\nReally interesting that bumper didn’t help fix it. If that has no effect then I am a bit confused as to the cause. Do you have another machine to test this on, preferably one that isn’t Windows? I know that Windows can sometimes have issues with multiprocessing in Julia. Maybe the garbage collection is having issues.\r\n\r\nIt would also be interesting to test this on Julia 1.11 (not yet released) as I know they’ve improved the garbage collection in a certain way. Perhaps it can help with issues like this. \r\n\r\nThough I admit I’m confused as to the actual source of the memory usage here since the bumper option didn’t change it.\r\n\r\nLastly, have you also tried multithreading instead of multiprocessing, and how did it change things?",
              "createdAt": "2024-08-27T23:24:25Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Oh wait it sounds like bumper actually did help, since the OOM happened 4x later? I guess it’s just the remaining cause of the continual increase.\r\n\r\nAre you also able to monitor per-process memory consumption? My current guess is that the head process is using all of the memory while the children processes use something close to that requested (200 MB). This is because JuliaCall does not yet give a way to size a heap size hint: https://github.com/JuliaPy/PythonCall.jl/issues/546 which would be required to set it for the head process (children processes are set up from within the backend, so should work fine).",
              "createdAt": "2024-08-27T23:33:05Z"
            },
            {
              "author": {
                "login": "sirisian"
              },
              "body": "> Lastly, have you also tried multithreading instead of multiprocessing, and how did it change things?\r\n\r\nSince multithreading=True by default I've been using a single process this whole time it seems. As seen here:\r\n\r\nWhen using the above procs=14 setups WSL is using 9.8GB at 3 hours ```top -H -p <pid>``` looks like:\r\n\r\n![image](https://github.com/user-attachments/assets/901bd174-f975-4fc7-a3ba-d82b0f295ab5)\r\n\r\n![image](https://github.com/user-attachments/assets/72578c3b-2c09-4438-8df7-02779a7dbe16)\r\n\r\nSo I've been using threads. I tested multiprocessing and it failed after 32 minutes outputting this:\r\n\r\n```\r\nUnhandled Task ERROR: IOError: read: connection reset by peer (ECONNRESET)\r\nStacktrace:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  [1] wait_readnb(x::Sockets.TCPSocket, nb::Int64)\r\n    @ Base ./stream.jl:410\r\n  [2] (::Base.var\"#wait_locked#739\")(s::Sockets.TCPSocket, buf::IOBuffer, nb::Int64)\r\n    @ Base ./stream.jl:949\r\n  [3] unsafe_read(s::Sockets.TCPSocket, p::Ptr{UInt8}, nb::UInt64)\r\n    @ Base ./stream.jl:955\r\n  [4] unsafe_read\r\n    @ ./io.jl:774 [inlined]\r\n  [5] unsafe_read(s::Sockets.TCPSocket, p::Base.RefValue{NTuple{4, Int64}}, n::Int64)\r\n    @ Base ./io.jl:773\r\n  [6] read!\r\n    @ ./io.jl:775 [inlined]\r\n  [7] deserialize_hdr_raw\r\n    @ ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/messages.jl:167 [inlined]\r\n  [8] message_handler_loop(r_stream::Sockets.TCPSocket, w_stream::Sockets.TCPSocket, incoming::Bool)\r\n    @ Distributed ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:172\r\n  [9] process_tcp_streams(r_stream::Sockets.TCPSocket, w_stream::Sockets.TCPSocket, incoming::Bool)\r\n    @ Distributed ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:133\r\n [10] (::Distributed.var\"#103#104\"{Sockets.TCPSocket, Sockets.TCPSocket, Bool})()\r\n    @ Distributed ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:121\r\n---------------------------------------------------------------------------\r\nJuliaError                                Traceback (most recent call last)\r\nCell In[1], line 54\r\n     19 y = data_array[:, 5]\r\n     21 model = PySRRegressor(\r\n     22     multithreading=False,\r\n     23     procs=16,\r\n   (...)\r\n     51     bumper=True\r\n     52 )\r\n---> 54 model.fit(X, y)\r\n\r\nFile ~/centripetalcatmull/lib/python3.10/site-packages/pysr/sr.py:2087, in PySRRegressor.fit(self, X, y, Xresampled, weights, variable_names, complexity_of_variables, X_units, y_units)\r\n   2084     self._checkpoint()\r\n   2086 # Perform the search:\r\n-> 2087 self._run(X, y, runtime_params, weights=weights, seed=seed)\r\n   2089 # Then, after fit, we save again, so the pickle file contains\r\n   2090 # the equations:\r\n   2091 if not self.temp_equation_file:\r\n\r\nFile ~/centripetalcatmull/lib/python3.10/site-packages/pysr/sr.py:1890, in PySRRegressor._run(self, X, y, runtime_params, weights, seed)\r\n   1887     jl_y_variable_names = None\r\n   1889 PythonCall.GC.disable()\r\n-> 1890 out = SymbolicRegression.equation_search(\r\n   1891     jl_X,\r\n   1892     jl_y,\r\n   1893     weights=jl_weights,\r\n   1894     niterations=int(self.niterations),\r\n   1895     variable_names=jl_array([str(v) for v in self.feature_names_in_]),\r\n   1896     display_variable_names=jl_array(\r\n   1897         [str(v) for v in self.display_feature_names_in_]\r\n   1898     ),\r\n   1899     y_variable_names=jl_y_variable_names,\r\n   1900     X_units=jl_array(self.X_units_),\r\n   1901     y_units=(\r\n   1902         jl_array(self.y_units_)\r\n   1903         if isinstance(self.y_units_, list)\r\n   1904         else self.y_units_\r\n   1905     ),\r\n   1906     options=options,\r\n   1907     numprocs=cprocs,\r\n   1908     parallelism=parallelism,\r\n   1909     saved_state=self.julia_state_,\r\n   1910     return_state=True,\r\n   1911     addprocs_function=cluster_manager,\r\n   1912     heap_size_hint_in_bytes=self.heap_size_hint_in_bytes,\r\n   1913     progress=progress and self.verbosity > 0 and len(y.shape) == 1,\r\n   1914     verbosity=int(self.verbosity),\r\n   1915 )\r\n   1916 PythonCall.GC.enable()\r\n   1918 self.julia_state_stream_ = jl_serialize(out)\r\n\r\nFile ~/.julia/packages/PythonCall/sQSpa/src/JlWrap/any.jl:240, in __call__(self, *args, **kwargs)\r\n    238     return ValueBase.__dir__(self) + self._jl_callmethod($(pyjl_methodnum(pyjlany_dir)))\r\n    239 def __call__(self, *args, **kwargs):\r\n--> 240     return self._jl_callmethod($(pyjl_methodnum(pyjlany_call)), args, kwargs)\r\n    241 def __bool__(self):\r\n    242     return True\r\n\r\nJuliaError: TaskFailedException\r\nStacktrace:\r\n  [1] wait\r\n    @ ./task.jl:352 [inlined]\r\n  [2] fetch\r\n    @ ./task.jl:372 [inlined]\r\n  [3] _main_search_loop!(state::SymbolicRegression.SearchUtilsModule.SearchState{Float32, Float32, Node{Float32}, Distributed.Future, Distributed.RemoteChannel}, datasets::Vector{Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}, ropt::SymbolicRegression.SearchUtilsModule.RuntimeOptions{:multiprocessing, 1, true}, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Int64}, DynamicExpressions.OperatorEnumModule.OperatorEnum, Node, false, true, nothing, StatsBase.Weights{Float64, Float64, Vector{Float64}}})\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:882\r\n  [4] _equation_search(datasets::Vector{Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}, ropt::SymbolicRegression.SearchUtilsModule.RuntimeOptions{:multiprocessing, 1, true}, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Int64}, DynamicExpressions.OperatorEnumModule.OperatorEnum, Node, false, true, nothing, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, saved_state::Nothing)\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:599\r\n  [5] equation_search(datasets::Vector{Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}; niterations::Int64, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Int64}, DynamicExpressions.OperatorEnumModule.OperatorEnum, Node, false, true, nothing, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, parallelism::String, numprocs::Int64, procs::Nothing, addprocs_function::Nothing, heap_size_hint_in_bytes::Int64, runtests::Bool, saved_state::Nothing, return_state::Bool, verbosity::Int64, progress::Bool, v_dim_out::Val{1})\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:571\r\n  [6] equation_search\r\n    @ ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:449 [inlined]\r\n  [7] #equation_search#26\r\n    @ ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:412 [inlined]\r\n  [8] equation_search\r\n    @ ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:360 [inlined]\r\n  [9] #equation_search#28\r\n    @ ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:442 [inlined]\r\n [10] pyjlany_call(self::typeof(equation_search), args_::Py, kwargs_::Py)\r\n    @ PythonCall.JlWrap ~/.julia/packages/PythonCall/sQSpa/src/JlWrap/any.jl:40\r\n [11] _pyjl_callmethod(f::Any, self_::Ptr{PythonCall.C.PyObject}, args_::Ptr{PythonCall.C.PyObject}, nargs::Int64)\r\n    @ PythonCall.JlWrap ~/.julia/packages/PythonCall/sQSpa/src/JlWrap/base.jl:73\r\n [12] _pyjl_callmethod(o::Ptr{PythonCall.C.PyObject}, args::Ptr{PythonCall.C.PyObject})\r\n    @ PythonCall.JlWrap.Cjl ~/.julia/packages/PythonCall/sQSpa/src/JlWrap/C.jl:63\r\n\r\n    nested task error: Distributed.ProcessExitedException(14)\r\n    Stacktrace:\r\n      [1] try_yieldto(undo::typeof(Base.ensure_rescheduled))\r\n        @ Base ./task.jl:931\r\n      [2] wait()\r\n        @ Base ./task.jl:995\r\n      [3] wait(c::Base.GenericCondition{ReentrantLock}; first::Bool)\r\n        @ Base ./condition.jl:130\r\n      [4] wait\r\n        @ ./condition.jl:125 [inlined]\r\n      [5] take_buffered(c::Channel{Any})\r\n        @ Base ./channels.jl:477\r\n      [6] take!(c::Channel{Any})\r\n        @ Base ./channels.jl:471\r\n      [7] take!(::Distributed.RemoteValue)\r\n        @ Distributed ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:726\r\n      [8] remotecall_fetch(f::Function, w::Distributed.Worker, args::Distributed.RRID; kwargs::@Kwargs{})\r\n        @ Distributed ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:461\r\n      [9] remotecall_fetch(f::Function, w::Distributed.Worker, args::Distributed.RRID)\r\n        @ Distributed ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:454\r\n     [10] remotecall_fetch\r\n        @ ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:492 [inlined]\r\n     [11] call_on_owner\r\n        @ ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:565 [inlined]\r\n     [12] fetch(r::Distributed.Future)\r\n        @ Distributed ~/centripetalcatmull/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:619\r\n     [13] (::SymbolicRegression.var\"#67#72\"{SymbolicRegression.SearchUtilsModule.SearchState{Float32, Float32, Node{Float32}, Distributed.Future, Distributed.RemoteChannel}, Int64, Int64})()\r\n        @ SymbolicRegression ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:984\r\n```\r\n\r\nMy program:\r\n\r\n```\r\nimport numpy as np\r\nimport json\r\nfrom pysr import PySRRegressor\r\n\r\nwith open('test2.json', 'r') as file:\r\n    data = json.load(file)\r\n\r\ndata_array = np.array(data)\r\n\r\nprint(len(data_array));\r\n\r\nsample_size = min(460309, len(data_array))\r\nprint(sample_size);\r\nrandom_indices = np.random.choice(len(data_array), size=sample_size, replace=False)\r\ndata_array = data_array[random_indices]\r\n\r\n# Split the array into X and y\r\nX = data_array[:, :5]\r\ny = data_array[:, 5]\r\n\r\nmodel = PySRRegressor(\r\n    multithreading=False,\r\n    procs=16,\r\n    populations=16,\r\n    population_size=50,\r\n    ncycles_per_iteration=1000,\r\n    niterations=10000000,\r\n    binary_operators=[\"-\", \"+\", \"*\", \"/\", \"^\"],\r\n    unary_operators=[\r\n        \"square\",\r\n    ],\r\n    nested_constraints={\r\n        \"square\": {\"square\": 1 }\r\n    },\r\n    constraints={\r\n        \"^\": (-1, 8),\r\n    },\r\n    elementwise_loss=\"\"\"\r\nloss(prediction, target) = try \r\n    trunc(Int, prediction) == trunc(Int, target) ? 0 : (prediction - target)^2\r\ncatch\r\n    (prediction - target)^2\r\nend\r\n\"\"\",\r\n    maxsize=40,\r\n    maxdepth=10,\r\n    complexity_of_constants=2,\r\n    batching=True,\r\n    batch_size=1000,\r\n    heap_size_hint_in_bytes=200000000,\r\n    bumper=True\r\n)\r\n\r\nmodel.fit(X, y)\r\n```\r\n\r\n(The ncycles_per_iteration is high because lower values had the head worker at 99%).",
              "createdAt": "2024-08-29T13:54:07Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "> Since multithreading=True by default I've been using a single process this whole time it seems. As seen here:\r\n\r\nAh, the `heap_size_hint_in_bytes` not affecting things makes much more sense now. The `heap_size_hint_in_bytes` is only used when `multithreading=False`. \r\n\r\nThis parameter is basically used to prevent OOM errors. When the memory usage gets close to the heap size hint, Julia's garbage collection gets much more aggressive. However, in PySR, this parameter only gets set in new processes. This is because Julia is not aware of the memory usage of other Julia processes, so it can help to set it in advance for newly spawned processes.\r\n\r\nFor a single process, it's usually not needed. I guess it is here because the garbage collection isn't working hard enough on WSL (?). Right now you can't set the heap size hint on the main Julia process if using PythonCall.jl. So, let me prioritise https://github.com/JuliaPy/PythonCall.jl/issues/546 and try to add this, and we can see if it helps at all.",
              "createdAt": "2024-08-29T18:21:56Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Ok I made a PR to PythonCall.jl here: https://github.com/JuliaPy/PythonCall.jl/pull/547. Once that gets in you can see if that fixes it (will be available via the `PYTHON_JULIACALL_HEAP_SIZE_HINT` environment variable; see https://julialang.org/blog/2023/04/julia-1.9-highlights/#memory_usage_hint_for_the_gc_with_--heap-size-hint)",
              "createdAt": "2024-08-29T18:39:02Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "See https://github.com/JuliaLang/julia/issues/56759 and https://github.com/JuliaLang/julia/pull/56801. Basically there was a real memory leak inside Julia which is now fixed.\r\n\r\nThis should be fixed in Julia 1.11.3 when it is released: https://github.com/JuliaLang/julia/pull/56741.\r\n\r\nI will also see if they can backport to 1.10.\r\n\r\nLet me know if this is still an issue in the latest Julia when it is released.",
              "createdAt": "2024-12-14T05:55:52Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpHOl5Ct_A=="
          }
        }
      }
    }
  }
}