{
  "data":
  {
    "repository":
    {
      "discussion":
      {
        "number": 273,
        "title": "Combined method of RF and Symbolic regression models",
        "body": "Hi Dr. Miles Cranmer,\r\n\r\nThank you for this wonderful tool. I am currently trying to get a mathematical operation (as an index) to estimate the impact of weather stress on agricultural yields by using your package \"pysr\", based on 40 years of county-monthly level data.\r\n\r\nI have 9 features that were narrowed down from the Random Forest model (Wadeker et al., 2020, 2021) and I would like to get one complete equation to capture partial dependence (accumulated local effect) plots of RF for the final relationship.\r\n\r\nQuestions are as follows:\r\n\r\n1.  **Selected features in a final equation (This is very important for me to understand!):**\r\n\r\n1.1) Compared to the important features (with interaction effects) from the RF model, the most important features do not show up in the final pysr equations (of multiple runs with different random states). I am confused by this inconsistency. Do you have any input on this?\r\n\r\n1.2) In addition, the final equation often chooses a couple of features with two or three times of repetitions. Is there any way to force which features to be included in a final equation? \r\n\r\n1.3)  I assume that the final features absorb (or minor improvements to be selected) the effects of the remaining features that did not show up in the final equation? Or do I need to adjust the default parameters such as the ones for mutations or migration parts?\r\n\r\n1.4) Due to the inconsistencies of selected features, I am not sure if a combination of RF+symbolic model could be good, in addition to the fact that pysr does not support plotting. Any inputs?\r\n\r\n2. Parameters:\r\n\r\n2.1) With changes in parameters such as populations and denoise, the running speed was incredibly slow (even on HPC) and it does not seem to stop with the timeout function as well.\r\n\r\n2.2) FYI, I’ve constrained some parameters in a way like Lemos et al., 2022 and Wadeker et al., 2020 but left the others to default values.  I have been testing with different parameters based on \"PySRRegressor reference\" page but with my limited understanding, it has not been easy. What are your strategies to best optimize parameters with different datasets?\r\n   - Training data structure: (8200, 9)\r\n   - Loss: Mean squared error (default)\r\n   - Optimizer iterations: 10\r\n   - Maxsize: 40\r\n   - Operations: [“plus”, “mult”, “sub”, “pow”, “div”, “square”, “cube”, “tan”, cos”, inv”]\r\n\r\n\r\nThank you again for your input and time in advance. \r\nAlso. if anyone has experience or recommendations, I will very much appreciate your comments!\r\n\r\n",
        "comments":
        {
          "nodes":
          [
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "> 1.1) Compared to the important features (with interaction effects) from the RF model, the most important features do not show up in the final pysr equations (of multiple runs with different random states). I am confused by this inconsistency. Do you have any input on this?\r\n\r\nIf they don't show up over multiple different runs, maybe those features aren't actually important? Computing feature importance with RF isn't a guarantee. Note that with the latest versions of PySR after 2022 (after Jay's paper was done) selecting important features isn't really important, because the crossover operation does this implicitly. So I would just give all the features you have and PySR should be okay to handle it.\r\n\r\n> 1.2) In addition, the final equation often chooses a couple of features with two or three times of repetitions. Is there any way to force which features to be included in a final equation?\r\n\r\nNot sure I follow. If a feature doesn't improve performance, it won't be included. PySR tries to find simple and accurate expressions; the simple aspect means fewer features.\r\n\r\nYou could write a custom loss function that requires features to be used (see https://github.com/MilesCranmer/PySR/discussions/161 for related question) but not sure why you would want that.\r\n\r\n> 1.3) I assume that the final features absorb (or minor improvements to be selected) the effects of the remaining features that did not show up in the final equation? Or do I need to adjust the default parameters such as the ones for mutations or migration parts?\r\n\r\nNot sure I follow.\r\n\r\n> 1.4) Due to the inconsistencies of selected features, I am not sure if a combination of RF+symbolic model could be good\r\n\r\nI would just use PySR by itself and run it for long enough. \r\n\r\n> , in addition to the fact that pysr does not support plotting. Any inputs?\r\n\r\nYou can plot PySR outputs with matplotlib, like: `plt.scatter(model.predict(X, i), y)`, where `i` is the row of the equation you wish to evaluate. See https://astroautomata.com/PySR/examples/#4-plotting-an-expression for an example.\r\n\r\n> 2.1) With changes in parameters such as populations and denoise, the running speed was incredibly slow (even on HPC) and it does not seem to stop with the timeout function as well.\r\n\r\nThe `denoise` parameter uses a Gaussian process. GPs can be extremely slow for many points, as (I think) they scale with n^3. So it probably just didn't get to the search yet. Maybe use fewer points or just avoid using denoising?\r\n\r\n> 2.2) FYI, I’ve constrained some parameters in a way like Lemos et al., 2022 and Wadeker et al., 2020 but left the others to default values. I have been testing with different parameters based on \"PySRRegressor reference\" page but with my limited understanding, it has not been easy. What are your strategies to best optimize parameters with different datasets?\r\n\r\nMy general tips would be to avoid using redundant operators, like how `pow` and `square` and `cube` are equivalent. The fewer operators the better. I usually do the following:\r\n\r\nI run from IPython on the head node of a slurm cluster. Passing `cluster_manager=\"slurm\"` will make PySR set up a run over the entire allocation. I set `procs` equal to the total number of cores over my entire allocation.\r\n\r\n1. Use the default parameters.\r\n2. Use only the operators I think it needs and no more.\r\n3. Set `niterations` to some very large value, so it just runs for a week until my job finishes. If the equation looks good, I quit the job early.\r\n4. Increase `populations` to `3*num_cores`.\r\n5. Set `ncyclesperiteration` to maybe `5000` or so, until the head node occupation is under `10%`.\r\n6. Set `constraints` and `nested_constraints` as strict as possible. These can help quite a bit with exploration. \r\n7. Set `maxdepth` as strict as possible.\r\n8. Set `maxsize` a bit larger than the final size you want. e.g., if you want a final equation of size `30`, you might set this to `35`, so that it has a bit of room to explore.\r\n9. Set `parsimony` equal to about the minimum loss you would expect, divided by 5-10. e.g., if you expect the final equation to have a loss of `0.001`, you might set `parsimony=0.0001`.\r\n10. Set `weight_optimize` to some larger value, maybe `0.001`. This is very important if `ncyclesperiteration` is large, so that optimization happens more frequently.\r\n11. Set `turbo` to `True`. This may or not work, if there's an error just turn it off (some operators are not SIMD-capable). If it does work, it should give you a nice 20% speedup.\r\n\r\nSince I am running in IPython, I can just hit \"q<enter>\" to stop the job, tweak the hyperparameters, and then start the search again.\r\n\r\nSome things I try out to see if they help:\r\n\r\n1. Play around with `complexity_of_operators`. Set operators you dislike (e.g., `pow`) to have a larger complexity.\r\n2. Try setting `adaptive_parsimony_scaling` a bit larger, maybe up to `1000`.\r\n3. Sometimes I try using `warmup_maxsize_by`. This is useful if you find that the search finds a very complex equation very quickly, and then gets stuck. It basically forces it to start at the simpler equations and build up complexity slowly.\r\n4. Play around with different losses:\r\n    i. I typically try `L2DistLoss()` and `L1DistLoss()`. L1 loss is more robust to outliers compared to L2, so is often a good choice for a noisy dataset. \r\n    ii. I might also provide the `weights` parameter to `fit` if there is some reasonable choice of weighting. For example, maybe I know the signal-to-noise of a particular row of `y` - I would set that SNR equal to the weights. Or, perhaps I do some sort of importance sampling, and weight the rows by importance.\r\n\r\nVery rarely I might also try tuning the mutation weights, the crossover probability, or the optimization parameters. I never use `denoise` or `select_k_features` as I find they aren't very useful.\r\n\r\nFor large datasets I usually just randomly sample ~1000 points or so. In case all the points matter, I might use `batching=True`.\r\n\r\nIf I find the equations get very complex and I'm not sure if they are numerically precise, I might set `precision=64`.\r\n\r\nHope this helps!",
              "createdAt": "2023-03-07T17:17:32Z"
            }
          ],
          "pageInfo":
          {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wMy0wN1QxNzoxNzozMiswMDowMM4AT9M2"
          }
        }
      }
    }
  }
}