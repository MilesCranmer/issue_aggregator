{
  "data": {
    "repository": {
      "discussion": {
        "number": 667,
        "title": "Implement mathematical constraints",
        "body": "I want to implement mathematical constraints in a soft way in Pysr. I think if I apply them in a hard way I lose a lot of functions. These constraints are like I know the positiveness or negativeness of function with respect to some values, and behavior of function derivative with respect to some variables.\r\n\r\n\r\nI find that it seems the main of your program is written in Julia. Then you wrapped it to work in Python. I tried to read \"src\" part of your code but it's hard for me because I am not familiar with Julia. So, I read the discussion part of the page and I saw this question: [Constraints?](\"https://github.com/MilesCranmer/PySR/discussions/304\") \"https://github.com/MilesCranmer/PySR/discussions/304\"\r\nIn answer, you said: \"Update: added full_objective (PySR) and loss_function (SymbolicRegression.jl) for this purpose.\"\r\n\r\nActually, I did not find \"full_objective\" part in the Python code. I found and read \"loss_function\" in Julia's part. I also read this question: \"https://github.com/MilesCranmer/PySR/discussions/449\" and this question:\"https://github.com/MilesCranmer/PySR/discussions/256\"\r\nIt seems to me you are trying to say that you can define a new loss function. I want to know: can I define this new loss criterion in Python? If I define a new loss function I must download your code manipulate it and compile it again? I see your loss criterion repeated somewhere in the code, Do I need to make changes elsewhere?",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "To implement a custom loss about function constraints, the right way is to pass a Julia function **as a string** in Python to the `loss_function ` parameter of PySRRegressor. You unfortunately cannot use a pure Python function as the loss function. This is technically possible but it would restrict you to serial computation only, due to Python’s “global interpreter lock”, and would also be very very slow as it couldn’t be compiled or differentiated (whereas Julia code can be).\r\n\r\nIf you describe your constraints exactly or provide an example in Python I can help get it working.\r\n\r\nThe examples that you cited are indeed the most relevant.",
              "createdAt": "2024-07-12T22:43:18Z"
            },
            {
              "author": {
                "login": "VOLCANIC-9"
              },
              "body": "It is crazy!!! your algorithm eliminates variables  instead of accepting the constraints\r\nI have 4 variables. Two of them have constraints. All of them have units.\r\nfirst I write this:\r\n```julia\r\nfunction eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    derivative_with_respect_to = 1\r\n    predicted, gradient, complete = eval_diff_tree_array(tree, dataset.X, options, derivative_with_respect_to)\r\n    if !complete\r\n        # encountered NaN/Inf, so return early\r\n        return L(Inf)\r\n    end\r\n\ti=1\r\n    positivity = sum(i -> gradient[i] > 0 ? L(0) : abs2(gradient[i]), eachindex(gradient))\r\n    sum_square_loss = sum(i -> abs2(predicted[i] - dataset.y[i]), eachindex(predicted, dataset.y))\r\n\t    \r\n\ti=2\r\n    positivity = sum(i -> gradient[i] < 0 ? L(0) : abs2(gradient[i]), eachindex(gradient))\r\n    sum_square_loss = sum(i -> abs2(predicted[i] - dataset.y[i]), eachindex(predicted, dataset.y))\r\n\t\r\n\t\r\n\t\r\n    beta = L(1e-2)\r\n    return (sum_square_loss + beta * positivity) / dataset.n\r\nend\r\n```\r\ncode eliminates variable 1 and variable 2\r\nThen I try to force the algorithm to use them; I write this:\r\n```julia\r\nfunction eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    derivative_with_respect_to = 1\r\n    predicted, gradient, complete = eval_diff_tree_array(tree, dataset.X, options, derivative_with_respect_to)\r\n    if !complete\r\n        # encountered NaN/Inf, so return early\r\n        return L(Inf)\r\n    end\r\n\ti=1\r\n    positivity = sum(i -> gradient[i] > 0 ? L(0) : abs2(gradient[i]), eachindex(gradient))\r\n    sum_square_loss = sum(i -> abs2(predicted[i] - dataset.y[i]), eachindex(predicted, dataset.y))\r\n\t\r\n\tcontains_x1 = any(\r\n        node -> (\r\n            node.degree == 0\r\n            && !(node.constant)\r\n            && node.feature == 1\r\n        ),\r\n        tree\r\n    )\r\n\tif !(contains_x1)\r\n        # penalty term\r\n        sum_square_loss += L(1e10)\r\n    end\r\n\t    \r\n\ti=2\r\n    positivity = sum(i -> gradient[i] < 0 ? L(0) : abs2(gradient[i]), eachindex(gradient))\r\n    sum_square_loss = sum(i -> abs2(predicted[i] - dataset.y[i]), eachindex(predicted, dataset.y))\r\n\t\r\n\tcontains_x3 = any(\r\n        node -> (\r\n            node.degree == 0\r\n            && !(node.constant)\r\n            && node.feature == 1\r\n        ),\r\n        tree\r\n    )\r\n\tif !(contains_x1)\r\n        # penalty term\r\n        sum_square_loss += L(1e10)\r\n    end\r\n\t\r\n    beta = L(1e-2)\r\n    return (sum_square_loss + beta * positivity) / dataset.n\r\nend\r\n```\r\nIt doesn't work. I increased the regularisation parameter but the algorithm can't converge.\r\nAlso, algorithm did not use variables that constant for some cycles in data.",
              "createdAt": "2024-08-06T17:05:57Z"
            },
            {
              "author": {
                "login": "VOLCANIC-9"
              },
              "body": "I just tried to produce a sample; in the main code, all is correct. the interesting part is the algorithm keeps two parameters that do not match with y unit.",
              "createdAt": "2024-08-07T09:28:09Z"
            },
            {
              "author": {
                "login": "VOLCANIC-9"
              },
              "body": "so you say that this part\r\n\"\ti=1\r\n    positivity = sum(i -> gradient[i] > 0 ? L(0) : abs2(gradient[i]), eachindex(gradient))\r\n    sum_square_loss = sum(i -> abs2(predicted[i] - dataset.y[i]), eachindex(predicted, dataset.y))\r\n    \"\r\n   doesn't mean: the derivative with respect to variable 1 must be greater than zero \r\nand this part:\r\n\"\ti=2\r\n    positivity = sum(i -> gradient[i] > 0 ? L(0) : abs2(gradient[i]), eachindex(gradient))\r\n    sum_square_loss = sum(i -> abs2(predicted[i] - dataset.y[i]), eachindex(predicted, dataset.y))\r\n    \"\r\n     doesn't mean: the derivative with respect to variable 2 must be greater than zero ???\r\nSo how I say derive with respect to variable 1?\r\n     \r\n    \" FYI I think the constraint you wrote only enforces that the model has x1\"\r\n\r\n\r\n    Ok, but the model just uses variables x3 and x4",
              "createdAt": "2024-08-08T08:02:45Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wOC0wOFQwOTowMjo0NSswMTowMM4AnL8G"
          }
        }
      }
    }
  }
}