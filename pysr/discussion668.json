{
  "data": {
    "repository": {
      "discussion": {
        "number": 668,
        "title": "Optimizing Objective Function by Reducing Variable Complexity",
        "body": "Hello everyone,\r\n\r\nI need the 4 trees in the following objective function to be functions of only variables 3, 4, and 8. However, my objective function requires 18 inputs. Searching between 18 variables requires more iterations. Is there an option for a custom objective function to be of the form my_custom_objective(tree, dataset::Dataset{T,L}, options, constant_dataset::Dataset{T,L}) where {T,L}, where dataset contains only variables 3, 4, and 8 that the trees use, and constant_dataset contains the remaining variables used for calculating the objective function? This will speed up the training.\r\n\r\n```python\r\nobjective = \"\"\"\r\nfunction my_custom_objective(tree, dataset::Dataset{T,L}, options) where {T,L}\r\n    penalty_term = L(0)\r\n    tot_features = 1:18\r\n    concrete_contribution = L(100)\r\n    web_reinf_contribution = L(100)\r\n    tol = L(0.0000000000000001)\r\n\r\n    r_EIs = dataset.X[1, :]\r\n    r_EIc = dataset.X[2, :]\r\n    eta = dataset.X[3, :]\r\n    e_ratio = dataset.X[4, :]\r\n    e_d = dataset.X[5, :]\r\n    e1_d = dataset.X[6, :]\r\n    lam = dataset.X[7, :]\r\n    slender = dataset.X[8, :]\r\n    ee = dataset.X[9, :]\r\n    ee1 = dataset.X[10, :]\r\n    zf_c = dataset.X[11, :]\r\n    zf_s = dataset.X[12, :]\r\n    EcIc = dataset.X[13, :]\r\n    EsIs = dataset.X[14, :]\r\n    pc1 = dataset.X[15, :]\r\n    ps1 = dataset.X[16, :]\r\n    Ll = dataset.X[17, :]\r\n    f_f = dataset.X[18, :]\r\n\r\n    TL1 = 0.94 .+ 0.003 .* f_f ./ lam\r\n    fac = 0.1\r\n    TL2 = 1.15\r\n    TR1 = 1.1 .+ slender .* fac\r\n    TR2 = 1.0\r\n    prop = 0.0001 .* dataset.y\r\n\r\n    if tree.degree != 2\r\n        penalty_term += L(10000)\r\n    else\r\n        left = tree.l\r\n        if left.degree != 2\r\n            penalty_term += L(1000)\r\n        else\r\n            fn = left.l\r\n            features = [3, 4, 8]\r\n            remain_feat = setdiff(tot_features, features)\r\n            should_nt = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in remain_feat\r\n            ]\r\n            should = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in features\r\n            ]\r\n            penalty_term +=\r\n                L(100) * (sum(should_nt)) + L(100) * (length(features) - sum(should))\r\n            TL1, flag = eval_tree_array(fn, dataset.X, options)\r\n            if !flag\r\n                return L(Inf)\r\n            end\r\n\r\n            fn = left.r\r\n            features = [3, 4, 8]\r\n            remain_feat = setdiff(tot_features, features)\r\n            should_nt = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in remain_feat\r\n            ]\r\n            should = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in features\r\n            ]\r\n            penalty_term +=\r\n                L(100) * (sum(should_nt)) + L(100) * (length(features) - sum(should))\r\n            TL2, flag = eval_tree_array(fn, dataset.X, options)\r\n            if !flag\r\n                return L(Inf)\r\n            end\r\n        end\r\n        right = tree.r\r\n        if right.degree != 2\r\n            penalty_term += L(1000)\r\n        else\r\n            fn = right.l\r\n            features = [3, 4, 8]\r\n            remain_feat = setdiff(tot_features, features)\r\n            should_nt = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in remain_feat\r\n            ]\r\n            should = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in features\r\n            ]\r\n            penalty_term +=\r\n                L(100) * (sum(should_nt)) + L(100) * (length(features) - sum(should))\r\n            TR1, flag = eval_tree_array(fn, dataset.X, options)\r\n            if !flag\r\n                return L(Inf)\r\n            end\r\n\r\n            fn = right.r\r\n            features = [3, 4, 8]\r\n            remain_feat = setdiff(tot_features, features)\r\n            should_nt = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in remain_feat\r\n            ]\r\n            should = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in features\r\n            ]\r\n            penalty_term +=\r\n                L(100) * (sum(should_nt)) + L(100) * (length(features) - sum(should))\r\n            TR2, flag = eval_tree_array(fn, dataset.X, options)\r\n            if !flag\r\n                return L(Inf)\r\n            end\r\n        end\r\n    end\r\n\r\n    if penalty_term < tol\r\n        f_fc_m = 1.0\r\n        f_fy_m = 1.0\r\n        MD = (zf_s .* f_fy_m .+ 0.5 .* zf_c .* f_fc_m)\r\n        f_fc = min.(1.5, max.(TL1, 0.6))\r\n        f_fy = min.(1.5, max.(TL2, 0.6))\r\n        TR1 = min.(2.0, max.(TR1, 0.3))\r\n        TR2 = min.(2.0, max.(TR2, 0.3))\r\n\r\n        pa = ps1 .+ pc1\r\n        pc = pc1\r\n        #println(\"Size of the ps1: \", size(ps1))\r\n        #println(\"Size of the f_fy: \", size(f_fy))\r\n        #println(\"Size of the f_fc: \", size(f_fc))\r\n        #println(\"Size of the pc1: \", size(pc1))\r\n        #println(\"Size of the pa: \", size(pa))\r\n        pa = max.(pa, pc1 .* f_fc .+ f_fy .* ps1)\r\n\r\n        b1 = TR2 .* (0.6 .+ 0.4 .* ee1 ./ ee)\r\n        EI2 = 0.9 .* (EsIs .+ EcIc .* 0.5)\r\n        Ncr = (π^2 .* EI2) ./ (Ll .^ 2)\r\n\r\n        ec = max.(Ll ./ 1000, ee .* b1)\r\n\r\n        for j in 1:length(prop)\r\n            ij = 0\r\n            iter = 1000\r\n            incr = 4\r\n            while ij < iter\r\n                ij += incr\r\n                nn = ij / iter * pa[j]\r\n                m = nn * max(ee[j], ec[j] / (1 - nn / Ncr[j] * TR1[j]))\r\n                ratto_i = (nn + pa[j] - pc[j]) / (pa[j] - pc[j] / 2)\r\n\r\n                th = ratto_i * π\r\n                for k in 1:100\r\n                    thy = sin(th) + ratto_i * π\r\n                    if abs(thy - th) < 1e-3\r\n                        break\r\n                    end\r\n                    th = thy\r\n                end\r\n\r\n                mm = MD[j] * sin(th / 2)^3\r\n                net = m - mm\r\n                if net > 0.0\r\n                    ij -= incr\r\n                    incr *= 0.5\r\n                    if incr < 0.13\r\n                        break\r\n                    end\r\n                end\r\n                prop[j] = nn / 1000\r\n            end\r\n        end\r\n    end\r\n\r\n    r = dataset.y ./ prop .- 1.0\r\n    MAPE = sum(abs.(r)) ./ length(r)\r\n    return (penalty_term + MAPE)\r\nend\r\n\r\nmy_custom_objective\r\n\"\"\"\r\nfrom pysr import PySRRegressor\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nPC  = pd.read_excel(\"da.xlsx\",'M')[['D','t','L','Fy','fc','et','eb','P']]\r\nD=PC['D'];t=PC['t'];L1=PC['L'];fy=PC['Fy'];fc=PC['fc'];et=PC['et'];eb=PC['eb'];P=PC['P']\r\nee = np.maximum(np.abs(et), np.abs(eb))\r\nee1=np.sign(et * eb) * np.minimum(np.abs(et), np.abs(eb))\r\nAco = (D - 2 * t) ** 2 / 4 * np.pi\r\nAso = D ** 2 / 4 * np.pi - Aco\r\nzco = (D - 2 * t) ** 3 / 6; zf_c = zco * fc\r\nzso = (D) ** 3 / 6 - zco;   zf_s = zso * fy\r\nIc = (D - 2 * t) ** 4 / 64 * np.pi\r\nIs = (D) ** 4 / 64 * np.pi - Ic\r\nEs = 210000\r\nEc = 22000 * ((fc + 8) / 10) ** 0.3  ;EsIs = Is * Es  ; EcIc = Ic * Ec;r_EIs=EsIs/(EsIs+EcIc);r_EIc=EcIc/(EsIs+EcIc)\r\npc1=Aco * fc;ps1=Aso * fy;eta=pc1/ps1;e_ratio=ee1/ee\r\nlam = D / t / Es * fy\r\n\r\nf_fc = 1.0\r\npc = pc1 * f_fc\r\npa = pc + ps1\r\nEI1 = EsIs + EcIc * 0.6\r\nNcr = (np.pi ** 2 * EI1) / (L1 * L1)\r\nslender = (pa / Ncr) ** 0.5\r\n\r\nPC['r_EIs']=r_EIs;PC['r_EIc']=r_EIc;PC['eta']=eta;PC['e_ratio']=e_ratio;PC['ee']=ee;PC['ee1']=ee1;PC['e_d']=ee/D;PC['e1_d']=ee1/D\r\nPC['Aco']=Aco;PC['Aso']=Aso;PC['EcIc']=EcIc;PC['EsIs']=EsIs;PC['lam']=lam;PC['slender']=slender\r\nPC['pc1']=pc1;PC['ps1']=ps1;PC['Ll']=L1;PC['zf_s']=zf_s;PC['zf_c']=zf_c;PC['f_f']=fy/fc\r\nfeatures=['r_EIs','r_EIc','eta','e_ratio','e_d','e1_d','lam','slender','ee','ee1','zf_c','zf_s','EcIc','EsIs','pc1','ps1','Ll','f_f']\r\n\r\nX=PC[features];y=PC['P']\r\n\r\nmodelior11=[]\r\nfor j in range(1):\r\n    #population_size is the equation size\r\n    model7 = PySRRegressor(niterations=200,populations=40,population_size=100,maxsize=80,\r\n                      nested_constraints={\"^\":{\"^\":1}},parsimony= 0.02,#adaptive_parsimony_scaling=1000,\r\n                      constraints={\"^\":(-1,10)},\r\n                      binary_operators=[\"+\", \"*\",\"^\"],\r\n                      denoise=True,loss_function=objective,model_selection ='accuracy',\r\n                      )\r\n    model7.fit(X,y)\r\n    i=0\r\n    modelior11.append(model7)\r\n    while i <len(model7.equations_):\r\n        print(model7.sympy(i))\r\n        i=i+1\r\n```",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Initial tips:\r\n\r\nI ran your loss function through `@code_warntype` in Julia to check whether there were any type instabilities, and it looks like there are a few:\r\n\r\n```julia\r\njulia> using SymbolicRegression\r\n\r\njulia> X = randn(Float32, 5, 256); y = randn(Float32, 256);\r\n\r\njulia> dataset = Dataset(X, y);\r\n\r\njulia> tree = Node{Float32}(val=0.1);\r\n\r\njulia> options = Options();\r\n\r\njulia> @code_warntype my_custom_objective(tree, dataset, options)\r\n```\r\n\r\nThis will highlight type instabilities in red which can hurt performance.\r\n\r\nIn particular it looks like it can't figure out the type of \r\n\r\n```julia\r\n  penalty_term::Any\r\n  should::Vector\r\n  should_nt::Vector\r\n```\r\n\r\nand there are also various types that are `Union{Float32, Float64}` which means Julia doesn't know if they are a 32-bit float or 64-bit float. To get rid of that you can use `penalty_term += L(...)` to convert to the right type (which in this case is `Float32`), because `Dataset{T,L}` will tell the compiler what `L` is.\r\n\r\nIt looks like this means the inferred return value of the custom objective is `Any` - which will slow things down a lot. Here is a page in the docs with more info: https://docs.julialang.org/en/v1/manual/performance-tips/#man-code-warntype.",
              "createdAt": "2024-07-13T13:54:27Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Now, for passing a constant dataset, you could set a constant global variable in Julia, and then access those from your loss function? For example:\r\n\r\n```python\r\nfrom pysr import jl\r\n# ^jl is the Julia runtime\r\n\r\ncreate_const = jl.seval(\"(const_name, ar) -> @eval const $(Symbol(const_name)) = convert(Array, $ar)\")\r\n```\r\n\r\nThis is a function in Julia, callable from Python, that creates constants with name `const_name` and value `ar` (converted to an array).\r\n\r\nMake sure to convert the numpy array you pass this to the right type using e.g., `.astype(np.float32)`. for example:\r\n\r\n```python\r\ncreate_const(\"my_constant\", np.random.randn(100).astype(np.float32))\r\n\r\njl.my_constant  # Now accessible within Julia\r\n\r\n# Including within functions:\r\njl.seval(\"\"\"\r\nfunction my_fnc()\r\n    return my_constant\r\nend\r\n\"\"\")\r\njl.my_fnc()\r\n```\r\n\r\nTherefore, you can use \"`my_constant`\" directly within the loss.\r\n\r\nJust make sure your loss function creates a copy of the constant globals so it doesn't modify them. e.g., `loc_r_EIs = copy(r_EIs)`.\r\n\r\n",
              "createdAt": "2024-07-13T14:04:25Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0xM1QxNTowNDoyNSswMTowMM4AmS-m"
          }
        }
      }
    }
  }
}