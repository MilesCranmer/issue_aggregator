{
  "data": {
    "repository": {
      "discussion": {
        "number": 947,
        "title": "Confidence intervals",
        "body": "### Background \r\n\r\nOne of the graduate students we are sponsoring has introduced me to this library and I am intrigued by it.  My background is in Experimental Nonlinear Dynamics - though that was a long time ago.  I have played with the library on some simple model data and I am impressed by the outcomes.   However, I have this nagging question about error-bars on the coefficients that are reported.   I have some ideas how to answer it via Monte-Carlo approaches etc, or even from the definition of **score**.   But feel I can't be the only one to be thinking about this.  I have done some background reading but feel like I am either just missing something obvious. \r\n\r\n\r\n### Trivial Example code\r\nThis is a trivial code example of what I am talking about \r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.pyplot import style\r\nfrom scipy.integrate import solve_ivp\r\nfrom scipy.optimize import curve_fit\r\nfrom pysr import PySRRegressor\r\n\r\nlina = 2\r\ncons = 5\r\n\r\nN = 100\r\n\r\n\r\nx = np.linspace(0, 1, N)\r\ntesty = lina*x + cons + np.random.normal(0, 0.1, N)\r\n\r\ntestx = x.reshape(-1, 1)\r\n\r\n\r\n\r\n\r\nmodel = PySRRegressor(\r\n    maxsize=20,\r\n    niterations=100,  \r\n    binary_operators=[\"*\",\"+\"], \r\n    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\r\n    model_selection=\"best\"\r\n)\r\n\r\n\r\nmodel.fit(testx, testy)\r\n\r\n\r\nplt.plot(testx,testy, 'o', label='data')\r\nplt.plot(testx, model.predict(testx), label='prediction $ y='+model.latex() + '$')\r\nplt.xlabel('x')\r\nplt.ylabel('y')\r\nplt.legend()\r\n```\r\n\r\nThis gives an almost perfect estimate of the equation I put in, despite the random scatter of the data.   Which to be honest I was shocked by and made me dig deeper.  Why choose a linear equation when a 100th order polynomial would give a 0 loss and was available ?    The answer as you will all know is complexity bias.    Brilliant ! \r\n\r\nBut... How do we sensibly discuss the error of the coefficients in this brave new world (well it is new to me :))\r\n\r\n\r\n### Traditional Regression approach \r\n\r\nI could of course fit the final equation using traditional regression techniques \r\n\r\n```python\r\ndef func(x, a, b):\r\n    return a * x + b \r\n \r\npopt, pcov = curve_fit(func, x, testy)\r\n\r\nprint(popt)\r\nprint(np.sqrt(np.diag(pcov)))\r\n```\r\n\r\nWhich gives me a estimates of $a = 1.99 \\pm 0.04$ and $b = 5.00 \\pm 0.02$ respectively which is certainly a credible result.   But... philosophically I feel like this approach has not quite captured the uncertainty (if that is the right word) in the complexity of the chosen model.      What _if_ there is a  nearby model with the form $y =  ax^2 + bx + c$ with a low loss.   If I repeat the traditional analysis with a quadratic equation.\r\n\r\n```python\r\ndef func2(x, a, b,c):\r\n    return a * x*x + b*x + c \r\n \r\npopt, pcov = curve_fit(func2, x, testy)\r\nprint(popt)\r\nprint(np.sqrt(np.diag(pcov)))\r\n```\r\n\r\nI get \r\n$a = 0.06 \\pm 0.13  b =  1.93 \\pm 0.14  c = 5.00 \\pm 0.03$\r\n\r\nthe errors in first coefficient that quadratic is too many degrees of freedom for this regression.   A similar analysis for y = c gives $c=5.99 \\pm 0.06$.\r\n\r\nThe traditional regression would choose the linear based on various test, like reduced chi-squared.\r\n\r\n### Finally the Question!\r\n\r\nWhat is the correct way of describing confidence using symbolic regression?  I guess I want to be able to say the answer is of a certain form with a given level of confidence.      \r\n",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "There is really no good way to do this. I think ultimately you should just go with _something_ that seems reasonable to you. Even the \"score\" metric that looks for drops in log-loss-vs-complexity is pretty arbitrary and nowhere near universally-applicable.\r\n\r\nA lot of the traditional approaches to getting uncertainty begin to fail when you bring in the symbolic search space and combine it with the space of real numbers. For example, take Bayesian evidence. Bayesian evidence integrates the likelihood of the model against the prior over your coefficients. But... what should the prior over coefficients be? Because, if you take _any_ prior (even a uniform prior!), it will result in nonsensical biases towards particular models. For example, `x * [constant]` might be preferred over `x / [constant]`, even though they are symbolically identical – purely because an arbitrary prior landed more probability mass on one of these models – giving an arbitrary result.\r\n\r\n(_By the way, Bayesian evidence is how a lot of traditional measures of complexity are derived, like AIC/BIC https://en.wikipedia.org/wiki/Bayesian_information_criterion. These are basically just Bayesian evidence, Taylor expanded with Gaussian priors – arbitrary! Using # of parameters as a measure of complexity is also totally arbitrary, because you can do things like this: https://arxiv.org/abs/1904.12320._)\r\n\r\nSo, picking some empirically-backed heuristic is actually the _best_ we can do. There are a bunch of pseudo-mathematical approaches one could try to make an argument for, but in the end, it's all arbitrary. I really think the best thing to do is simply mock up some toy problems for your domain area, generate realistic noise and dataset sizes, and see if your heuristic gives you something reasonable in terms of model selection or confidence interval. I don't think there is any universal heuristic or measure. (I mean, if we want to get philosophical, even mathematical operators themselves are quite arbitrary! The only reason `+` seems simple to us is because it is familiar (and its familiar because it is widely useful). But it is actually quite an abstract concept.)\r\n\r\nYou can also try conformal prediction. There's an example here linking backend of PySR (SymbolicRegression.jl) to a conformal prediction package in Julia: https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl (scroll down). I think it's a nice approach. Very empirical.\r\n\r\nSo, in summary: just pick something that works by experimenting on toy problems close to your domain.",
              "createdAt": "2025-05-29T11:39:17Z"
            },
            {
              "author": {
                "login": "pukpr"
              },
              "body": "It seems that confidence intervals are being replaced by cross-validation approaches in many machine learning applications -- as it replaces a questionable/debatable numerical value with something that directly tells you that you are going in the right direction.\r\n\r\nConsider the idea of confidence intervals in modeling some behavior. At one time a rule-of-thumb was that a normal C.I. estimate could be generated by considering the degrees of freedom. Yet, consider the analysis of tides for tidal prediction, a purely fitted model.  A handful of tidal factor periodicities does a fairly decent job of calibrating a time-series, but it's possible to add a couple hundred more tidal factors from the table of tidal cycles.  This will indeed make the tidal analysis more accurate but it becomes increasingly difficult to calculate a realistic error estimate, as it could also lead to over-fitting if there is some noise in the data.\r\n\r\nHow does ChatGPT respond to the prompt per the above paragraph: https://chatgpt.com/share/6838dab1-c474-8010-a942-87912469b0db\r\n\r\nIn summary it does respond with alternatives such as the aforementioned Bayesian metric, bootstrapped MC approach, regularization techniques such as Lasso, and the cross-validation that I mention.",
              "createdAt": "2025-05-29T22:27:27Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyNS0wNS0yOVQyMzoyNzoyNyswMTowMM4AyyvK"
          }
        }
      }
    }
  }
}