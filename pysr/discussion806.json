{
  "data": {
    "repository": {
      "discussion": {
        "number": 806,
        "title": "Utilising PySR on matrix input data",
        "body": "Hi! I have been trying to modify PySR for an application where my input features are pairs of matrices. That is, my input data is a list of pairs of matrices, say [(A1, B1), (A2, B2),....]. In general, the size of the matrices can vary. I am searching for symbolic expressions between A and B and want to utilize methods within Julia's LinearAlgebra package (there are predefined functions for determinant, trace etc.).  As an example, my input data has the following form\r\n\r\n```julia\r\nx1 = Matrix{Float64}[]\r\nx2 = Matrix{Float64}[]\r\nfor i in 1:10\r\n    push!(x1, rand(1:10, 2, 2))\r\n    push!(x2, rand(1:10, 2, 2))\r\nend\r\n\r\nX = DataFrame(x1=x1, x2=x2)\r\nX = Tables.columntable(X)\r\n```\r\n\r\nand I would like to use the following operators for example\r\n\r\n```julia\r\nunary = [-, LinearAlgebra.det]\r\nbinary = [+, *, /]\r\n```\r\nand potential output symbolic expressions of the form: y = x1 + LinearAlgebra.det(x2) \r\n\r\nI have been running into issues since the scitype of my input doesn't match the model specifications for SRRegressor. Is this possible to allow through a custom implementation? Or even passing the input data as a concatenation of vectors along with indices and then reconstructing my matrices while computing loss in a custom loss function?\r\n\r\nMy current error: \r\n\r\n```julia\r\n┌ Error: Problem fitting the machine machine(SRRegressor(defaults = nothing, …), …). \r\n└ @ MLJBase ~/.julia/packages/MLJBase/7nGJF/src/machines.jl:694\r\n[ Info: Running type checks... \r\n┌ Warning: The number and/or types of data arguments do not match what the specified model\r\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\r\n│ \r\n│ Run `@doc SymbolicRegression.SRRegressor` to learn more about your model's requirements.\r\n│ \r\n│ Commonly, but non exclusively, supervised models are constructed using the syntax\r\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\r\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\r\n│ sample or class weights.\r\n│ \r\n│ In general, data in `machine(model, data...)` is expected to satisfy\r\n│ \r\n│     scitype(data) <: MLJ.fit_data_scitype(model)\r\n│ \r\n│ In the present case:\r\n│ \r\n│ scitype(data) = Tuple{ScientificTypesBase.Table{AbstractVector{AbstractMatrix{ScientificTypesBase.Continuous}}}, AbstractVector{ScientificTypesBase.Continuous}}\r\n│ \r\n│ fit_data_scitype(model) = Union{Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector}, Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}\r\n└ @ MLJBase ~/.julia/packages/MLJBase/7nGJF/src/machines.jl:237\r\n[ Info: It seems an upstream node in a learning network is providing data of incompatible scitype. See above. \r\nERROR: MethodError: no method matching equation_search(::Matrix{…}, ::Vector{…}; niterations::Int64, weights::Nothing, variable_names::Vector{…}, display_variable_names::Vector{…}, options::Options{…}, parallelism::Symbol, numprocs::Nothing, procs::Nothing, addprocs_function::Nothing, heap_size_hint_in_bytes::Nothing, worker_imports::Nothing, runtests::Bool, saved_state::Nothing, return_state::Bool, run_id::Nothing, loss_type::DataType, X_units::Nothing, y_units::Nothing, verbosity::Int64, extra::@NamedTuple{}, logger::Nothing, v_dim_out::Val{…})\r\nThe function `equation_search` exists, but no method is defined for this combination of argument types.\r\n\r\nClosest candidates are:\r\n  equation_search(::AbstractMatrix{T}, ::AbstractVector; kw...) where T<:Number\r\n   @ SymbolicRegression ~/Desktop/research_projects_coding/matrix-op-dynamic-expressions/SymbolicRegression.jl/src/SymbolicRegression.jl:503\r\n  equation_search(::Vector{D}; options, saved_state, runtime_options, runtime_options_kws...) where {T<:Number, L<:Real, D<:(Dataset{T, L, AX} where AX<:AbstractMatrix{T})}\r\n   @ SymbolicRegression ~/Desktop/research_projects_coding/matrix-op-dynamic-expressions/SymbolicRegression.jl/src/SymbolicRegression.jl:513\r\n  equation_search(::Dataset; kws...)\r\n   @ SymbolicRegression ~/Desktop/research_projects_coding/matrix-op-dynamic-expressions/SymbolicRegression.jl/src/SymbolicRegression.jl:509\r\n  ...\r\n\r\n```\r\n\r\nReading the new version of DynamicExpressions.jl, it seemed to me that _eval_tree_array_generic should allow for input and output data of arbitrary type\r\n\r\n\r\n> Evaluate a generic binary tree (equation) over a given input data,\r\n> whatever that input data may be. The `operators` enum contains all\r\n> of the operators used. Unlike `eval_tree_array` with the normal\r\n> `OperatorEnum`, the array `cX` is sliced only along the first dimension.\r\n> i.e., if `cX` is a vector, then the output of a feature node\r\n> will be a scalar. If `cX` is a 3D tensor, then the output\r\n> of a feature node will be a 2D tensor.\r\n> Note also that `tree.feature` will index along the first axis of `cX`.\r\n> \r\n> However, there is no requirement about input and output types in general.\r\n> You may set up your tree such that some operator nodes work on tensors, while\r\n> other operator nodes work on scalars. `eval_tree_array` will simply\r\n> return `nothing` if a given operator is not defined for the given input type.\"\r\n\r\nThank you again for the wonderful package!",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "So the first thing that needs to be changed is to use a `GenericOperatorEnum` instead of an `OperatorEnum`. You can see the examples at the bottom of DynamicExpressions.jl README which demonstrates some examples on arbitrary types: https://github.com/SymbolicML/DynamicExpressions.jl.\r\n\r\nHowever, this still won't work out-of-the-box in SymbolicRegression.jl, because of the fact that https://github.com/MilesCranmer/SymbolicRegression.jl/blob/9fabc303dd33c624739e759d960374c7f85e56f7/src/ProgramConstants.jl#L5 is set:\r\n\r\n```julia\r\nconst DATA_TYPE = Number\r\n```\r\n\r\nMeaning that all elements of `X` are constrained to be a `Number`. The reason this is set is because I simply haven't tried to get such a thing working yet, so I didn't want to have it working out-of-the-box for fear of people encountering some sort of bug. But in principle, it should be possible.\r\n\r\nSo the first to do is checkout a local version of SymbolicRegression.jl and make a change to:\r\n\r\n```julia\r\nconst DATA_TYPE = Any\r\n```\r\nand then see what functions need to be updated to accommodate it. Hopefully not _too_ many!",
              "createdAt": "2025-01-13T17:50:15Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyNS0wMS0xM1QxNzo1MDoxNSswMDowMM4AtGfN"
          }
        }
      }
    }
  }
}