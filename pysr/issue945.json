{
  "data": {
    "repository": {
      "issue": {
        "number": 945,
        "title": "Getting a uft-8 Error",
        "body": "\n### Discussed in https://github.com/MilesCranmer/PySR/discussions/944\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **AaronRoseDA** May 28, 2025</sup>\n\n@AaronRoseDA\n\nHi Miles,\n\nI'm really struggling to deal with a particular utf-8 encoding issue. I have no problems with any other dataset so I think it might be the data frame but I can't find anything that stands out in the data:\n\n```\nC:\\Users\\arose\\.conda\\envs\\test-env\\Lib\\site-packages\\pysr\\sr.py:2774: UserWarning: Note: it looks like you are running in Jupyter. The progress bar will be turned off.\n  warnings.warn(\n[ Info: Started!\n[ Info: Final population:\n[ Info: Results saved to:\nElapsed time: 64.96 seconds\nError in callback _flush_stdio (for post_execute), with arguments args (),kwargs {}:\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nFile ~\\.julia\\packages\\PythonCall\\WMWY0\\src\\JlWrap\\any.jl:262, in __call__(self, *args, **kwargs)\n    260     return ValueBase.__dir__(self) + self._jl_callmethod($(pyjl_methodnum(pyjlany_dir)))\n    261 def __call__(self, *args, **kwargs):\n--> 262     return self._jl_callmethod($(pyjl_methodnum(pyjlany_call)), args, kwargs)\n    263 def __bool__(self):\n    264     return True\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 4095: unexpected end of data\n```\nI've narrowed it down to a few things but I can't put my finger on the exact reason. It could be something wrong with my data frame but I can't find anything that stands out. \n\nI've tried pairing down the model as much as possible and here's my model setup:\n```\n# Define input (X) and target (y)\nX = data[feature_names].values\n# y = data[\"entropy\"].values\ny = data[\"entropy_kde_XY\"].values\n# y = data[\"entropy_theoretical_XY\"].values\n\n\nmodel = PySRRegressor(\n    model_selection=\"best\",\n    run_id=f'Bi-Skew-Normal - {datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S.%f\")[:-4]}',\n    niterations=10000000,\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n    unary_operators=[\"log\", \"sqrt\", \"square\",\"exp\",\"cube\",\"cbrt\"\n                     # , \"sin\", \"cos\", \"tan\"\n                     ],\n    nested_constraints = {\n    \"log\": {\"log\": 0},\n    \"sqrt\": {\"sqrt\": 0},\n    \"square\": {\"square\": 0},\n    \"exp\": {\"exp\": 0},\n    \"cube\": {\"cube\": 0},\n    \"cbrt\": {\"cbrt\": 0}\n    },\n    populations=48,\n    ncycles_per_iteration=5000,\n    # batching=True,\n    # weight_optimize=.005,\n    # turbo=True,\n    \n    maxsize=26,\n    warm_start=True,\n    temp_equation_file=False,\n    output_directory=r\"C:/Users/arose/OneDrive/Desktop/output/BivariateSkewNormal\",\n    verbosity=1,\n    timeout_in_seconds=60,\n    complexity_of_constants=1,\n    # logger_spec=logger_spec\n    # early_stop_condition=\"f(loss, complexity) = (loss <= 5.526e-15) && (complexity < 6)\",print_precision=24\n)\n\n# model.weight_optimize = .005\n# model.maxsize = 26\n# model.ncycles_per_iteration = 5000\n# model.timeout_in_seconds = 120\n# model.print_precision = 16\n# model.model_selection = 'accuracy'\n# model.unary_operators = [\"log\", \"square\"]\n# model.populations = 96\n\n\n\nst = time.time()\nmodel.fit(X, y, variable_names=feature_names)\nruntime = round(time.time() - st, 2)\nprint(f\"Elapsed time: {runtime} seconds\")\n\n```\n\nAny expertise you can provide would be immensely helpful. I'm going to take a deeper dive into my data and see if I can find anything that stands out in my data.</div>",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "@AaronRoseDA\nThanks for the report. Is it possible at all to share the dataset, or maybe an example dataset that generates the same bug?\n\nI guess I'm wondering if there is some variable name with a special character that could be causing it.",
              "createdAt": "2025-05-28T15:56:43Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Also does it help if you disable the `print_precision` parameter setting?",
              "createdAt": "2025-05-28T15:57:35Z"
            },
            {
              "author": {
                "login": "AaronRoseDA"
              },
              "body": "Hey @MilesCranmer, \n\nI played around trying to reproduce the error and I can confirm it's **not** the data frame and probably not the ```print_precision``` parameter. \n\nI tested a few different cases today and I believe the issue lies somewhere with how ```verbosity``` and ```logger_spec``` interact.\n\nThe easiest way to get the error was running the fit call below three times while ```warm_start = True```. Usually the error happened between runs but that's not always the case:\n\n```\nmodel.fit(X, y, variable_names=feature_names)\n```\n\nIn case 1, ```verbosity = 1``` and ```logger_spec = None``` the code executes as expected. The three runs work with no issues.\n\nIn case 2, ```verbosity = 0``` and ```logger_spec = logger_spec``` the code also runs great. I can monitor the progress on Tensorboard with no issues.\n\nIn case 3, ```verbosity = 1``` and ```logger_spec = logger_spec``` and I **_don't_** launch TensorBoard the model will run twice or start the third run and produce the utf-8 error. When I get this error my whole python kernel gets screwed up and even running something like ```a = \"hello\"``` in the console causes the same error to repeat and sometimes the python kernel dies:\n```\nError in callback _flush_stdio (for post_execute), with arguments args (),kwargs {}:\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nFile ~\\.julia\\packages\\PythonCall\\L4cjh\\src\\JlWrap\\any.jl:262, in __call__(self, *args, **kwargs)\n    260     return ValueBase.__dir__(self) + self._jl_callmethod($(pyjl_methodnum(pyjlany_dir)))\n    261 def __call__(self, *args, **kwargs):\n--> 262     return self._jl_callmethod($(pyjl_methodnum(pyjlany_call)), args, kwargs)\n    263 def __bool__(self):\n    264     return True\n\nUnicodeDecodeError: 'utf-8' codec can't decode bytes in position 4094-4095: unexpected end of data\n\na = \"test\"\nError in callback _flush_stdio (for post_execute), with arguments args (),kwargs {}:\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nFile ~\\.julia\\packages\\PythonCall\\L4cjh\\src\\JlWrap\\any.jl:262, in __call__(self, *args, **kwargs)\n    260     return ValueBase.__dir__(self) + self._jl_callmethod($(pyjl_methodnum(pyjlany_dir)))\n    261 def __call__(self, *args, **kwargs):\n--> 262     return self._jl_callmethod($(pyjl_methodnum(pyjlany_call)), args, kwargs)\n    263 def __bool__(self):\n    264     return True\n\nUnicodeDecodeError: 'utf-8' codec can't decode bytes in position 4094-4095: unexpected end of data\n```\n\nIn case 4, where ```verbosity = 1``` and ```logger_spec = logger_spec``` and I **_am_** watching progress in TensorBoard I don't get the same utf-8 error, however, the model stalls somewhere during or after the second run. My CPU usage goes to 0% and the code just spins at this point:\n\n```\nPress 'q' and then <enter> to stop execution early.\n\nExpressions evaluated per second: 1.020e+05\nProgress: 297 / 480000000 total iterations (0.000%)\n═══════════════\nElapsed time: 97.07 seconds\n[ Info: Final population:\n[ Info: Results saved to:\nC:\\Users\\arose\\.conda\\envs\\sr-env\\Lib\\site-packages\\pysr\\sr.py:2811: UserWarning: Note: it looks like you are running in Jupyter. The progress bar will be turned off.\n  warnings.warn(\n```\n\nHere's my full model def incase anything stands out:\n\n```\nlogger_spec = TensorBoardLoggerSpec(\n    # log_dir=r\"C:/Users/arose/OneDrive/Desktop/output/BivariateSkewNormal_testCase/logger\",\n    log_dir=r\"C:/Users/arose/output/BivariateSkewNormal_testCase/logger\",\n\n    log_interval=10\n)\n\nmodel = PySRRegressor(\n    model_selection=\"best\",\n    run_id=f'Bi-Skew-Normal-{datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S.%f\")[:-4]}',\n    niterations=10000000,\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n    unary_operators=[\"log\", \"sqrt\", \"square\",\"exp\",\"cube\",\"cbrt\"],\n    nested_constraints = {\n      \"log\": {\"log\": 0},\n      \"sqrt\": {\"sqrt\": 0},\n      \"square\": {\"square\": 0},\n      \"exp\": {\"exp\": 0},\n      \"cube\": {\"cube\": 0},\n      \"cbrt\": {\"cbrt\": 0}\n    },\n    populations=48,\n    ncycles_per_iteration=5000,    \n    maxsize=26,\n    warm_start=True,\n    temp_equation_file=False,\n    output_directory=r\"C:/Users/arose/output/BivariateSkewNormal_testCase\",\n    timeout_in_seconds=90,\n    complexity_of_constants=1,\n\n    # 1:\n    #verbosity=1,\n    #logger_spec=None\n    # 2:\n    #verbosity=0,\n    #logger_spec=logger_spec\n    # 3 & 4 - Error Case:\n    verbosity=1,\n    logger_spec=logger_spec\n    #\n)\n```\n\nIn conclusion I'm not sure whether ```warm_start``` has anything to do with the issue OR if I ran the model in a single trial for long enough I would get the same issue. \n\nI have noticed that the output from ```verbosity``` lags far behind the updates observed on TensorBoard though which I don't know if thats by design or a limitation.\n\nMy environment is running python 3.12.7, pySR 1.5.8, and Tensorboard 2.19.0.\n\nHopefully, you can reproduce the same error and it's not just something on my end! In the meantime I'll turn verbosity off and hopefully that solves the issue. \n\nThanks!\n",
              "createdAt": "2025-05-28T23:12:34Z"
            },
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "Thanks. Very weird indeed. But good to know it is related to the tensorboard logging. Do you know if any other things are correlated with the error? e.g., if you remove all the parameters _except for_ verbosity and logger_spec, does it still hit the error? (i.e., a \"minimal reproducible example\": https://stackoverflow.com/help/minimal-reproducible-example)",
              "createdAt": "2025-05-29T18:23:43Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpHOrg8C2w=="
          }
        }
      }
    }
  }
}