{
  "data": {
    "repository": {
      "discussion": {
        "number": 977,
        "title": "Dimensionless output with TemplateExpressionSpec",
        "body": "Hi all, \r\nI need help regarding how to force the target expressions to be dimensionless, given that I must use TemplateExpressionSpec , so I can not use units since this feature is not available yet to my understanding. I am trying to find 3 coefficients that when each of them multiplied by other known expressions that I include in  TemplateExpressionSpec , and added together form a vector. My question consists of three parts:\r\n\r\n1. I want to confirm that this approach is correct\r\n I am defining my y as just a vector of zeros , since in the TemplateExpressionSpec  I calculate the output y as follows \r\n\r\n```\r\nX = np.array(dataset)\r\ny=np.zeros(len(dataset))\r\n\r\ntemplate = TemplateExpressionSpec(\r\n    expressions=[ \"c_6\", \"c_7\", \"c_8\"],\r\n    variable_names=[\"x_1\", \"x_2\", \"x_3\", \"x_4\" , \"s\", \"m\", \"r\", \"s_1\", \"s_2\", \"s_3\", \"s_4\"],\r\n    combine=f\"\"\"\r\n        abs2(s_1- (c_6(x_1, x_2, x_3, x_4)*({c_6_expressions[0]}) + c_7(x_1, x_2, x_3, x_4)*({c_7_expressions[0]}) +\r\n              c_8(x_1, x_2, x_3, x_4)*({c_8_expressions[0]})))+ \r\n\r\n        abs2(s_2- (c_6(x_1, x_2, x_3, x_4)*({c_6_expressions[1]}) + c_7(x_1, x_2, x_3, x_4)*({c_7_expressions[1]}) +\r\n              c_8(x_1, x_2, x_3, x_4, y_1, y_2)*({c_8_expressions[1]})))+ \r\n\r\n        abs2(s_3- (c_6(x_1, x_2, x_3, x_4)*({c_6_expressions[2]}) + c_7(x_1, x_2, x_3, x_4)*({c_7_expressions[2]}) +\r\n              c_8(x_1, x_2, x_3, x_4)*({c_8_expressions[2]})))+ \r\n\r\n        abs2(s_4- (c_6(x_1, x_2, x_3, x_4)*({c_6_expressions[3]}) + c_7(x_1, x_2, x_3, x_4)*({c_7_expressions[3]}) +\r\n              c_8(x_1, x_2, x_3, x_4)*({c_8_expressions[3]})))\r\n    \"\"\"\r\n)\r\n```\r\n\r\nwith this loss function,\r\n\r\n```\r\nelementwise_loss = \"\"\"function loss_fnc(prediction, target)\r\n    scatter_loss = abs(log((abs(prediction)+1e-20) / (abs(target)+1e-20)))\r\n    sign_loss = 10 * (sign(prediction) - sign(target))^2\r\n    return scatter_loss + sign_loss\r\nend\r\n```\r\n\r\n2. I want to force `c_6` , `c_7` and `c_8` to be unitless, given that all their arguments are in _mm_, so each expression must be a ratio where the nominator and denominator are of the same power. I also want to make sure that constants are only multiplied or divided not added, so I dont want such expressions for instance `c_6=#4/#3 + 1.234` \r\n\r\n\r\n3. Finally, if I doubt that the  answer might include such parts `x_1-x_2` or `x_3+x_4` is there a way to add this as an inductive bias so there is more chance to use these specific variables, not any pairs?\r\n\r\nThank you!!",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "For dimensional analysis on template expressions you could check out https://github.com/MilesCranmer/PySR/discussions/972.\r\n\r\n@ibengtsson maybe you would have some advice on this?",
              "createdAt": "2025-07-08T11:13:47Z"
            },
            {
              "author": {
                "login": "dka-3"
              },
              "body": "Thank you, I think I am close to it now, however, I followed #972 , just changed `ex.trees.expression_name` to `ex.trees.expression_name.tree` and `y_units` from `[u\"1\"]` to `u\"1\"`, `dataset` to `dataset.X` ,now I get this error in the method `violates_dimensional_constraints_dispatch`\r\n\r\n\r\n```\r\nJuliaError: MethodError: no method matching violates_dimensional_constraints_dispatch(::Node{Float32}, ::Vector{Quantity{Float64, Dimensions{FixedRational{Int32, 25200}}}}, ::Vector{Float32}, ::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, ::Bool)\r\n\r\nClosest candidates are:\r\n  violates_dimensional_constraints_dispatch(::AbstractExpressionNode{T}, !Matched::Vector{Q}, ::AbstractVector{T}, ::Any, ::Any) where {T, Q<:(AbstractQuantity{T})}\r\n   @ SymbolicRegression ~/.julia/packages/SymbolicRegression/L5TJa/src/DimensionalAnalysis.jl:157\r\n\r\nStacktrace:\r\n  [1] violates_dimensional_constraints(tree::Node{Float32}, X_units::Vector{Quantity{Float64, Dimensions{FixedRational{Int32, 25200}}}}, y_units::Quantity{Float64, Dimensions{FixedRational{Int32, 25200}}}, x::Vector{Float32}, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Int64}, OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, Node, TemplateExpression, @NamedTuple{structure::TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}}, MutationWeights, false, false, nothing, Nothing, 5})\r\n    @ SymbolicRegression.DimensionalAnalysisModule ~/.julia/packages/SymbolicRegression/L5TJa/src/DimensionalAnalysis.jl:208\r\n  [2] (::var\"#checker#356\")(::Node{Float32}, ::Vararg{Any})\r\n    @ Main ./none:11\r\n  [3] my_loss(ex::TemplateExpression{Float32, TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}, Node{Float32}, ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, @NamedTuple{c_6::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, c_7::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, c_8::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}}, @NamedTuple{structure::TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}, operators::OperatorEnum{Tuple{typeof(+), typeof(*), t...\r\n    @ Main ./none:12\r\n  [4] evaluator(f::typeof(my_loss), tree::TemplateExpression{Float32, TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}, Node{Float32}, ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, @NamedTuple{c_6::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, c_7::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, c_8::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}}, @NamedTuple{structure::TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}, operators::OperatorEnum{Tuple...\r\n    @ SymbolicRegression.LossFunctionsModule ~/.julia/packages/SymbolicRegression/L5TJa/src/LossFunctions.jl:134\r\n  [5] eval_loss(tree::TemplateExpression{Float32, TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}, Node{Float32}, ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, @NamedTuple{c_6::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, c_7::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}, c_8::ComposableExpression{Float32, Node{Float32}, @NamedTuple{operators::OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, variable_names::Nothing, eval_options::EvalOptions{false, false, true, Nothing}}}}, @NamedTuple{structure::TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}, operators::OperatorEnum{Tuple{typeof(+), typeof(*...\r\n    @ SymbolicRegression.LossFunctionsModule ~/.julia/packages/SymbolicRegression/L5TJa/src/LossFunctions.jl:153\r\n  [6] eval_loss\r\n    @ ~/.julia/packages/SymbolicRegression/L5TJa/src/LossFunctions.jl:139 [inlined]\r\n  [7] update_baseline_loss!(dataset::SymbolicRegression.CoreModule.DatasetModule.BasicDataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Int64}, OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, Node, TemplateExpression, @NamedTuple{structure::TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}}, MutationWeights, false, false, nothing, Nothing, 5})\r\n    @ SymbolicRegression.LossFunctionsModule ~/.julia/packages/SymbolicRegression/L5TJa/src/LossFunctions.jl:225\r\n  [8] _validate_options(datasets::Vector{SymbolicRegression.CoreModule.DatasetModule.BasicDataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}, ropt::SymbolicRegression.SearchUtilsModule.RuntimeOptions{:serial, 1, true, Nothing}, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Int64}, OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, Node, TemplateExpression, @NamedTuple{structure::TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}}, MutationWeights, false, false, nothing, Nothing, 5})\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/L5TJa/src/SymbolicRegression.jl:597\r\n  [9] _equation_search(datasets::Vector{SymbolicRegression.CoreModule.DatasetModule.BasicDataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}, ropt::SymbolicRegression.SearchUtilsModule.RuntimeOptions{:serial, 1, true, Nothing}, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Int64}, OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, Node, TemplateExpression, @NamedTuple{structure::TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}}, MutationWeights, false, false, nothing, Nothing, 5}, saved_state::Nothing)\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/L5TJa/src/SymbolicRegression.jl:567\r\n [10] equation_search(datasets::Vector{SymbolicRegression.CoreModule.DatasetModule.BasicDataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}; options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Int64}, OperatorEnum{Tuple{typeof(+), typeof(*), typeof(-), typeof(/)}, Tuple{typeof(square), typeof(safe_sqrt)}}, Node, TemplateExpression, @NamedTuple{structure::TemplateStructure{(:c_6, :c_7, :c_8), (), typeof(__sr_template_4727212924912217469), @NamedTuple{c_6::Int64, c_7::Int64, c_8::Int64}, @NamedTuple{}}}, MutationWeights, false, false, nothing, Nothing, 5}, saved_state::Nothing, runtime_options::Nothing, runtime_options_kws::@Kwargs{niterations::Int64, parallelism::String, numprocs::Nothing, procs::Nothing, addprocs_function::Nothing, heap_size_hint_in_bytes::Nothing, worker_imports::Nothing, runtests::Bool, return_state::Bool, run_id::String, verbosity::Int64, logger::Nothing, progress::Bool, v_dim_out::Val{1}})\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/L5TJa/src/SymbolicRegression.jl:561\r\n [11] equation_search\r\n    @ ~/.julia/packages/SymbolicRegression/L5TJa/src/SymbolicRegression.jl:542 [inlined]\r\n [12] #equation_search#23\r\n    @ ~/.julia/packages/SymbolicRegression/L5TJa/src/SymbolicRegression.jl:511 [inlined]\r\n [13] equation_search\r\n    @ ~/.julia/packages/SymbolicRegression/L5TJa/src/SymbolicRegression.jl:456 [inlined]\r\n [14] #equation_search#24\r\n    @ ~/.julia/packages/SymbolicRegression/L5TJa/src/SymbolicRegression.jl:535 [inlined]\r\n [15] pyjlany_call(self::typeof(equation_search), args_::Py, kwargs_::Py)\r\n    @ PythonCall.JlWrap ~/.julia/packages/PythonCall/L4cjh/src/JlWrap/any.jl:44\r\n [16] _pyjl_callmethod(f::Any, self_::Ptr{PythonCall.C.PyObject}, args_::Ptr{PythonCall.C.PyObject}, nargs::Int64)\r\n    @ PythonCall.JlWrap ~/.julia/packages/PythonCall/L4cjh/src/JlWrap/base.jl:73\r\n [17] _pyjl_callmethod(o::Ptr{PythonCall.C.PyObject}, args::Ptr{PythonCall.C.PyObject})\r\n    @ PythonCall.JlWrap.Cjl ~/.julia/packages/PythonCall/L4cjh/src/JlWrap/C.jl:63\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n```\r\ncustom_loss =\"\"\"\r\nfunction my_loss(ex::TemplateExpression, dataset::Dataset{T,L}, options) where {T,L}\r\n    prediction, complete = eval_tree_array(ex, dataset.X, options)\r\n    if !complete\r\n        return L(Inf)\r\n    end\r\n\r\n    c_6 = ex.trees.c_6.tree\r\n    c_7 = ex.trees.c_7.tree\r\n    c_8 = ex.trees.c_8.tree\r\n\r\n    checker(args...) = SymbolicRegression.DimensionalAnalysisModule.violates_dimensional_constraints(args...)\r\n    c_6_violates = checker(\r\n        c_6,\r\n        [u\"mm\", u\"mm\", u\"mm\" , u\"mm\"], \r\n        u\"1\", \r\n        ones(T, 4),  \r\n        options\r\n    )\r\n    c_7_violates = checker(\r\n        c_7,\r\n        [u\"mm\", u\"mm\", u\"mm\" , u\"mm\"], \r\n        u\"1\", \r\n        ones(T, 4),  \r\n        options\r\n    )\r\n    c_8_violates = checker(\r\n        c_8,\r\n        [u\"mm\", u\"mm\", u\"mm\" , u\"mm\"], \r\n        u\"1\", \r\n        ones(T, 4),  \r\n        options\r\n    )\r\n\r\n    #Define some penalty for the dimensional violations from checkers\r\n\r\n    return #some_loss\r\n    end\r\n\"\"\"\r\n```",
              "createdAt": "2025-07-09T14:04:16Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyNS0wNy0wOVQxNjowNDoxNiswMjowMM4A0TGd"
          }
        }
      }
    }
  }
}