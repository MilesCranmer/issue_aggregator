[
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 693,
          "title": "SR.jl to include the constant length, variables to be present in equations and previous state information to be used in current state",
          "body": "Hello, everyone. I am using SR.jl for fluid dynamics application and need my SR to work with some constraints:\r\n\r\n1. The constants generated are currently in the range of e-16,  and the value is 0. I want to constrain SRRegressor so that the constant value should be of a particular length. Also, the equations primarily use constants rather than variables.\r\n2. I would like all the variables in equations since all the inputs impact the output feature. Currently, only 1 or 2 variables are used in the equations. A lower loss value is achieved but hardly has any input variable, for example:\r\n\r\n> Complexity,Loss,Equation\r\n> 18,1.5375358344591114e-7,\"(0.003040617289178417 + 0.003040617289178417) / (sqrt(x₁) + ((2.009394600234112 + x₁) + 0.003040617289178417))\"\r\n\r\n3. Is there any feature in SR.jl with previous state information to predict the current state, somewhat like an RNN?\r\n\r\nI am new to SR and Julia, so if there are any redundant questions, please let me know the most relevant solution.",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 685,
          "title": "Difficulties in rediscovering Von Neumann entropy from eigenvalue data",
          "body": "The Von Neumann equation states that the entropy of a density matrix $\\rho$ is of the form:\r\n\r\n$$S(\\rho) = -\\sum_i \\lambda_i \\ln \\lambda_i$$\r\nwhere $\\{\\lambda_i\\}$ are the eigenvalues of $\\rho$. \r\n\r\nI have 10,000+ rows of data for the eigenvalues for density matrices as they evolve over time and I have 10,000+ rows of data for the entropy of the density matrices. I have checked that the eigenvalue data matches the correct entropy value when the Von Neumann equation is applied to the eigenvalues. I am trying to use PySR to rediscover the Von Neumann entropy equation using the eigenvalue data before I try applying the package to more complicated scenarios in quantum information. \r\n\r\nI can't seem to get PySR to find the correct expression. I'm including a screenshot of its results when working with my data. In this, \"l1\" and \"l2\" are the two eigenvalues of the density matrices. My density matrices only ever have two eigenvalues. \r\n\r\n<img width=\"780\" alt=\"PySR_HOF\" src=\"https://github.com/user-attachments/assets/f61b42f8-530a-4f62-bd17-60b28ecc89da\">\r\n\r\n... I was assuming that this would have been an easy problem for PySR to solve given that I just have two columns of data and the correct expression just a sum with some natural logs attached. I'm completely new to using PySR so I am assuming that I just don't understand how to work with the package yet. This is the code that I'm using to generate the model, which I've left almost completely unedited from the example given on the repo's README file: \r\n\r\n<img width=\"1073\" alt=\"PySR_code\" src=\"https://github.com/user-attachments/assets/332b2fa5-d1e2-47ed-86c5-2c16cbd3080f\">\r\n\r\nShould I be using different operators? Is the number of iterations too small? Is it problematic to try discovering an expression that only depends on two variables? Is it a known issue that PySR struggles to predict expressions involving natural logs and sums?\r\n\r\nAny guidance would be greatly appreciated. \r\n  \r\n",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hm, yeah it should be pretty easy for it to find that. Are there any data preprocessing steps that could be generating invalid results for `y`?\r\n\r\nOut of curiosity, why not give it `+` too? It is needed for some of the simplification operators to work.\r\n\r\nAlso since it’s so simple you likely don’t need that many datapoints. Even 100 should be okay.\r\n\r\n",
                "createdAt": "2024-07-29T17:39:47Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0yOVQxODozOTo0NyswMTowMM4Am19I"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 646,
          "title": "Get the score/loss associated with ground truth equations.",
          "body": "Hi,\r\n\r\nThanks for the great work! I was wondering if there is a way to get the loss/score value associated with a known ground truth equation. For example i ran search using PySr however i found that it didn't find the correct equation, so i wanted to see the score value given to the ground truth equation that i have access to. This might be useful as i would want to know if the search couldn't find a equation with higher score or my complexity definition were bad which resulted in lower score for the ground truth equation. ",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "So the loss is specified with `elementwise_loss`. By default this is just MSE. So if you have `y` and `X` you could potentially compute `ypredicted` directly in Python, and then compute `np.sum(np.square(y - ypredicted)) / y.shape[0]`. Does that work for you?\r\n\r\nNote the \"score\" is just a metric to display users, it doesn't actually affect the search itself. The search is all based on loss, which by default is MSE.\r\n\r\nSee https://astroautomata.com/PySR/api/#the-objective for details",
                "createdAt": "2024-06-19T00:04:18Z"
              },
              {
                "author": {
                  "login": "mihirp1998"
                },
                "body": "Thanks for the reply. Although i thought score affects the best equation u pick and the best equation u pick depends on complexity of that equation? So i just wanted to see if the correct equation had a higher score (i.e a good ratio between complexity and loss)? \r\n\r\nPlz let me know if i'm getting something wrong here\r\n\r\n\r\n",
                "createdAt": "2024-06-19T01:08:12Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0xOVQwMjowODoxMiswMTowMM4AlbeC"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 689,
          "title": "Replace spaces with underscores in column names also for the predict function",
          "body": "I found that PySR warns about spaces in column names when passing the .fit function data where this occurs. It then replaces the spaces in the column names with underscores and prints a warning about this. You can then proceed with fitting the data as per normal.\r\nWhen later calling the .predict function, this does not attempt to make the same replacement of spaces with underscores in the column names.\r\nSo, if we have a fitted model and want to use it to make predictions, and we pass data to the .predict function in the same format that we used for the .fit function, we can run into the following issue:\r\nThe predict function (in sr.py) contains the following code line \"X = X.reindex(columns=self.feature_names_in_)\". This results in NaN values in case the column names have spaces, because now it tries to match the column names (with spaces) with the feature names of the model, but in the latter the spaces were replaced by underscores.\r\nWe then get the somewhat confusing message \"ValueError: Input X contains NaN.\", which leads one to believe that there are NaN values in the data even while there are none, they only get introduced by the reindex which can't match the column names.\r\n\r\nAll this can be avoided of course, once you are aware of the problem and avoid using spaces in the column names from the beginning. However, it might be more consistent, and allow for a better user experience, if the .predict function also replaces spaces in the column names with underscores?\r\n\r\n",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "I think this is a bug. I transferred to https://github.com/MilesCranmer/PySR/issues/690.",
                "createdAt": "2024-08-02T16:36:13Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wOC0wMlQxNzozNjoxMyswMTowMM4AnAil"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 631,
          "title": "[Feature]: How to get the value of a constant in a fixed expression",
          "body": "### Feature Request\n\nHi, thanks for developing the helpful tool.\r\nI want to use pysr to get the values of the constants C1 and C2 in the following fixed expression:\r\ny=(1-C1)*x1/(x2+1)+x3/(C2+1)\r\nx1,x2,x3,y are known data sets.\r\nHow should I use pysr to get the constant C1 and C2?\r\nThanks！",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "If the functional form is fixed aside from two constants, why not use scipy optimize directly? Or do you mean C1 and C2 are expressions?",
                "createdAt": "2024-05-24T08:28:38Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNS0yNFQwOToyODozOCswMTowMM4AkaKH"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 667,
          "title": "Implement mathematical constraints",
          "body": "I want to implement mathematical constraints in a soft way in Pysr. I think if I apply them in a hard way I lose a lot of functions. These constraints are like I know the positiveness or negativeness of function with respect to some values, and behavior of function derivative with respect to some variables.\r\n\r\n\r\nI find that it seems the main of your program is written in Julia. Then you wrapped it to work in Python. I tried to read \"src\" part of your code but it's hard for me because I am not familiar with Julia. So, I read the discussion part of the page and I saw this question: [Constraints?](\"https://github.com/MilesCranmer/PySR/discussions/304\") \"https://github.com/MilesCranmer/PySR/discussions/304\"\r\nIn answer, you said: \"Update: added full_objective (PySR) and loss_function (SymbolicRegression.jl) for this purpose.\"\r\n\r\nActually, I did not find \"full_objective\" part in the Python code. I found and read \"loss_function\" in Julia's part. I also read this question: \"https://github.com/MilesCranmer/PySR/discussions/449\" and this question:\"https://github.com/MilesCranmer/PySR/discussions/256\"\r\nIt seems to me you are trying to say that you can define a new loss function. I want to know: can I define this new loss criterion in Python? If I define a new loss function I must download your code manipulate it and compile it again? I see your loss criterion repeated somewhere in the code, Do I need to make changes elsewhere?",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "To implement a custom loss about function constraints, the right way is to pass a Julia function **as a string** in Python to the `loss_function ` parameter of PySRRegressor. You unfortunately cannot use a pure Python function as the loss function. This is technically possible but it would restrict you to serial computation only, due to Python’s “global interpreter lock”, and would also be very very slow as it couldn’t be compiled or differentiated (whereas Julia code can be).\r\n\r\nIf you describe your constraints exactly or provide an example in Python I can help get it working.\r\n\r\nThe examples that you cited are indeed the most relevant.",
                "createdAt": "2024-07-12T22:43:18Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0xMlQyMzo0MzoxOCswMTowMM4AmSPZ"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 651,
          "title": "Issues with Regressing Circuit Using PySR: Positive Parameters and Custom Loss Functions",
          "body": "Dear Developers,\r\n\r\nThank you for your excellent work on the PySR repository. It is truly fantastic. I have been working on regressing a ZARC circuit using the code below, but I have encountered some issues:\r\n\r\n1. The code seems unable to fit the model because it uses complex parameters (not only complex inputs) with some semi-default loss function (see below).\r\n2. When I attempt to use a custom loss function, the regression still does not work, to handle positivity as discussed in https://github.com/MilesCranmer/PySR/discussions/449 \r\n\r\nCould there be a workaround for this? Specifically, I am looking for a solution where the symbolic regression parameters are positive, but the function input and output remain complex.\r\n\r\nThank you for your kind help.\r\n\r\nP.S. This question is related but somewhat different to https://github.com/MilesCranmer/PySR/discussions/338\r\n\r\nCODE:\r\n\r\n```python \r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom pysr import PySRRegressor\r\nfrom sympy import latex, re\r\n\r\n# File name\r\nfile_name = './results/ZARC.csv'\r\n\r\n# Parameters for ZARC model\r\nR_inf, R_ct, phi, tau_0 = 10, 50, 0.8, 0.01\r\n\r\n# Compute impedance of ZARC model\r\ndef Z_ZARC(s, R_inf, R_ct, phi, tau_0):\r\n    return R_inf + R_ct / (1 + (s * tau_0)**phi)\r\n\r\n# Frequency range\r\nN_freqs = 71\r\nfreq_vec = np.logspace(-1, 6, N_freqs)\r\n\r\n# Exact impedance\r\nZ_exact = Z_ZARC(1j * 2 * np.pi * freq_vec, R_inf, R_ct, phi, tau_0)\r\n\r\n# Synthetic data with noise\r\nnp.random.seed(1225)\r\nsigma_n_exp = 0.2\r\nZ_exp = Z_exact + sigma_n_exp * (np.random.normal(0, 1, N_freqs) + 1j * np.random.normal(0, 1, N_freqs))\r\n\r\n# Normalize data\r\nZ_abs_mean, Z_abs_std = np.mean(np.abs(Z_exp)), np.std(np.abs(Z_exp))\r\nZ_normalized = (Z_exp - Z_abs_mean) / Z_abs_std\r\n\r\n\r\n# PySRRegressor model setup\r\nmodel = PySRRegressor(\r\n    equation_file=file_name,\r\n    binary_operators=[\"+\", \"-\", \"*\", \"/\", \"mypow(x,y)=(x)^(y.re)\"],\r\n    constraints={\"/\": (-1, 9)},\r\n    extra_sympy_mappings ={\"mypow\": lambda x,y: x**re(y)},\r\n    niterations=100,\r\n    maxsize=30,\r\n    maxdepth=10,\r\n    adaptive_parsimony_scaling=100.0,\r\n    precision=64,\r\n    verbosity=1,\r\n    elementwise_loss=\"f(x, y) = abs2(x - y)/abs2(x)\"\r\n)\r\n\r\n# Fit the model\r\nmodel.fit((1j * 2 * np.pi * freq_vec).reshape(-1, 1), Z_normalized)\r\n\r\n# Predict using the model\r\nZ_pred_normalized = model.predict((1j * 2 * np.pi * freq_vec).reshape(-1, 1))\r\nZ_pred = Z_abs_mean + Z_abs_std * Z_pred_normalized\r\n\r\n# Plot results\r\nplt.figure(figsize=(10, 5))\r\nplt.plot(np.real(Z_exact), -np.imag(Z_exact), '--', linewidth=2, color='blue', label='Exact')\r\nplt.plot(np.real(Z_exp), -np.imag(Z_exp), 'o', markersize=7, color='red', label='Synthetic Exp')\r\nplt.plot(np.real(Z_pred), -np.imag(Z_pred), 'x', markersize=7, color='green', label='Symbolic Regression')\r\nplt.legend(frameon=False, fontsize=15)\r\nplt.axis('scaled')\r\nplt.gca().set_aspect('equal', adjustable='box')\r\nplt.xlabel(r'$Z_{\\mathrm{re}}/\\Omega$', fontsize=20)\r\nplt.ylabel(r'$-Z_{\\mathrm{im}}/\\Omega$', fontsize=20)\r\nplt.tight_layout()\r\nplt.show()\r\n\r\n# Print best equation\r\nbest_equation = model.sympy()\r\nprint('*********')\r\nprint('Best equation found:', best_equation)\r\nprint('Best equation in LaTeX:', latex(best_equation))\r\n```",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Thanks!\r\n\r\n> Specifically, I am looking for a solution where the symbolic regression parameters are positive, but the function input and output remain complex.\r\n\r\nBy this, do you mean parameters which have no imaginary component and whose real component is positive? (Since “positive” doesn’t apply to complex numbers)\r\n\r\nOr do you mean positive imaginary and positive real component?",
                "createdAt": "2024-06-17T06:12:29Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "So, right now, there are various assumptions that the expressions have has the same numeric type as the dataset. i.e., expressions will have complex-valued constants.\r\n\r\nHowever, perhaps you could try a soft constraint that the parameters are close to the positive real line? See https://astroautomata.com/PySR/examples/#9-custom-objectives for an example. There are many other examples in the discussions page.\r\n\r\nHere's how you could do it:\r\n\r\n```julia\r\nfunction my_objective(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    (prediction, completion) = eval_tree_array(tree, dataset.X, options)\r\n    y = dataset.y\r\n    if !completion\r\n        return L(Inf)\r\n    end\r\n    \r\n    function node_penalty(node)\r\n        is_constant_node = node.degree == 0 && node.constant\r\n        if is_constant_node\r\n            val = node.val\r\n            return L(abs(abs(val) - val))  # How far the constant is from its projection to the positive real line\r\n        else\r\n            return L(0)\r\n        end\r\n    end\r\n    \r\n    # Aggregate the node penalty over every node in the tree:\r\n    total_node_penalty = sum(node_penalty, tree)\r\n    total_node_penalty *= 1000.0  # Upweight penalty (likely need to tune this?)\r\n    \r\n    # Regular relative MSE loss:\r\n    prediction_loss = sum(i -> abs2(prediction[i] - y[i])/abs2(y[i]), eachindex(y)) / length(y)\r\n\r\n    return L(total_node_penalty + prediction_loss)\r\nend\r\n```\r\n\r\nthen pass that as a string to `loss_function`.\r\n\r\nLet me know how that works!\r\n\r\nNote that constants will still have some small complex component. But hopefully this will eliminate most of them...",
                "createdAt": "2024-06-17T13:45:37Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "P.S., your initial loss was provided as `elementwise_loss`\r\n```python\r\n    elementwise_loss=\"f(prediction, target) = abs2(prediction - target)/abs2(prediction)\"\r\n```\r\n\r\nhowever, you don't want to do this, as the genetic algorithm will \"game the system\" and send prediction to infinity! You probably want to use \r\n```python\r\n    elementwise_loss=\"f(prediction, target) = abs2(prediction - target)/abs2(target)\"\r\n```\r\n\r\ninstead. \r\n\r\nBut use the `loss_function` parameter instead with the full loss function in https://github.com/MilesCranmer/PySR/discussions/651#discussioncomment-9794977",
                "createdAt": "2024-06-17T13:56:46Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0xN1QxNDo1Njo0NiswMTowMM4AlXYd"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 647,
          "title": "Use PythonCall inside custom loss function",
          "body": "I'm working with @SoumiKDaS1701 trying to customize PySR to constrain aspects of the generated functions like the asymptotic limit or its derivative at long range. In order to do this, we're trying to call sympy inside of a custom loss function using PythonCall which seems like the inverse of how PySR calls into Julia. Here's our code, which looks similar to [this example](https://github.com/MilesCranmer/PySR/discussions/617).\r\n\r\n```python\r\njl.seval(\r\n    \"\"\"\r\n    import Pkg\r\n    Pkg.add(\"DynamicExpressions\")\r\n\r\n    using PythonCall\r\n    using DynamicExpressions: string_tree\r\n\r\n    function my_custom_objective(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n       \r\n       prediction, flag = eval_tree_array(tree, dataset.X, options)\r\n       if !flag\r\n            return L(Inf)\r\n       end\r\n       \r\n       eq_str = string_tree(tree, options)\r\n\r\n       # import and call our python code that uses sympy\r\n       utils = pyimport(\"cheml_pysr.utils\")\r\n       limit1 = utils.eval_function_limit(x_val=0, f=eq_str)\r\n       limit2 = utils.eval_derivative_limit(x_val=0, f=eq_str)\r\n\r\n       lim_loss = abs(0.5 - pyconvert(Float32, limit1))\r\n       deriv_loss = abs(0.25 - pyconvert(Float32, limit2))\r\n\r\n       prediction_loss = sum((prediction .- dataset.y) .^ 2) / dataset.n\r\n\r\n       lambda1 = 10\r\n       lambda2 = 10\r\n       loss = prediction_loss + lambda1*lim_loss + lambda2*deriv_loss\r\n\r\n       return loss\r\n    end\r\n    \"\"\"\r\n)\r\n```\r\n\r\nBoth of us are new to Julia and the interfacing between Python and Julia so we're having some difficulty getting this to work.\r\n\r\nOur immediate issue is getting a segmentation fault which might be related to [this issue](https://github.com/JuliaPy/PythonCall.jl/issues/219)\r\n\r\n```\r\n[26897] signal (11.1): Segmentation fault\r\nin expression starting at none:0\r\nunknown function (ip: 0x50b07b)\r\nPyUnicode_DecodeUTF8 at /export/zimmerman/soumikd/.julia/packages/PythonCall/S5MOg/src/C/pointers.jl:297 [inlined]\r\npystr_fromUTF8 at /export/zimmerman/soumikd/.julia/packages/PythonCall/S5MOg/src/Core/builtins.jl:574 [inlined]\r\npystr_fromUTF8 at /export/zimmerman/soumikd/.julia/packages/PythonCall/S5MOg/src/Core/builtins.jl:575 [inlined]\r\npystr at /export/zimmerman/soumikd/.julia/packages/PythonCall/S5MOg/src/Core/builtins.jl:583 [inlined]\r\nPy at /export/zimmerman/soumikd/.julia/packages/PythonCall/S5MOg/src/Core/Py.jl:139 [inlined]\r\nmacro expansion at /export/zimmerman/soumikd/.julia/packages/PythonCall/S5MOg/src/Core/Py.jl:131 [inlined]\r\npyimport at /export/zimmerman/soumikd/.julia/packages/PythonCall/S5MOg/src/Core/builtins.jl:1444\r\nmy_custom_objective at ./none:15\r\nevaluator at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/LossFunctions.jl:92\r\nunknown function (ip: 0x7feb8fb8f4b0)\r\n\r\n_jl_invoke at /cache/build/builder-amdci4-2/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]\r\nijl_apply_generic at /cache/build/builder-amdci4-2/julialang/julia-release-1-dot-10/src/gf.c:3077\r\n#eval_loss#3 at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/LossFunctions.jl:108\r\neval_loss at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/LossFunctions.jl:97 [inlined]\r\n#score_func#5 at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/LossFunctions.jl:164 [inlined]\r\nscore_func at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/LossFunctions.jl:161 [inlined]\r\n#PopMember#2 at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/PopMember.jl:99\r\nPopMember at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/PopMember.jl:88 [inlined]\r\nPopMember at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/PopMember.jl:88 [inlined]\r\n#2 at ./none:0 [inlined]\r\niterate at ./generator.jl:47 [inlined]\r\ncollect at ./array.jl:834\r\n#Population#1 at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/Population.jl:49 [inlined]\r\nPopulation at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/Population.jl:35 [inlined]\r\nmacro expansion at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/SymbolicRegression.jl:765 [inlined]\r\n#54 at /export/zimmerman/soumikd/.julia/packages/SymbolicRegression/FtJSD/src/SearchUtils.jl:116\r\nunknown function (ip: 0x7feb8fb99147)\r\n_jl_invoke at /cache/build/builder-amdci4-2/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]\r\nijl_apply_generic at /cache/build/builder-amdci4-2/julialang/julia-release-1-dot-10/src/gf.c:3077\r\njl_apply at /cache/build/builder-amdci4-2/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]\r\nstart_task at /cache/build/builder-amdci4-2/julialang/julia-release-1-dot-10/src/task.c:1238\r\nAllocations: 22890647 (Pool: 22860626; Big: 30021); GC: 29\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nWe would appreciate some guidance either on this specific issue or more broadly on how we would go about constraining the functions generated by PySR based on analytical properties.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Does it work if you turn parallelism off? i.e., `numprocs=0, multithreading=false`. (Python doesn't support multithreading, so if you call back into Python, you likely need to avoid parallelism, so the code doesn't violate the GIL https://wiki.python.org/moin/GlobalInterpreterLock)",
                "createdAt": "2024-06-14T21:07:36Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0xNFQyMjowNzozNiswMTowMM4AlTW-"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 684,
          "title": "How many datapoints are needed?",
          "body": "I am running an experiment that is quite expensive. Therefore each datapoint takes time and cost budget.\r\nWhat is the minimum number of datapoints that are needed to have some reliability?\r\n\r\nIs there any documentation/references/previous discussion?",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "It depends on how complex you want the equation to be, and also noise, and how many operators you are searching with. But symbolic regression is pretty data-efficient as equations are not that expressive, so you can get away with very very few datapoints. \r\n\r\nWhat I would basically do is a train/validation/test split. Train on the train data, and evaluate the model on the validation dataset. Are the predictions good/bad? If the predictions are bad, you need more data. If the predictions are close in performance to the results on the training data, then  you probably have enough data.\r\n\r\nhttps://scikit-learn.org/stable/modules/cross_validation.html",
                "createdAt": "2024-07-29T13:56:09Z"
              },
              {
                "author": {
                  "login": "cmougan"
                },
                "body": "We are looking at something like this. 10-12 datapoints, around 2/3 features. \r\n\r\nYour package provides a loss that can be calculated in train/test, but there are no pvalues right?\r\n\r\n![Screenshot 2024-07-29 at 19 06 03](https://github.com/user-attachments/assets/63d2d2f3-cf77-4cfe-83aa-481428e50b9d)\r\n ",
                "createdAt": "2024-07-29T17:07:47Z"
              },
              {
                "author": {
                  "login": "cmougan"
                },
                "body": "After some work, I have realised that the question is off. \r\nThe approach here is not from classical stats (where we are looking for a pvalue) but an ML one, where we have a loss function that we can compute in test set.\r\n\r\nStill, for a function like the above. X =[x1,n] and Y. There are many functions that in this range can have the same shape. \r\n\r\nHow do you provide scientifical statistical validity to the equatian?",
                "createdAt": "2024-08-03T09:23:27Z"
              },
              {
                "author": {
                  "login": "cmougan"
                },
                "body": "In binary classification I have seen some people using Classifier Two Sample Tests:\r\n - Model-independent detection of new physics signals using interpretable SemiSupervised classifier tests https://projecteuclid.org/journals/annals-of-applied-statistics/volume-17/issue-4/Model-independent-detection-of-new-physics-signals-using-interpretable-SemiSupervised/10.1214/22-AOAS1722.short\r\n - Revisiting Classifier Two-Sample Tests https://arxiv.org/abs/1610.06545\r\n\r\nIn binary classification is easier, as it naturally allows for a hypothesis testing.\r\nPerhaps in SR something similar can be achieved. ",
                "createdAt": "2024-08-04T09:02:25Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wOC0wNFQxMDowMjoyNSswMTowMM4AnCmu"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 692,
          "title": "Limit on constants value and having all the variables in the equation",
          "body": "Hello, everyone; I am new to Symbolic Regression and the Julia programming language. I am using the SymbolicRegression.jl package to get equations for my use case. I want to limit the size of the constants. Currently, some constant values are in the range of e-16, and I need to limit the length of the constant. Secondly, the equations do not use all the input variables in the equations, and only one or two variables are filled with constants to achieve a lower loss value. I wonder if both scenarios are possible or if this problem was already solved earlier. ",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 676,
          "title": "Help with custom loss function",
          "body": "Hi,\r\n\r\nI am trying to create a custom loss function based on the dataset. But I am not sure how to use this way in the julia loss function or whether it would be even possible.\r\n\r\nAny help would be greatly appreciated!\r\n\r\nCurrent Code:\r\n\r\n```\r\nimport numpy as np\r\n\r\nt1 = []\r\nt2 = []\r\ndudx = []\r\ndvdy = []\r\nS11 = []\r\nS22 = []\r\nS12_Real = []\r\nS13_Real = []\r\n\r\nfilename = r'C:\\Users\\lenovo\\Desktop\\data.dat'\r\nwith open(filename, 'r') as file:\r\n    for line in file:\r\n        columns=line.split()\r\n        t1.append(float(columns[0]))\r\n        t2.append(float(columns[1]))\r\n        dudx.append(float(columns[2]))\r\n        dvdy.append(float(columns[3]))\r\n        S11.append(float(columns[4]))\r\n        S22.append(float(columns[5]))\r\n        S12_Real.append(float(columns[6]))\r\n        S13_Real.append(float(columns[7]))\r\n\r\nt1 = np.array(t1)\r\nt2 = np.array(t2)\r\ndudx = np.array(dudx)\r\ndvdy = np.array(dvdy)\r\nS11 = np.array(S11)\r\nS22 = np.array(S22)\r\nS12_Real = np.array(S11)\r\nS13_Real = np.array(S22)\r\n\r\nX=np.zeros((len(t1),2))\r\nX[:,0]=t1\r\nX[:,1]=t2\r\n\r\nfrom pysr import PySRRegressor\r\n\r\nobjective = \"\"\"\r\nfunction eval_loss(tree, dataset::Dataset, options)\r\n\r\n    prediction, flag = eval_tree_array(tree, dataset.X, options)\r\n    if !flag\r\n        return Inf\r\n    end\r\n\r\n    S12 = (1.0 .- prediction) .* (S11 ./ dudx)\r\n    S13 = (1.0 .- prediction) .* (S22 ./ dvdy)\r\n\r\n    diffs1 = S12 - S12_Real\r\n    diffs2 = S13 - S13_Real\r\n\r\n    error1 = sum(diffs1 .^ 2) / length(diffs1)\r\n    error2 = sum(diffs2 .^ 2) / length(diffs2)\r\n    \r\n    error = error1 + error2\r\n\r\n    return error\r\nend\r\n\"\"\"\r\n\r\nmodel = PySRRegressor(\r\n    niterations=1000,\r\n    batch_size=1000,\r\n    binary_operators=[ \"+\", \"-\", \"*\", \"/\"],\r\n    loss_function=objective,\r\n)\r\nmodel.fit(X, y)\r\nmodel.get_best().equation\r\n```",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 660,
          "title": "Using non-tabular datasets, like sequences and grids, in PySR",
          "body": "Moving from #323:\r\n\r\n> @puja93: Hi @MilesCranmer... Thanks for the great paper and repo. \r\n> \r\n> Regarding the 8th point in your paper : \r\n> <img width=\"551\" alt=\"image\" src=\"https://github.com/MilesCranmer/PySR/assets/10529614/f373d3bb-bf34-44d8-ae38-7e8aef32695b\">\r\n> \r\n> Do you have any links for documentation or tutorial on that ? Especially for the sequential and grids dataset.\r\n> \r\n> Thanks in advance\r\n\r\n",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hi @puja93,\r\n\r\nGood question! So the primary recommended way to solve this is via PySR's integration with deep learning frameworks. For example, see the last part of the example colab notebook here: https://github.com/MilesCranmer/symbolic_deep_learning for a time series example.\r\n\r\nThe other way to solve this to create a custom loss function that incorporates your dataset structure. For example, you can learn recursive functions like https://github.com/MilesCranmer/PySR/discussions/540#discussioncomment-8356138 which could be used for time series data, or unpack the data explicitly as in https://github.com/MilesCranmer/PySR/discussions/426#discussioncomment-7025901. \r\n\r\nVectors and matrices will soon be supported in the symbolic expressions themselves (see https://github.com/SymbolicML/DynamicExpressions.jl/pull/85 for progress) – though the API for this isn't yet worked out. So for right now the above two options.\r\n\r\nCheers,\r\nMiles",
                "createdAt": "2024-07-04T18:31:15Z"
              },
              {
                "author": {
                  "login": "puja93"
                },
                "body": "Please correct me if i'm wrong @MilesCranmer \r\n\r\nWhat i understand is, we can eventually get either :\r\n\r\n**F(x_1, x_2, ..., x_m) = (y_1, y_2, ..., y_n)**      OR      **F(x_1, x_2, ..., x_m) = z**\r\n\r\nWith m = number of inputs, n = number of logprob results, z is the final output\r\n\r\nWhich then implies, we could find analytical formula for \r\n1. Fixed Input size - Fixed output size NN model (CNN, Vanilla DNN) / Grid  & Tabular\r\n2. Arbitrary input size - Fixed output size NN model (Transformer) / Sequence\r\n",
                "createdAt": "2024-07-22T07:20:43Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0yMlQwODoyMDo0MyswMTowMM4Amkoz"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 637,
          "title": "Fit equations in particular shape",
          "body": "Hello everyone,\r\n\r\nI want to fit my equation to be as a sum of two equations, each equation is function of particular variables, for example\r\nmy function = f(x,y) + g(z,u)\r\nMy function is sum of two functions, the first function is function in x, y variables, and the second one is function in z,u variables\r\n\r\nalso other shape like 1/f(x,y) + 1/g(z,u)",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "There's an example here: https://astroautomata.com/PySR/examples/#9-custom-objectives\r\n\r\nAlso see:\r\n- https://github.com/MilesCranmer/PySR/discussions/291\r\n- https://github.com/MilesCranmer/PySR/discussions/465\r\n- https://github.com/MilesCranmer/PySR/discussions/528\r\n- https://github.com/MilesCranmer/PySR/discussions/557\r\n\r\n\r\nOther potentially related:\r\n- https://github.com/MilesCranmer/PySR/discussions/401\r\n",
                "createdAt": "2024-06-02T22:48:33Z"
              },
              {
                "author": {
                  "login": "kmegahed"
                },
                "body": "Hi Miles, thank you  for your swift reply and thorough answers, I read the\r\nlater discussions, especially #557 and applied it in the following function\r\ny = f(x4,x5) * g(x3,x4) + h(x1,x2)\r\nwhere f = 3 * np.sin(X[:, 3] + X[:, 4]), g = np.sin(X[:, 2]*2.0)+X[:, 3] **\r\n2, h = 2.0*X[:, 0]*X[:, 1] ** 2 + X[:, 1]\r\nBut it fail to search for the right function\r\nAny further recommendations would be very appreciated!\r\n\r\nimport numpy as np\r\nfrom pysr import PySRRegressor\r\n\r\n# Create a synthetic dataset\r\nnp.random.seed(0)\r\nX = np.random.rand(100, 5) * 5.0  # 100 samples, 6 features\r\ny = (3 * np.sin(X[:, 3] + X[:, 4])) * (np.sin(X[:, 2]*2.0)+X[:, 3] ** 2)  +\r\n(2.0*X[:, 0]*X[:, 1] ** 2 + X[:, 1] )+ np.random.randn(100) * 0.01  #\r\nTarget variable\r\n# y = f(x4,x5) * g(x3,x4) + h(x1,x2)\r\n# where f = 3 * np.sin(X[:, 3] + X[:, 4]), g = np.sin(X[:, 2]*2.0)+X[:, 3]\r\n** 2, h = 2.0*X[:, 0]*X[:, 1] ** 2 + X[:, 1]\r\n\r\n# Define the custom objective function\r\nobjective = \"\"\"\r\nfunction contains(t, features)\r\n    if t.degree == 0\r\n        return !t.constant && t.feature in features\r\n    elseif t.degree == 1\r\n        return contains(t.l, features)\r\n    else\r\n        return contains(t.l, features) || contains(t.r, features)\r\n    end\r\nend\r\n\r\nfunction my_custom_objective(tree, dataset::Dataset{T,L}, options) where\r\n{T,L}\r\n    tree.degree != 2 && return L(Inf)\r\n    left = tree.l\r\n    right = tree.r\r\n\r\n    left.degree != 2 && return L(Inf)\r\n    bot_left = left.l\r\n    bot_right = left.r\r\n\r\n    bot_left_pred, flag = eval_tree_array(bot_left, dataset.X, options)\r\n    !flag && return L(Inf)\r\n\r\n    bot_right_pred, flag = eval_tree_array(bot_right, dataset.X, options)\r\n    !flag && return L(Inf)\r\n\r\n    right_pred, flag = eval_tree_array(right, dataset.X, options)\r\n    !flag && return L(Inf)\r\n\r\n    prediction = bot_left_pred .* bot_right_pred .+ right_pred\r\n\r\n    right_violating = Int(contains(right, (5,3,4))) + Int(!contains(right,\r\n(1,2))) # h(x1,x2) and apply penalty if contains x3,x4 or x5\r\n    bot_left_violating = Int(contains(bot_left, (1,2))) +\r\nInt(!contains(bot_left, (4,5))) # f(x4,x5) and apply penalty if contains\r\nothers say x1 or x2\r\n    bot_right_violating = Int(contains(bot_right, (1,2,5))) +\r\nInt(!contains(bot_right, (3,4))) # g(x3,x4) and apply penalty if contains\r\nothers say x1 or x2 or x5\r\n\r\n    regularization = L(100) * (right_violating .+ bot_left_violating .+\r\nbot_right_violating)\r\n\r\n    diffs = (prediction .- dataset.y) .^ 2\r\n    pp = (sum(diffs) / length(dataset.y))^ 0.5\r\n    return pp + regularization\r\nend\r\n\r\nmy_custom_objective\r\n\"\"\"\r\n\r\n# Initialize and train the PySRRegressor\r\nmodel = PySRRegressor(\r\n    procs=16,\r\n    maxsize=30,\r\n    maxdepth=7,\r\n    populations=48,\r\n    population_size=65,\r\n    niterations=500,\r\n    ncyclesperiteration=500,\r\n    binary_operators=[\"+\", \"*\", \"-\", \"/\", \"^\"],\r\n    unary_operators=[\"sin\"],\r\n    parsimony=0.05,\r\n    adaptive_parsimony_scaling=1000,\r\n    precision=64,\r\n    loss_function=objective\r\n)\r\n\r\n# Train the model\r\nmodel.fit(X, y)\r\n\r\n# Predict on the test set\r\ny_pred = model.predict(X)\r\nprint(model.equations_)\r\n\r\n\r\nOn Mon, Jun 3, 2024 at 1:48 AM Miles Cranmer ***@***.***>\r\nwrote:\r\n\r\n> See\r\n>\r\n>    - #291 <https://github.com/MilesCranmer/PySR/discussions/291>\r\n>    - #465 <https://github.com/MilesCranmer/PySR/discussions/465>\r\n>    - #528 <https://github.com/MilesCranmer/PySR/discussions/528>\r\n>    - #557 <https://github.com/MilesCranmer/PySR/discussions/557>\r\n>\r\n> Other potentially related:\r\n>\r\n>    - #401 <https://github.com/MilesCranmer/PySR/discussions/401>\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/MilesCranmer/PySR/discussions/637#discussioncomment-9641261>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ATB6Q46DVJLED5F3RWCLGB3ZFOONNAVCNFSM6AAAAABIVLQKOKVHI2DSMVQWIX3LMV43SRDJONRXK43TNFXW4Q3PNVWWK3TUHM4TMNBRGI3DC>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n",
                "createdAt": "2024-06-06T10:10:52Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0wNlQxMToxMDo1MiswMTowMM4Ak9Pm"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 657,
          "title": "Forcing PySR to use specific formula and shortcut for activating all unary operators",
          "body": "Hi @MilesCranmer \r\nFrist of all thank you for that wonderful tool that you have created with PySR. Unfortunately I am fairly new to the data science world and am facing some difficulties as a result. I work in the field of material science and I’m measuring material properties at for different conditions for example temperatur. For my data analysis I have identified to steps, where I want to use or maybe misuse PySR.\r\n\r\nIn the first step I want to fit my data with PySR, but the mathematical function to fit the data is given by the literature as: A*( digmma(1/2-B/abs(x)) – ln(B/abs(x)) ) – C*abs(x)^2\r\nIs there a way to force PySR to use a specific formula and only optimize the factors A, B and C (similar to functions like for example scipy.optimize.curve_fit)? I have already tried using different modules like scipy for this problem but often the optimisation fails.\r\n\r\nIn a second step I wanted to use PySR to find a temperature dependent expression for A, B and C.\r\nAs far as I know, we don’t know which kind of expression this temperature dependencies have. Therefore, is there a possible shortcut to activate all possible unary operators?\r\n\r\nThank you for your time and input in advance!\r\n",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hi @Sta1ger, since you are only optimising constants, perhaps you could use `scipy.optimize.minimize`? PySR is better for finding new expressions where you don't know the specific functional form\r\n\r\nRelated:\r\n- https://github.com/milescranmer/pysr/discussions/631\r\n- https://github.com/MilesCranmer/PySR/discussions/547",
                "createdAt": "2024-06-21T15:34:36Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0yMVQxNjozNDozNiswMTowMM4Aliq5"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 641,
          "title": "\"Custom function my_pow throws MethodError despite NaN handling\"",
          "body": "## Description\r\nI'm trying to define a custom function `my_pow` in PYSR that should handle negative inputs by returning `NaN`. However, despite various attempts to handle this in the Julia code, PYSR still throws a `MethodError` when the function is evaluated with negative inputs.\r\nI'd like to use PYSR to test the function such that it tries the power of features to the powers of n, which ranges between 3 to 9.\r\n## Code\r\n```python\r\n\r\nimport numpy as np\r\nimport sympy\r\nfrom pysr import PySRRegressor, \r\ndefault_pysr_params = dict(\r\n    populations=80,\r\n    model_selection=\"best\",\r\n)\r\n\r\n\r\n# Generate some data\r\nnp.random.seed(0)\r\nX = np.random.randn(100, 5)\r\ny = np.sum(X**2, axis=1) + 0.1 * np.random.randn(100)\r\n\r\n\r\n# Define a custom power function class\r\nclass my_pow(sympy.Function):\r\n    @classmethod\r\n    def eval(cls, x, n):\r\n        if x.is_negative:\r\n            return sympy.nan\r\n        return x ** n\r\n\r\n\r\n# Learn equations\r\nmodel_p = PySRRegressor(\r\n    niterations=60,\r\n    binary_operators=[\"+\", \"*\",\"/\", \"-\"],\r\n    # unary_operators=[\"my_pow(x::T, n::T) where T = (x >= zero(T)) ? x^clamp(n, 3, 9) : T(NaN)\"],\r\n    # unary_operators=[ \"my_pow(x::T, n::T) where T = abs(x)^clamp(n, 3, 9)\"],\r\n    unary_operators=[\r\n        \"my_pow(x, n) = x >= 0 ? x^(3 + 6 * (n - 0.5)^2) : convert(typeof(x), NaN)\"\r\n    ],    \r\n    extra_sympy_mappings={\"my_pow\": lambda x, n: np.where(x >= 0, x ** (3 + 6 * (n - 0.5)**2), np.nan)},\r\n    parsimony = 6.4e-8,\r\n    **default_pysr_params,\r\n)\r\n\r\nmodel_p.fit(X2, y)\r\n\r\nexplanation :\r\nI use Julia's ternary operator ?: to handle invalid inputs.\r\nx >= 0 ? checks if x is non-negative.\r\nIf x is non-negative, we compute x^(3 + 6 * (n - 0.5)^2).\r\n\r\nThis is a more complex expression that ensures n is always between 3 and 9:\r\n\r\n(n - 0.5)^2 maps any n to a value between 0 and 0.25.\r\n6 * (n - 0.5)^2 maps this to a value between 0 and 6.\r\nAdding 3 gives us a range of 3 to 9.\r\nIf x is negative, we return convert(typeof(x), NaN) to ensure type stability.\r\n\r\nBut I faced the following issue:\r\n```julia\r\n---------------------------------------------------------------------------\r\nJuliaError                                Traceback (most recent call last)\r\n[<ipython-input-26-d46dfbb57121>](https://localhost:8080/#) in <cell line: 24>()\r\n     22 )\r\n     23 \r\n---> 24 model_p.fit(X2, y)\r\n\r\n2 frames\r\n[/usr/local/lib/python3.10/dist-packages/juliacall/__init__.py](https://localhost:8080/#) in __call__(self, *args, **kwargs)\r\n    221             return ValueBase.__dir__(self) + self._jl_callmethod($(pyjl_methodnum(pyjlany_dir)))\r\n    222         def __call__(self, *args, **kwargs):\r\n--> 223             return self._jl_callmethod($(pyjl_methodnum(pyjlany_call)), args, kwargs)\r\n    224         def __bool__(self):\r\n    225             return True\r\n\r\nJuliaError: The operator `my_pow` is not well-defined over the real line, as it threw the error `MethodError` when evaluating the input -100.0. You can work around this by returning NaN for invalid inputs. For example, `safe_log(x::T) where {T} = x > 0 ? log(x) : T(NaN)`.\r\n```\r\n\r\nI would appreciate it if any idea how to fix this issue or other alternatives.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "I think it's because you put my_pow in unary operators, but it is actually a binary operator.\r\n\r\n(I guess the error message for such an error should be better)\r\n\r\n---\r\n\r\nAlso your `extra_sympy_mappings` can't use numpy functions – only sympy ones.",
                "createdAt": "2024-06-05T22:19:32Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0wNVQyMzoxOTozMiswMTowMM4Ak7_5"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 635,
          "title": "How to add dimensionless constraints - Π theorem",
          "body": "Hello, how can I use the custom loss function to\r\n(1) obtain the dimension exponential vector of the variable\r\n(2) If I can obtain it, I can add the dimensionless constraint -Π theorem to get the desired dimensionless parameter\r\nI am not very good at writing Julia code, and I always get an error when using Unitful.Dimensions()\r\nI would like to ask you how to do it, thank you\r\n<img width=\"570\" alt=\"屏幕截图_20240602_114149\" src=\"https://github.com/MilesCranmer/PySR/assets/132361044/d9fcb77e-259c-4185-83c7-9dcac22ed5bf\">",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "Alexia369"
                },
                "body": "`dimensionless_constants_only= True `\r\nI wonder if this line of code has converged to or found a trend for dimensionless parameters，i guess",
                "createdAt": "2024-06-02T04:47:39Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0wMlQwNTo0NzozOSswMTowMM4AkwU0"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 674,
          "title": "Dimensional analysis: provide natural constants/constants with given unit possible?",
          "body": "I think the best way to explain my question is with my specific problem:\r\nI have a dataset with a given pressure p0, temperature T0 and mass flow rate mflow and I would like to fit them to an outlet pressure. \r\n( I know the ground truth equation, it is however computationally expensive to solve and I would like to search for an easier one) \r\nBut to get a more general solution I would like to add some known constants like specific gas constant, area of outlet and isentropic coefficient. \r\nIf I use the dimensional analysis I get some random constants with a unit to choose so it fits my output unit. \r\nIf I force the algorithm to use dimensionless constants only, it just uses p0 because combining my three input variables doesnt lead to a pressure unit. \r\nSo I was wondering if there is way to allow the algorithm to use some given constants with a given unit and additionally to use dimensionless constants that it needs to find. Because the inputs and the constants I mentioned can be combined to a pressure unit. \r\n\r\nDid anyone try something like this or implemented a similar example? ",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 623,
          "title": "Learning parametric functions with PySR",
          "body": "Hi PySR community!\r\n\r\nWe are currently stuck with a physics problem and it seems like PySR is just what we need. Instead of brute-forcing the problem with neural networks, we would like to know more about our system so having some equations to look at would help us gain more insight for future research.\r\n\r\nThe problem is very straight-forward: we have a 100 bodies with three euclidean coordinates each, so a total number of 300 parameters in our $X$. Our observable $y$ is the total potential energy of the system, i.e. the sum for all bodies but we do not know what the indivual components are. And this is why we would like to use PySR to find these contributions.\r\n\r\nOur questions essentially are:\r\n1. Has there been documeted cases where it possible to find candidate functions with PySR when there are so many input parameters? We have access to a vast amount of data so the number of data points should not be an issue.\r\n2. Is there any tricks to guide the model in the right direction? E.g. Newton's law of universal gravitation $F_{ij} = \\frac{G m_{i} m_{j}}{r_{ij}^2}$ has a positional dependency, where $r_{ij}^2 = (x_{j}-x_{i})^2 + (y_{j}-y_{i})^2 + (z_{j}-z_{i})^2$, and our problem is probably distance dependent too, but storing all the unique pair distances is even more costly parameter-wise ($\\frac{n(n-1)}{2} = \\frac{100(100-1)}{2} = 4950$ instead of 300).\r\n3. In the distance-dependent example above, it does not matter which coordinate that is considered to be $x$, $y$ and $z$, as long as apples are compared to apples and oranges to oranges, so to speak. Is there a smart way of telling this to PySR (without using the pair distances, if that is an issue)?\r\n4. We also assume that the final equation is the same for each pair, so it seems a bit tedious to let PySR \"re-invent the wheel\" over and over again. In other words, $\\frac{C_{ij}}{(x_{j}-x_{i})^2 + (y_{j}-y_{i})^2 + (z_{j}-z_{i})}$ should be the same equation for any pair $i$ and $j$ but just a different constant $C_{ij} = G m_{i} m_{j}$. This is just an issue as we can not break up our system into individual pairs as we just know the total energy of all possible pairs.\r\n\r\nI think that was it for now. We really look forward to trying PySR in a later project! :smiley: \r\n\r\nEDIT: It is also worth noting that some of these bodies are of the same character. So in that case it would definitely be unnecassary to find the same expression yet again and very worrying if it did not!",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hi @pastaalfredo,\r\n\r\nGreat question! We have done this in https://github.com/MilesCranmer/symbolic_deep_learning (check out the colab demo here: https://colab.research.google.com/github/MilesCranmer/symbolic_deep_learning/blob/master/GN_Demo_Colab.ipynb – which goes through this exact sort of problem). However for you, you will need to also learn a \"parameter dictionary\" which we did in https://iopscience.iop.org/article/10.1088/2632-2153/acfa63/meta as a way to do \"basis learning\".\r\n\r\nBasically you would first learn a deep learning model on your problem, and inside that model, you would have a set of per-object parameters (act as your function \"parameters\" such as $m_i$ and $m_j$ in your example) that you would also learn a neural net, you have:\r\n\r\n```python\r\nclass ParametricModel(nn.Module):\r\n    def __init__(self, num_objects, num_params):\r\n        self.parameter_map = nn.Parameter(torch.randn(num_objects, num_params, dtype=torch.float32))\r\n        # Implement neural net model for learning expression\r\n        self.mlp = ... # has (num_params + num_features) input\r\n\r\n    def forward(self, x):\r\n        object_index = x[:, 0]  # For example - you put in `i` here.\r\n        parameters = self.parameter_map[object_index]\r\n        features = x[:, 1:]\r\n        features_and_parameters = torch.cat((parameters, features), dim=1)\r\n        return self.mlp(feature_and_parameters)\r\n```\r\n\r\nAfter training this, you can approximate `self.mlp` using PySR (see bottom of https://colab.research.google.com/github/MilesCranmer/PySR/blob/master/examples/pysr_demo.ipynb for an example!).\r\n\r\nThat will be a parametric function, and the per-object parameters will be found in `self.parameter_map`.\r\n\r\nThis essentially turns a 100-input problem into a parametrized 5-input problem, which is much easier for PySR to handle.\r\n\r\nCheers,\r\nMiles",
                "createdAt": "2024-05-07T14:15:31Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNS0wN1QxNToxNTozMSswMTowMM4Ajo3b"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 654,
          "title": "Making PySR approximate a specific type of equation (Linear, non-linear, polynomial, etc.)",
          "body": "I have been working in a certain material science problem and have used a few ML models such as AutoML and simple models like multiple-linear regression (MLR). I find it odd that PySR cannot come even close to the performance of even a simple MLR. I even tuned the parameter space to look over a lot and few operators, setting constrains, and increasing number of iterations and population. @MilesCranmer do you have any idea of why this might be happening? Do you have any recommendations? Although the MLR equation is not the definitive model with the best performance, it still outperformed every symbolic equation PySR gave. PySR is trying too hard to find complex symbolic equations that might fit the data. It may be interesting to be able to set the model to a starting point. For example, tell the algorithm first to use a linear regression using all the variables and start exploring from there. Another thing I need to try is to use the equation of the MLR as the input for the PySRegressor. The MLR is already very good, so my hope is that PySR might be able to improve it. I will give updates on it on the following days.\r\n\r\nVariable handling\r\nAlso, I would like to specify that PySR use all the variables I gave in all the expressions it explores. Based on my previous knowledge of my problem, I know the variables I gave have to be fixed in the equation, so exploring symbolic expressions while missing one would be unrealistic. That could save PySR computation and time exploring equations with missing variables.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Can you share a bit more about the example, maybe with some code and datasets? More diagnostics would be helpful too.\r\n\r\nAlso, do you know what the equivalent complexity of the linear regression model would be, if PySR were to find it? e.g., `y = a * x + b * z` would have a complexity of 7. The default max complexity is 20 (`maxsize` parameter – which you can increase).\r\n\r\nAlso see related thread here: https://github.com/MilesCranmer/PySR/issues/536",
                "createdAt": "2024-06-18T21:54:56Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Other related questions you should check out –\r\n- #572 \r\n- #273\r\n- #161 ",
                "createdAt": "2024-06-18T22:54:25Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0xOFQyMzo1NDoyNSswMTowMM4AlbVV"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 642,
          "title": "Evaluation of equations to test data",
          "body": "I found have found it v.time consuming to find the sweet spot of complexity that has the best fit for test data with the higher complexities on the training data overfitting and performing poorly on the test data. \r\n\r\nIn addition I also needed to apply the equations to different subgroups of the data and see how each equation applied to those.\r\n\r\nI have put together some python code to make my life easier, and someone else might find it useful.\r\nIt assumes that in the main database (xlsx), has multiple otherwise identical worksheets, that only differ with the data. For example you can have \"train\", \"test_group1\", \"test_group2\" etc. \r\n\r\nThe output would look something like this: \r\nThis table is for all training and test data combined\r\n![image](https://github.com/MilesCranmer/PySR/assets/127948719/6d338054-2c1a-4fb5-801a-98e7bc88752e)\r\n\r\nThe line graphs separate train and test, and present combined data. Separate line graphs are produced for each subcategory\r\n![image](https://github.com/MilesCranmer/PySR/assets/127948719/c0d4094c-0423-454a-bbb2-eb4a5bce6b80)\r\n\r\nApologies if code is highly unoptimised!\r\n\r\nExample:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport math\r\nimport warnings\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\r\nfrom tabulate import tabulate\r\n\r\n# Suppress the performance warning\r\nwarnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\r\n\r\n# Define the file paths and sheet names\r\ndata_file_path = \"filepath_database.xlsx\"\r\nsheet_names = [\"Train\",\"Test\"] #works with as many sheet names as you want tested. \r\nnew_label = sheet_names + [\"All Cases\"]\r\nsubgroup_categories = ['All', 'Short', 'Medium', 'Long', 'Eq_Wt'] #Eq_Wt means all groups are given equal weighting even if unequal n. \r\n \r\n# Define the target variable and subcategory criteria\r\ntarget_variable = 'Postoperative_BCVA' #Y\r\nsubcategory_variable = 'Preoperative_BCVA' #column in database that subgroup is based on\r\nshort_threshold = 0.3 #<0.3\r\nmedium_threshold_lower = short_threshold #>=0.3\r\nmedium_threshold_upper = 0.6 #<0.6\r\nlong_threshold = medium_threshold_upper #>=0.6\r\n\r\nequations_df = pd.read_csv(\"filepath_to_hall_of_fame_csv.csv\")\r\nequation_strs = equations_df[\"Equation\"].str.lower().tolist() #lower case to prevent frustrating errors, remove if not relevant. \r\n\r\nall_dfs = {i: pd.read_excel(data_file_path, sheet_name=sheet_name) for i, sheet_name in enumerate(sheet_names)}\r\n\r\n# Define mapping between variable names and column names\r\nvariable_mapping = { #left is variable names in equations, x0,x1,x2,x3 etc. Right are column names in database (xlsx)\r\n    'phaco_ppv': 'Phaco_PPV',\r\n    'eifl_present': 'EIFL_Present',\r\n    'eifl_t': 'EIFL_T',\r\n    'eifl_v': 'EIFL_V',\r\n    'preoperative_bcva': 'Preoperative_BCVA',\r\n    'crt': 'CRT',\r\n    'r_mv': 'R_MV',\r\n    'r_fv': 'R_FV',\r\n    'onl_t': 'ONL_T',\r\n    'onl_mv': 'ONL_MV',\r\n    'onl_fv': 'ONL_FV',\r\n    'inl_t': 'INL_T',\r\n    'inl_mv': 'INL_MV',\r\n    'inl_fv': 'INL_FV'\r\n}\r\n\r\nrelevant_columns = list(variable_mapping.values())\r\n\r\ndef get_sample_values(row):\r\n    sample_values = {key: row[col] for key, col in variable_mapping.items()}\r\n    sample_values.update({ #Add unary variables used in pysr\r\n        'sin': math.sin,\r\n        'relu': lambda x: max(0, x),\r\n        'cond': lambda x, y: y if x > 0 else 0,\r\n        'cos': math.cos,\r\n        'tan': math.tan,\r\n        'exp': math.exp,\r\n        'log': math.log,\r\n        'ln': math.log,\r\n        'sqrt': math.sqrt,\r\n        'square': lambda x: x ** 2,\r\n        'inv': lambda x: 1 / x,\r\n    })\r\n    return sample_values\r\n\r\n#------------------no further customisation required below this line for it to work\r\n\r\n# Eval the equations\r\ndef evaluate_equation(row, equation_str):\r\n    try:\r\n        sample_values = get_sample_values(row)\r\n        return eval(equation_str, sample_values)\r\n    except Exception as e:\r\n        #print(f\"Error evaluating equation on row {row.name}: {e}\")\r\n        return np.nan\r\n\r\nfor i, equation_str in enumerate(equation_strs):\r\n    for x, df in all_dfs.items():\r\n        col_e = f'Eval_{i+1}'\r\n        df[col_e] = df.apply(lambda row: evaluate_equation(row, equation_str), axis=1)\r\n\r\n# Combine all individual DF into a single combined DF\r\ncombined_df = pd.concat(all_dfs.values(), ignore_index=True)\r\nall_dfs['combined'] = combined_df\r\n\r\n# Custom error functions that handle NaN values\r\ndef custom_mean_absolute_error(y_true, y_pred):\r\n    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\r\n    return np.mean(np.abs(y_true[mask] - y_pred[mask]))\r\n\r\ndef custom_mean_squared_error(y_true, y_pred):\r\n    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\r\n    return np.sqrt(np.mean((y_true[mask] - y_pred[mask]) ** 2))\r\n\r\ndef get_target_variable(df, category):\r\n    if category == 'All':\r\n        return df[target_variable]\r\n    elif category == 'Short':\r\n        return df[df[subcategory_variable] < short_threshold][target_variable]\r\n    elif category == 'Medium':\r\n        return df[(df[subcategory_variable] >= medium_threshold_lower) & (df[subcategory_variable] < medium_threshold_upper)][target_variable]\r\n    elif category == 'Long':\r\n        return df[df[subcategory_variable] >= long_threshold][target_variable]\r\n    elif category == 'Eq_Wt':\r\n        return df[target_variable]\r\n\r\nnum_equations = len(equation_strs)\r\nnum_dfs = len(all_dfs)\r\nnum_categories = len(subgroup_categories)\r\nmae_lists = np.zeros((num_equations, num_dfs, num_categories))\r\nrmse_lists = np.zeros((num_equations, num_dfs, num_categories))\r\n\r\nfor i, equation_str in enumerate(equation_strs):\r\n    for x, (key, df) in enumerate(all_dfs.items()):\r\n        for j, category in enumerate(subgroup_categories):\r\n            true_data = get_target_variable(df, category)\r\n            estimate_data = df[f'Eval_{i+1}'].reindex(true_data.index)  \r\n\r\n            if j == len(subgroup_categories) - 1:  # If Eq_Wt category\r\n                mae_lists[i][x][j] = np.mean(mae_lists[i][x][j - 3:j])\r\n                rmse_lists[i][x][j] = np.mean(rmse_lists[i][x][j - 3:j])\r\n            else:\r\n                mae_lists[i][x][j] = custom_mean_absolute_error(true_data, estimate_data)\r\n                rmse_lists[i][x][j] = custom_mean_squared_error(true_data, estimate_data)\r\n\r\nmin_mae = np.nanmin(mae_lists, axis=0)  \r\nmin_rmse = np.nanmin(rmse_lists, axis=0) \r\n\r\nmin_mae_eq_num = np.argmin(mae_lists, axis=0)  # Equation number for min MAE\r\nmin_rmse_eq_num = np.argmin(rmse_lists, axis=0)  # Equation number for min RMSE\r\n\r\nround_precision = 5\r\n\r\n# Define the data for the table\r\ndata = [[\"Subgroup Category\", \"Min RMSE\", \"Best Equation Number (RMSE)\"] + [category + \" (RMSE)\" for category in subgroup_categories] + [category + \" (MAE)\" for category in subgroup_categories]]\r\n\r\n#RMSE table\r\nfor category in subgroup_categories:\r\n    category_index = subgroup_categories.index(category)\r\n    min_rmse_value = round(min_rmse[len(all_dfs) - 1, category_index], round_precision)\r\n    equation_number_rmse = min_rmse_eq_num[len(all_dfs) - 1, category_index] + 1\r\n    row_data = [category, min_rmse_value, equation_number_rmse]\r\n    for i in subgroup_categories:\r\n        row_data.append(round(rmse_lists[equation_number_rmse-1, len(all_dfs) - 1, subgroup_categories.index(i)], round_precision))\r\n    for i in subgroup_categories:\r\n        row_data.append(round(mae_lists[equation_number_rmse-1, len(all_dfs) - 1, subgroup_categories.index(i)], round_precision))\r\n    data.append(row_data)\r\n\r\nprint(tabulate(data, tablefmt=\"grid\"))\r\n\r\ndata = [[\"Subgroup Category\", \"Min MAE\", \"Best Equation Number (MAE)\"] + [category + \" (RMSE)\" for category in subgroup_categories] + [category + \" (MAE)\" for category in subgroup_categories]]\r\n\r\n#MAE table\r\nfor category in subgroup_categories:\r\n    category_index = subgroup_categories.index(category)\r\n    min_mae_value = round(min_mae[len(all_dfs) - 1, category_index], round_precision)\r\n    equation_number_mae = min_mae_eq_num[len(all_dfs) - 1, category_index] + 1\r\n    row_data = [category, min_mae_value, equation_number_mae]\r\n    for i in subgroup_categories:\r\n        row_data.append(round(rmse_lists[equation_number_mae-1, len(all_dfs) - 1, subgroup_categories.index(i)], round_precision))\r\n    for i in subgroup_categories:\r\n        row_data.append(round(mae_lists[equation_number_mae-1, len(all_dfs) - 1, subgroup_categories.index(i)], round_precision))\r\n    data.append(row_data)\r\n\r\nprint(tabulate(data, tablefmt=\"grid\"))\r\n\r\n# Define x-axis (equation numbers)\r\nx_axis = range(1, len(equation_strs) + 1)\r\n# Calculate the width of the figure based on the number of equations\r\nmin_width = 10  # Minimum width of the figure\r\nadditional_width_per_equation = 0.5  # Additional width per equation\r\nnum_equations = len(equation_strs)\r\ntotal_width = min_width + additional_width_per_equation * num_equations\r\n\r\n# Plot RMSE and MAE for each eye length category\r\nfor j, subgroup_category in enumerate(subgroup_categories):\r\n    plt.figure(figsize=(total_width, 10)) #10 is height of figure, can increase if lots of equations\r\n    for x, df in enumerate(all_dfs):\r\n        label = new_label[x]\r\n        color = plt.cm.tab10(x)\r\n        plt.plot(x_axis, rmse_lists[:, x, j], marker='o', label=label, color=color)\r\n        if label != 'Min Test RMSE':\r\n            plt.axhline(y=min_rmse[x, j], linestyle='--', color=color)\r\n    plt.xlabel('Equation Number')\r\n    plt.ylabel('RMSE')\r\n    plt.title(f'RMSE for Each Equation ({subgroup_category} Eyes)')\r\n    plt.xticks(x_axis)\r\n    plt.legend()\r\n    plt.grid(True)\r\n    plt.show()\r\n    \r\n    plt.figure(figsize=(total_width, 10))\r\n    for x, df in enumerate(all_dfs):\r\n        label = new_label[x]\r\n        color = plt.cm.tab10(x)\r\n        plt.plot(x_axis, mae_lists[:, x, j], marker='o', label=label, color=color)\r\n        if label != 'Min Test MAE':\r\n            plt.axhline(y=min_mae[x, j], linestyle='--', color=color)\r\n    plt.xlabel('Equation Number')\r\n    plt.ylabel('MAE')\r\n    plt.title(f'MAE for Each Equation ({subgroup_category} Eyes)')\r\n    plt.xticks(x_axis)\r\n    plt.legend()\r\n    plt.grid(True)\r\n    plt.show()\r\n\r\n```\r\n  ",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 679,
          "title": "Using custom dimensions from `DynamicQuantities`",
          "body": "Hello,\r\n\r\nI'm trying to do equation discovery in a dimensionally consistent way but I'm having a hard time of it. Many of my quantities are either number concentration (#/kg) or mass mixing ratio (kg of A/kg of B). I can not figure out how to express these units using the `PySR` utility. \r\n\r\nI've tried switching to the `SymbolicRegression.jl` library and then implementing my own set of dimensions (following the documentation for `DynamicQuantities`) as show here:\r\n\r\n```julia\r\n# ...\r\n\r\n# create our dimensions\r\nstruct ExtendedDimensions{R} <: AbstractDimensions{R}\r\n    length::R\r\n    mass::R\r\n    time::R\r\n    current::R\r\n    temperature::R\r\n    luminosity::R\r\n    amount::R\r\n    angle::R\r\n    mass_h2o::R\r\n    mass_air::R\r\n    count::R\r\nend\r\n\r\n# ... \r\n\r\nconst mmr   = Quantity(1.0, ExtendedDimensions(mass_h2o=1, mass_air=-1))\r\nconst nconc = Quantity(1.0, ExtendedDimensions(count=1, mass_air=-1))\r\nconst dens  = Quantity(1.0, ExtendedDimensions(mass_air=1, length=-3))\r\n\r\nX = (; A=data.A * mmr, B=data.B * nconc)\r\ny = data.C * mmr\r\n\r\n# create model\r\nmodel = SRRegressor(\r\n\tbinary_operators=[*,^,/],\r\n\tdimensional_constraint_penalty=10^5,\r\n\tnested_constraints=[(^) => [(^) => 0]],\r\n\tconstraints=[(^) => (-1,1)]\r\n)\r\n\r\n# create machine\r\nmach = machine(model, X, y)\r\n\r\n# do the fitting\r\nfit!(mach)\r\n\r\nr = report(mach)\r\n```\r\n\r\nbut I get this error when I run the script:\r\n\r\n```\r\nERROR: LoadError: Cannot create a dimensions of `Dimensions{DynamicQuantities.FixedRational{Int32, 25200}}` from `ExtendedDimensions{DynamicQuantities.FixedRational{Int32, 25200}}`. Please write a custom method for construction.\r\nStacktrace:\r\n  [1] Dimensions{DynamicQuantities.FixedRational{Int32, 25200}}(d::ExtendedDimensions{DynamicQuantities.FixedRational{Int32, 25200}})\r\n    @ DynamicQuantities ~/.julia/packages/DynamicQuantities/5QflN/src/types.jl:118\r\n  [2] convert(::Type{Dimensions{DynamicQuantities.FixedRational{Int32, 25200}}}, d::ExtendedDimensions{DynamicQuantities.FixedRational{Int32, 25200}})\r\n    @ DynamicQuantities ~/.julia/packages/DynamicQuantities/5QflN/src/utils.jl:323\r\n  [3] convert(::Type{Quantity{Float64, Dimensions{DynamicQuantities.FixedRational{Int32, 25200}}}}, q::Quantity{Float64, ExtendedDimensions{DynamicQuantities.FixedRational{Int32, 25200}}})\r\n    @ DynamicQuantities ~/.julia/packages/DynamicQuantities/5QflN/src/utils.jl:314\r\n  [4] get_units(::Type{Float64}, ::Type{Dimensions{DynamicQuantities.FixedRational{Int32, 25200}}}, x::ExtendedDimensions{DynamicQuantities.FixedRational{Int32, 25200}}, ::Function)\r\n    @ SymbolicRegression.InterfaceDynamicQuantitiesModule ~/.julia/packages/SymbolicRegression/RwU5c/src/InterfaceDynamicQuantities.jl:39\r\n  [5] get_si_units(::Type{Float64}, units::ExtendedDimensions{DynamicQuantities.FixedRational{Int32, 25200}})\r\n```\r\n\r\nI'm very new to Julia, but it seems to me like the `SRRegressor` needs to \"cast\" a derived type (`ExtendedDimensions`) to a \"base\" type (`Dimensions`), and as I understand that, it is not something that can be done. \r\n\r\nDoes anyone have any advice? Is it possible to use a custom version of `Dimensions`? \r\n\r\n(I'm asking here since it did not seem reasonable to open an issue on `SymbolicRegression.jl`'s github for this, and the forums link there sent me here).",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hi @HenryDane,\r\n\r\nThanks for the post! Do you want to post this is as an issue on SymbolicRegression.jl? Custom `AbstractDimensions` have actually not yet been set up within `SymbolicRegression` – simply because I didn't know anybody would want to customize this :). But that sounds like a cool use-case, and it should hopefully be fairly easy to add this.\r\n\r\nCheers,\r\nMiles",
                "createdAt": "2024-07-22T23:02:39Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0yM1QwMDowMjozOSswMTowMM4Amms7"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 680,
          "title": "Sigmoid custom operator",
          "body": "Hello,\r\n\r\nI want to add a new unary operator when defining my PySRRegressor, which is the sigmoid function. I define it as follows:\r\n```\r\nmodel_2 = PySRRegressor(\r\n    niterations=50,\r\n    binary_operators=[‘+’, ‘-’, ‘*’, ‘/’],\r\n    unary_operators=[‘exp’, ‘inv(x) = 1/x’, ‘sig(x)=1/(1+exp(-x))’],\r\n    extra_sympy_mappings = {\r\n        ‘inv\": lambda x: 1/x,\r\n        ‘sig\": lambda x: 1/(1+np.exp(-x))\r\n    },\r\n    **default_pysr_params,\r\n)\r\n```\r\n\r\nI get the following error:\r\n```\r\nValueError                                Traceback (most recent call last)\r\nValueError: Error from parse_expr with transformed code: <code object <module> at 0x7f62306e5f20, file \"<string>\", line 1>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/export_sympy.py:91, in pysr2sympy(equation, feature_names_in, extra_sympy_mappings)\r\n     [90](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/export_sympy.py:90) try:\r\n---> [91](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/export_sympy.py:91)     return sympify(equation, locals=local_sympy_mappings, evaluate=False)\r\n     [92](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/export_sympy.py:92) except TypeError as e:\r\n\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/core/sympify.py:493, in sympify(a, locals, convert_xor, strict, rational, evaluate)\r\n    [492](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/core/sympify.py:492)     a = a.replace('\\n', '')\r\n--> [493](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/core/sympify.py:493)     expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\r\n    [494](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/core/sympify.py:494) except (TokenError, SyntaxError) as exc:\r\n\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1090, in parse_expr(s, local_dict, transformations, global_dict, evaluate)\r\n   [1089](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1089)     local_dict[i] = null\r\n-> [1090](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1090) raise e from ValueError(f\"Error from parse_expr with transformed code: {code!r}\")\r\n\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1081, in parse_expr(s, local_dict, transformations, global_dict, evaluate)\r\n   [1080](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1080) try:\r\n-> [1081](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1081)     rv = eval_expr(code, local_dict, global_dict)\r\n   [1082](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:1082)     # restore neutral definitions for names\r\n\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:909, in eval_expr(code, local_dict, global_dict)\r\n    [904](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:904) \"\"\"\r\n    [905](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:905) Evaluate Python code generated by ``stringify_expr``.\r\n    [906](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:906) \r\n    [907](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:907) Generally, ``parse_expr`` should be used.\r\n    [908](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:908) \"\"\"\r\n--> [909](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:909) expr = eval(\r\n    [910](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:910)     code, global_dict, local_dict)  # take local objects in preference\r\n    [911](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/sympy/parsing/sympy_parser.py:911) return expr\r\n\r\nFile <string>:1\r\n\r\nCell In[10], [line 7](vscode-notebook-cell:?execution_count=10&line=7)\r\n      [1](vscode-notebook-cell:?execution_count=10&line=1) model_2 = PySRRegressor(\r\n      [2](vscode-notebook-cell:?execution_count=10&line=2)     niterations=50,\r\n      [3](vscode-notebook-cell:?execution_count=10&line=3)     binary_operators=[\"+\", \"-\", \"*\", \"/\"],\r\n      [4](vscode-notebook-cell:?execution_count=10&line=4)     unary_operators=[\"exp\", \"inv(x) = 1/x\", \"sig(x)=1/(1+exp(-x))\"],\r\n      [5](vscode-notebook-cell:?execution_count=10&line=5)     extra_sympy_mappings = {\r\n      [6](vscode-notebook-cell:?execution_count=10&line=6)         \"inv\": lambda x: 1/x,\r\n----> [7](vscode-notebook-cell:?execution_count=10&line=7)         \"sig\": lambda x: 1/(1+np.exp(-x))\r\n      [8](vscode-notebook-cell:?execution_count=10&line=8)     },\r\n      [9](vscode-notebook-cell:?execution_count=10&line=9)     **default_pysr_params,\r\n     [10](vscode-notebook-cell:?execution_count=10&line=10) )\r\n\r\nTypeError: loop of ufunc does not support argument 0 of type Mul which has no callable exp method\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[11], [line 1](vscode-notebook-cell:?execution_count=11&line=1)\r\n----> [1](vscode-notebook-cell:?execution_count=11&line=1) model_2.fit(X, y)\r\n\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2087, in PySRRegressor.fit(self, X, y, Xresampled, weights, variable_names, complexity_of_variables, X_units, y_units)\r\n   [2084](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2084)     self._checkpoint()\r\n   [2086](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2086) # Perform the search:\r\n-> [2087](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2087) self._run(X, y, runtime_params, weights=weights, seed=seed)\r\n   [2089](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2089) # Then, after fit, we save again, so the pickle file contains\r\n   [2090](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2090) # the equations:\r\n   [2091](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2091) if not self.temp_equation_file:\r\n\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:1921, in PySRRegressor._run(self, X, y, runtime_params, weights, seed)\r\n   [1918](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:1918) self.julia_state_stream_ = jl_serialize(out)\r\n   [1920](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:1920) # Set attributes\r\n-> [1921](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:1921) self.equations_ = self.get_hof()\r\n   [1923](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:1923) if self.delete_tempfiles:\r\n   [1924](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:1924)     shutil.rmtree(self.tempdir_)\r\n\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2412, in PySRRegressor.get_hof(self)\r\n   [2409](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2409) torch_format = []\r\n   [2411](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2411) for _, eqn_row in output.iterrows():\r\n-> [2412](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2412)     eqn = pysr2sympy(\r\n   [2413](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2413)         eqn_row[\"equation\"],\r\n   [2414](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2414)         feature_names_in=self.feature_names_in_,\r\n   [2415](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2415)         extra_sympy_mappings=self.extra_sympy_mappings,\r\n   [2416](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2416)     )\r\n   [2417](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2417)     sympy_format.append(eqn)\r\n   [2419](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/sr.py:2419)     # NumPy:\r\n\r\nFile /home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/export_sympy.py:95, in pysr2sympy(equation, feature_names_in, extra_sympy_mappings)\r\n     [93](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/export_sympy.py:93) if \"got an unexpected keyword argument 'evaluate'\" in str(e):\r\n     [94](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/export_sympy.py:94)     return sympify(equation, locals=local_sympy_mappings)\r\n---> [95](https://vscode-remote+wsl-002bwsl4datascience.vscode-resource.vscode-cdn.net/home/default/miniconda/envs/bibsr-pysr/lib/python3.10/site-packages/pysr/export_sympy.py:95) raise TypeError(f\"Error processing equation '{equation}'\") from e\r\n\r\nTypeError: Error processing equation '(x0 * sig(x0)) * 2.1461575'\r\n```\r\n\r\nDo you have any ideas on how to resolve this error?\r\n\r\nThank you in advance.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "It's because `np.exp` is not a sympy function. You need to use `sympy.exp`",
                "createdAt": "2024-07-26T12:07:22Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0yNlQxMzowNzoyMiswMTowMM4AmwWa"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 638,
          "title": "Avoid variables appearing in the exponent position",
          "body": "How to avoid 0.9735712^x0, where x0 appears at the exponent",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "You could do it with\r\n\r\n```julia\r\nfunction my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    index_of_exp = 1 # if its the first operator in your list etc\r\n    has_x_in_exp = any(tree) do node\r\n        if node.degree == 1 && node.op == index_of_exp\r\n            subnode = node.l\r\n            return any(subnode) do n\r\n                return n.degree == 0 && !n.constant\r\n            end\r\n        end\r\n        return false\r\n    end\r\n    has_x_in_exp && return L(10_000_000_000)\r\n    prediction, flag = eval_tree_array(tree, dataset.X, options)\r\n    if !flag\r\n        return L(10_000_000)\r\n    end\r\n    return sum(i -> (prediction[i] - dataset.y[i]) ^ 2, eachindex(dataset.y)) / dataset.n\r\nend\r\n```",
                "createdAt": "2024-06-03T08:08:16Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Also see https://astroautomata.com/PySR/api/#working-with-complexities",
                "createdAt": "2024-06-03T08:09:13Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0wM1QwOTowOToxMyswMTowMM4Akyrf"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 643,
          "title": "Interpretation for complex equations",
          "body": "The ethos behind SR is interpretability. \r\nHowever, in the event that a complex equation has much better fit to test data, the resulting equation, although much easier to work with than a blackbox model, is difficult to interpret. \r\n\r\nHow do you tackle this problem?\r\nI have played around with using the SHAP library?\r\n\r\nThe permutation in SHAP is very slow and I am doubtful this is the best approach, but it does allow for feature importance.\r\n\r\nThis would give me this example output: \r\n![image](https://github.com/MilesCranmer/PySR/assets/127948719/eb4dc613-0971-463c-b455-1157c1a86656)\r\n\r\n```\r\nimport shap\r\n\r\nclass CustomModel:\r\n    def __init__(self, equation_str):\r\n        self.equation_str = equation_str\r\n\r\n    def predict(self, X):\r\n        results = []\r\n        for _, row in X.iterrows():\r\n            sample_values = get_sample_values(row)\r\n            result = eval(self.equation_str, sample_values)\r\n            results.append(result)\r\n        return np.array(results)\r\n\r\nselected_equation_index = 38  # Change this to select a different equation\r\nselected_equation = equation_strs[selected_equation_index]\r\nmodel = CustomModel(selected_equation)\r\n\r\n# Prepare the data using only relevant columns for SHAP\r\nrelevant_columns = list(variable_mapping.values())\r\nX = combined_df[relevant_columns]  # Use combined_df instead of df\r\ny_true = combined_df[target_variable]  # Use combined_df instead of df\r\n\r\n# Ensure all data types are numeric, coerce errors\r\nX = X.apply(pd.to_numeric, errors='coerce')\r\ny_true = y_true.apply(pd.to_numeric, errors='coerce')\r\n\r\n# Drop any remaining rows with NaN values\r\nX = X.dropna()\r\ny_true = y_true.loc[X.index]\r\n\r\n# Generate SHAP values using Permutation Explainer\r\nexplainer = shap.Explainer(model.predict, X, algorithm=\"permutation\")\r\nshap_values = explainer(X)\r\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\r\nshap.summary_plot(shap_values, X)\r\nshap.plots.beeswarm(shap_values)\r\n\r\n```",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 622,
          "title": "How to select candidate equations using performance of test data more efficiently?",
          "body": "Hi,\r\n\r\nThanks for developing such a useful tool! \r\n\r\nI try to discover equations from observational data. I run PySR with different parameters settings (e.g., complexity, operators), and I want to select equations according to the performance of test data (e.g., RMSE < a & R > b). But I have to select the equations manually which is time-costing. Is there any method to select candidate equations more efficiently?\r\n\r\nBest regards,\r\nLu",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Thanks!\r\n\r\nWould the following help?\r\n\r\n```julia\r\nimport copy\r\n\r\nequations = copy.deepcopy(model.equations_)\r\n\r\n# this is a pandas dataframe, so we can add new columns:\r\nequations[\"my_metric\"] = [\r\n    my_metric(\r\n        model.predict(Xtest, index=i),\r\n        ytest\r\n    )\r\n    for i in range(len(equations))\r\n]\r\n\r\n\r\nchoice = equations[\"my_metric\"].idxmin()\r\n# ^ or idxmax() if maximizing\r\n\r\nmodel.predict(X, index=index)\r\n# ^ Predict with best (or can pass to .sympy/.latex/.jax/.pytorch)\r\n```\r\n\r\n",
                "createdAt": "2024-05-07T08:26:31Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNS0wN1QwOToyNjozMSswMTowMM4Ajn8o"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 675,
          "title": "Searching for multivariate polynomials: restrict the maximum degree?",
          "body": "I have an application where I know that the data is generated by a multivariate polynomial (e.g. $a \\cdot x_0 + b \\cdot x_0 \\cdot x_0 + c \\cdot x_0 \\cdot x_1$) up to order $degree_{max}$ (2 in example). \r\nI want to restrict the pysr search space to not include polynomials with higher order than $degree_{max}$ (e.g. for $degree_{max} = 2$, an expression including the term $x_0 \\cdot x_0 \\cdot x_1$ should not be searched).\r\n\r\nRelevant pysr-settings:\r\n```\r\nbinary_operators=[\"+\", \"*\", \"-\"]\r\nunary_operators=[]\r\n```\r\n\r\nNow, I do not understand how to set correct `constraints `/ `nested_constraints `/ complexities to restrict the search space.\r\nMy idea was to restrict the usage of the `\"*\"` operator (note I am not using `\"^\"`) to only allow for at most max_degree factors (but a constant should be allowed). \r\n\r\nThank you for your help!",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "FelixSchloms"
                },
                "body": "you could restrict the \"*\" operator to just use a constant or a variable as arguments. \r\nIt is documented at: https://astroautomata.com/PySR/options/#constraining-use-of-operators\r\nwith \r\n`constraints={'mult': (1, 1)} `\r\nyou restrict the multiplication operator to use a max. complextiy of 1 at each side, which is by default just a variable or a constant. \r\n\r\n",
                "createdAt": "2024-07-18T16:31:06Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0xOFQxNzozMTowNiswMTowMM4AmeoM"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 663,
          "title": "Symbolic regression with know equation",
          "body": "Dear all member,\r\n\r\nIs it possible to train the symbolic regression model with know equation?\r\nfor example, \r\nthe known equation is \r\n\r\n$$ y = x_1 + x_2 \\times \\alpha + \\frac{\\beta}{x_3} $$\r\n\r\nand the training process that can follow the samilar equation structure.\r\n\r\nBest,\r\nZhao",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 634,
          "title": "high-complexity equations fitting",
          "body": "Dear Everynoe,\r\n\r\nI am currently using PySR to fit high-complexity equations such as the Arrhenius equation like y=A*x^n*exp(-E/R/x). However, I am encountering a problem where the loss function is extremely high, resulting in a failure to fit the model properly. Could you please suggest any methods to avoid such issues? Additionally, can you provide guidance on how to implement constraints or limitations within the fitting process?\r\n\r\nBelow is my code:\r\n```ruby\r\nimport pysr\r\nimport sympy\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\nfrom pysr import PySRRegressor\r\nfrom sklearn.model_selection import train_test_split\r\nfrom math import exp\r\nA = 3.55*10**15\r\nn = -0.41\r\nE = 16.6  #  J/mol\r\nR = 8.314  #  J/(mol*K)\r\nX = np.linspace(250, 1250, 1000)\r\ny = A*X**n*np.exp(-E/R/X)\r\ndefault_pysr_params = dict(\r\n    populations=30,\r\n    model_selection=\"best\",\r\n    #random_state=42,\r\n    #proce=0,\r\n    #deterministic=True\r\n)\r\n# Learn equations\r\nmodel = PySRRegressor(\r\n    niterations=300,\r\n    binary_operators=[\"+\", \"-\", \"*\", \"/\",\"^\"],\r\n    unary_operators=[\"exp\"],\r\n    #extra_sympy_mappings={\"inv\": lambda x:np.exp(1/x) },\r\n    **default_pysr_params,\r\n)\r\nX_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\r\nmodel.fit(X_train, y_train)\r\nmodel.sympy()\r\n```",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hi @Enqiliu125, did you try the tips on https://astroautomata.com/PySR/tuning/?",
                "createdAt": "2024-06-02T22:51:00Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0wMlQyMzo1MTowMCswMTowMM4Akx01"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 659,
          "title": "Cross-validation error as PySR objective function?",
          "body": "Hello. As the title suggests, I am trying to write a custom objective function based on the cross-validation error. However, I am not really well-versed in Julia. This is my amateur attempt at writing a cross-validation error-based objective function:\r\n\r\n```\r\nfrom pysr import PySRRegressor\r\nimport numpy as np\r\n\r\n# Writing an objective function based on 60/40 cross-validation error\r\nobjective = \"\"\"\r\nusing SymbolicRegression\r\n\r\n# Function to perform 60/40 cross-validation\r\nfunction cross_validation_objective(tree, dataset::Dataset{T, L}, options)::L where {T, L}\r\n    n = dataset.n\r\n    train_size = Int(round(0.6 * n))\r\n    train_idx = 1:train_size\r\n    valid_idx = (train_size + 1):n\r\n\r\n    train_idx = filter(x -> x <= n, train_idx)\r\n    valid_idx = filter(x -> x <= n, valid_idx)\r\n\r\n    train_data = Dataset(dataset.X[train_idx, :], dataset.y[train_idx])\r\n    valid_data = Dataset(dataset.X[valid_idx, :], dataset.y[valid_idx])\r\n\r\n    prediction_valid, flag_valid = eval_tree_array(tree, valid_data.X, options)\r\n\r\n    if !flag_valid\r\n        error = sum((prediction_valid .- valid_data.y) .^ 2) / length(valid_idx)\r\n        return error\r\n    else\r\n        return L(Inf)\r\n    end\r\nend\r\n\"\"\"\r\n\r\nmodel = PySRRegressor(\r\n    niterations=100,\r\n    populations=20,\r\n    loss_function=objective,\r\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\r\n    verbosity=1,\r\n)\r\n\r\n# Example dataset\r\nX = np.random.rand(100, 5)  # 100 samples, 5 features\r\ny = np.random.rand(100)     # 100 target values\r\n\r\n# Use PySR for model fitting\r\nmodel.fit(X, y)\r\n\r\n```\r\n\r\nI am getting a Julia error if I use this code. It appears to be related to accessing invalid index ranges. Is there already a code for some sort of cross-validation error already available here?",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Maybe just do it from the Python side? It should be faster too as then you aren't doing the split every single evaluation, but only once:\r\n\r\n```python\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\r\n\r\nmodel.fit(X_train, y_train)\r\n\r\ntrain_loss = np.mean(np.square(y_train - model.predict(X_train, index=-1)))\r\ntest_loss = np.mean(np.square(y_test - model.predict(X_test, index=-1)))\r\n```\r\n\r\n(And, by the way, Julia indexes with column-major order, so you would write the row first, feature second, like `X[:, train_idx]`)",
                "createdAt": "2024-07-04T08:51:30Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0wNFQwOTo1MTozMCswMTowMM4Al-0E"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 644,
          "title": "Running SR on a distributed cluster",
          "body": "We have been SymbolicRegression.jl for our research but have hit a point where the equation search takes too long on a single compute node (with ~16 cores or so). We're now looking into using our distributed computing resources (we have both an MPI cluster as well as a slurm HPC cluster available) and were wondering if you have used SR in such an environment before or might know someone who has? we're hoping to not having to re-invent the wheel for writing the entire orchestration code (copy from slack DM)",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hey Christian,\r\n\r\nYes, I use SR like this in my own work. Basically if you just set `parallelism=:multiprocessing` then you can either:\r\n\r\n1. Pass the process objects explicitly to the procs parameter (whether those procs are on the same node, or multiple nodes, etc.)\r\n2. Or, set, for example, `numprocs=num_nodes * num_cores, addprocs_function=addprocs_slurm` , and SR.jl will try its best to set it up for you\r\n\r\nI usually do the 2nd out of convenience\r\n\r\nJust be sure to launch SR only once, on a single node, from a single task on the slurm job. ClusterManagers.jl will run srun internally for you.",
                "createdAt": "2024-06-07T10:27:35Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0wN1QxMToyNzozNSswMTowMM4AlARh"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 668,
          "title": "Optimizing Objective Function by Reducing Variable Complexity",
          "body": "Hello everyone,\r\n\r\nI need the 4 trees in the following objective function to be functions of only variables 3, 4, and 8. However, my objective function requires 18 inputs. Searching between 18 variables requires more iterations. Is there an option for a custom objective function to be of the form my_custom_objective(tree, dataset::Dataset{T,L}, options, constant_dataset::Dataset{T,L}) where {T,L}, where dataset contains only variables 3, 4, and 8 that the trees use, and constant_dataset contains the remaining variables used for calculating the objective function? This will speed up the training.\r\n\r\n```python\r\nobjective = \"\"\"\r\nfunction my_custom_objective(tree, dataset::Dataset{T,L}, options) where {T,L}\r\n    penalty_term = L(0)\r\n    tot_features = 1:18\r\n    concrete_contribution = L(100)\r\n    web_reinf_contribution = L(100)\r\n    tol = L(0.0000000000000001)\r\n\r\n    r_EIs = dataset.X[1, :]\r\n    r_EIc = dataset.X[2, :]\r\n    eta = dataset.X[3, :]\r\n    e_ratio = dataset.X[4, :]\r\n    e_d = dataset.X[5, :]\r\n    e1_d = dataset.X[6, :]\r\n    lam = dataset.X[7, :]\r\n    slender = dataset.X[8, :]\r\n    ee = dataset.X[9, :]\r\n    ee1 = dataset.X[10, :]\r\n    zf_c = dataset.X[11, :]\r\n    zf_s = dataset.X[12, :]\r\n    EcIc = dataset.X[13, :]\r\n    EsIs = dataset.X[14, :]\r\n    pc1 = dataset.X[15, :]\r\n    ps1 = dataset.X[16, :]\r\n    Ll = dataset.X[17, :]\r\n    f_f = dataset.X[18, :]\r\n\r\n    TL1 = 0.94 .+ 0.003 .* f_f ./ lam\r\n    fac = 0.1\r\n    TL2 = 1.15\r\n    TR1 = 1.1 .+ slender .* fac\r\n    TR2 = 1.0\r\n    prop = 0.0001 .* dataset.y\r\n\r\n    if tree.degree != 2\r\n        penalty_term += L(10000)\r\n    else\r\n        left = tree.l\r\n        if left.degree != 2\r\n            penalty_term += L(1000)\r\n        else\r\n            fn = left.l\r\n            features = [3, 4, 8]\r\n            remain_feat = setdiff(tot_features, features)\r\n            should_nt = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in remain_feat\r\n            ]\r\n            should = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in features\r\n            ]\r\n            penalty_term +=\r\n                L(100) * (sum(should_nt)) + L(100) * (length(features) - sum(should))\r\n            TL1, flag = eval_tree_array(fn, dataset.X, options)\r\n            if !flag\r\n                return L(Inf)\r\n            end\r\n\r\n            fn = left.r\r\n            features = [3, 4, 8]\r\n            remain_feat = setdiff(tot_features, features)\r\n            should_nt = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in remain_feat\r\n            ]\r\n            should = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in features\r\n            ]\r\n            penalty_term +=\r\n                L(100) * (sum(should_nt)) + L(100) * (length(features) - sum(should))\r\n            TL2, flag = eval_tree_array(fn, dataset.X, options)\r\n            if !flag\r\n                return L(Inf)\r\n            end\r\n        end\r\n        right = tree.r\r\n        if right.degree != 2\r\n            penalty_term += L(1000)\r\n        else\r\n            fn = right.l\r\n            features = [3, 4, 8]\r\n            remain_feat = setdiff(tot_features, features)\r\n            should_nt = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in remain_feat\r\n            ]\r\n            should = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in features\r\n            ]\r\n            penalty_term +=\r\n                L(100) * (sum(should_nt)) + L(100) * (length(features) - sum(should))\r\n            TR1, flag = eval_tree_array(fn, dataset.X, options)\r\n            if !flag\r\n                return L(Inf)\r\n            end\r\n\r\n            fn = right.r\r\n            features = [3, 4, 8]\r\n            remain_feat = setdiff(tot_features, features)\r\n            should_nt = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in remain_feat\r\n            ]\r\n            should = [\r\n                any(n -> n.degree == 0 && n.constant == false && n.feature == f, fn) for\r\n                f in features\r\n            ]\r\n            penalty_term +=\r\n                L(100) * (sum(should_nt)) + L(100) * (length(features) - sum(should))\r\n            TR2, flag = eval_tree_array(fn, dataset.X, options)\r\n            if !flag\r\n                return L(Inf)\r\n            end\r\n        end\r\n    end\r\n\r\n    if penalty_term < tol\r\n        f_fc_m = 1.0\r\n        f_fy_m = 1.0\r\n        MD = (zf_s .* f_fy_m .+ 0.5 .* zf_c .* f_fc_m)\r\n        f_fc = min.(1.5, max.(TL1, 0.6))\r\n        f_fy = min.(1.5, max.(TL2, 0.6))\r\n        TR1 = min.(2.0, max.(TR1, 0.3))\r\n        TR2 = min.(2.0, max.(TR2, 0.3))\r\n\r\n        pa = ps1 .+ pc1\r\n        pc = pc1\r\n        #println(\"Size of the ps1: \", size(ps1))\r\n        #println(\"Size of the f_fy: \", size(f_fy))\r\n        #println(\"Size of the f_fc: \", size(f_fc))\r\n        #println(\"Size of the pc1: \", size(pc1))\r\n        #println(\"Size of the pa: \", size(pa))\r\n        pa = max.(pa, pc1 .* f_fc .+ f_fy .* ps1)\r\n\r\n        b1 = TR2 .* (0.6 .+ 0.4 .* ee1 ./ ee)\r\n        EI2 = 0.9 .* (EsIs .+ EcIc .* 0.5)\r\n        Ncr = (π^2 .* EI2) ./ (Ll .^ 2)\r\n\r\n        ec = max.(Ll ./ 1000, ee .* b1)\r\n\r\n        for j in 1:length(prop)\r\n            ij = 0\r\n            iter = 1000\r\n            incr = 4\r\n            while ij < iter\r\n                ij += incr\r\n                nn = ij / iter * pa[j]\r\n                m = nn * max(ee[j], ec[j] / (1 - nn / Ncr[j] * TR1[j]))\r\n                ratto_i = (nn + pa[j] - pc[j]) / (pa[j] - pc[j] / 2)\r\n\r\n                th = ratto_i * π\r\n                for k in 1:100\r\n                    thy = sin(th) + ratto_i * π\r\n                    if abs(thy - th) < 1e-3\r\n                        break\r\n                    end\r\n                    th = thy\r\n                end\r\n\r\n                mm = MD[j] * sin(th / 2)^3\r\n                net = m - mm\r\n                if net > 0.0\r\n                    ij -= incr\r\n                    incr *= 0.5\r\n                    if incr < 0.13\r\n                        break\r\n                    end\r\n                end\r\n                prop[j] = nn / 1000\r\n            end\r\n        end\r\n    end\r\n\r\n    r = dataset.y ./ prop .- 1.0\r\n    MAPE = sum(abs.(r)) ./ length(r)\r\n    return (penalty_term + MAPE)\r\nend\r\n\r\nmy_custom_objective\r\n\"\"\"\r\nfrom pysr import PySRRegressor\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nPC  = pd.read_excel(\"da.xlsx\",'M')[['D','t','L','Fy','fc','et','eb','P']]\r\nD=PC['D'];t=PC['t'];L1=PC['L'];fy=PC['Fy'];fc=PC['fc'];et=PC['et'];eb=PC['eb'];P=PC['P']\r\nee = np.maximum(np.abs(et), np.abs(eb))\r\nee1=np.sign(et * eb) * np.minimum(np.abs(et), np.abs(eb))\r\nAco = (D - 2 * t) ** 2 / 4 * np.pi\r\nAso = D ** 2 / 4 * np.pi - Aco\r\nzco = (D - 2 * t) ** 3 / 6; zf_c = zco * fc\r\nzso = (D) ** 3 / 6 - zco;   zf_s = zso * fy\r\nIc = (D - 2 * t) ** 4 / 64 * np.pi\r\nIs = (D) ** 4 / 64 * np.pi - Ic\r\nEs = 210000\r\nEc = 22000 * ((fc + 8) / 10) ** 0.3  ;EsIs = Is * Es  ; EcIc = Ic * Ec;r_EIs=EsIs/(EsIs+EcIc);r_EIc=EcIc/(EsIs+EcIc)\r\npc1=Aco * fc;ps1=Aso * fy;eta=pc1/ps1;e_ratio=ee1/ee\r\nlam = D / t / Es * fy\r\n\r\nf_fc = 1.0\r\npc = pc1 * f_fc\r\npa = pc + ps1\r\nEI1 = EsIs + EcIc * 0.6\r\nNcr = (np.pi ** 2 * EI1) / (L1 * L1)\r\nslender = (pa / Ncr) ** 0.5\r\n\r\nPC['r_EIs']=r_EIs;PC['r_EIc']=r_EIc;PC['eta']=eta;PC['e_ratio']=e_ratio;PC['ee']=ee;PC['ee1']=ee1;PC['e_d']=ee/D;PC['e1_d']=ee1/D\r\nPC['Aco']=Aco;PC['Aso']=Aso;PC['EcIc']=EcIc;PC['EsIs']=EsIs;PC['lam']=lam;PC['slender']=slender\r\nPC['pc1']=pc1;PC['ps1']=ps1;PC['Ll']=L1;PC['zf_s']=zf_s;PC['zf_c']=zf_c;PC['f_f']=fy/fc\r\nfeatures=['r_EIs','r_EIc','eta','e_ratio','e_d','e1_d','lam','slender','ee','ee1','zf_c','zf_s','EcIc','EsIs','pc1','ps1','Ll','f_f']\r\n\r\nX=PC[features];y=PC['P']\r\n\r\nmodelior11=[]\r\nfor j in range(1):\r\n    #population_size is the equation size\r\n    model7 = PySRRegressor(niterations=200,populations=40,population_size=100,maxsize=80,\r\n                      nested_constraints={\"^\":{\"^\":1}},parsimony= 0.02,#adaptive_parsimony_scaling=1000,\r\n                      constraints={\"^\":(-1,10)},\r\n                      binary_operators=[\"+\", \"*\",\"^\"],\r\n                      denoise=True,loss_function=objective,model_selection ='accuracy',\r\n                      )\r\n    model7.fit(X,y)\r\n    i=0\r\n    modelior11.append(model7)\r\n    while i <len(model7.equations_):\r\n        print(model7.sympy(i))\r\n        i=i+1\r\n```",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Initial tips:\r\n\r\nI ran your loss function through `@code_warntype` in Julia to check whether there were any type instabilities, and it looks like there are a few:\r\n\r\n```julia\r\njulia> using SymbolicRegression\r\n\r\njulia> X = randn(Float32, 5, 256); y = randn(Float32, 256);\r\n\r\njulia> dataset = Dataset(X, y);\r\n\r\njulia> tree = Node{Float32}(val=0.1);\r\n\r\njulia> options = Options();\r\n\r\njulia> @code_warntype my_custom_objective(tree, dataset, options)\r\n```\r\n\r\nThis will highlight type instabilities in red which can hurt performance.\r\n\r\nIn particular it looks like it can't figure out the type of \r\n\r\n```julia\r\n  penalty_term::Any\r\n  should::Vector\r\n  should_nt::Vector\r\n```\r\n\r\nand there are also various types that are `Union{Float32, Float64}` which means Julia doesn't know if they are a 32-bit float or 64-bit float. To get rid of that you can use `penalty_term += L(...)` to convert to the right type (which in this case is `Float32`), because `Dataset{T,L}` will tell the compiler what `L` is.\r\n\r\nIt looks like this means the inferred return value of the custom objective is `Any` - which will slow things down a lot. Here is a page in the docs with more info: https://docs.julialang.org/en/v1/manual/performance-tips/#man-code-warntype.",
                "createdAt": "2024-07-13T13:54:27Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Now, for passing a constant dataset, you could set a constant global variable in Julia, and then access those from your loss function? For example:\r\n\r\n```python\r\nfrom pysr import jl\r\n# ^jl is the Julia runtime\r\n\r\nfrom juliacall import convert\r\n\r\ndef create_const(x, const_name: str):\r\n    jl.seval(f\"x -> @eval const {const_name} = convert(Array, $x)\")(x)\r\n```\r\n\r\nMake sure to convert the numpy array you pass this to the right type using e.g., `.astype(np.float32)`. for example:\r\n\r\n```python\r\ncreate_const(np.random.randn(100).astype(np.float32), \"my_constant\")\r\n\r\njl.my_constant  # Now accessible within Julia\r\n```\r\n\r\nThen you can use \"`my_constant`\" directly within the loss.\r\n\r\nJust make sure your loss function creates a copy of the constant globals so it doesn't modify them. e.g., `loc_r_EIs = copy(r_EIs)`.\r\n\r\n",
                "createdAt": "2024-07-13T14:04:25Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0xM1QxNTowNDoyNSswMTowMM4AmS-m"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 633,
          "title": "introduce specific expressions in the inital population",
          "body": "Hi,\r\nI am wondering if we can tell pysr that include specific expressions in the inital population. E.g. assume : I know that three expressions y=x1**2+1, y=x**1.5+2, and y=exp(x1/10)/5 could kinda be close to the final solution, and I am interested that pysr includes these specific epxressions in the initial population. Is there any built-in feature in pysr to address such a request? \r\n\r\nAny advice is much appreciated.  ",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "See:\r\n\r\n- https://github.com/MilesCranmer/PySR/discussions/418\r\n- https://github.com/MilesCranmer/PySR/discussions/443",
                "createdAt": "2024-06-02T22:50:15Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0wMlQyMzo1MDoxNSswMTowMM4Akx0y"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 664,
          "title": "At the same time meet the dimension and a priori",
          "body": "Hi Miles,\r\nIn my research, I need to make some kind of a priori of the equation form, and at the same time, I need to satisfy the balance of dimensions. However, when I custom-designed the loss, I found that the penalty of the dimensional part was offset. Here is my code example :\r\n```python\r\nx = np.random.rand(1000,3)\r\nones_array = np.ones((1000, 1))\r\nX = np.hstack((x, ones_array))\r\ny = X[:, 0]/(X[:, 1]*X[:, 2])+X[:, 1]/(X[:, 0]*X[:, 0]*X[:, 3])\r\n\r\nobjective = \"\"\"\r\nfunction my_custom_objective(tree, dataset::Dataset{T}, options) where {T<:Real}\r\n.....my loss.....(not contain dimension)\r\n\"\"\"\r\nmodel = PySRRegressor(\r\n    population_size=50,\r\n    ncyclesperiteration=500,\r\n    niterations=800,\r\n    early_stop_condition=(\"stop_if(loss, complexity) = loss < 1e-5\"),\r\n    binary_operators=[\"+\",\"-\",\"*\",\"/\"],\r\n    unary_operators=[\"square\"],\r\n    complexity_of_constants=100,\r\n    maxsize=40,\r\n    maxdepth=40,    \r\n    loss_function=objective,\r\n    dimensional_constraint_penalty=10**4,\r\n)\r\nmodel.fit(X, y, X_units=[\"Pa\", \"Pa\", \"1\", \"1/Pa\"],y_units=[\"1\"])\r\n```\r\nI found that the above loss does not deal with the problem of dimension.\r\nCould you help me see how to add dimensional constraints to custom loss ?\r\nLooking forward to your reply! Thanks.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "The default loss is here: https://github.com/MilesCranmer/SymbolicRegression.jl/blob/cd23a6e25c64d00565c3ae3905d06dc3c63033ed/src/LossFunctions.jl#L45-L75\r\n\r\nIn particular the dimensional regularization is calculated on this line: https://github.com/MilesCranmer/SymbolicRegression.jl/blob/cd23a6e25c64d00565c3ae3905d06dc3c63033ed/src/LossFunctions.jl#L71\r\n\r\nSo within a custom loss you could add a manual call, like\r\n\r\n```julia\r\nloss_val += SymbolicRegression.LossFunctionsModule.dimensional_regularization(tree, dataset, options)\r\n```",
                "createdAt": "2024-07-10T16:03:46Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0xMFQxNzowMzo0NiswMTowMM4AmMWE"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 625,
          "title": "Custom loss function with Hessians",
          "body": "Hello @MilesCranmer, and thank you for making PySR!\r\n\r\nI would like to write a custom loss function that looks like this:\r\n\r\n```julia\r\nfunction loss_hess(tree, dataset::Dataset{T,L}, options) where {T,L}\r\n    X = copy(dataset.X)  # shape (n, x)\r\n    y = copy(dataset.y)  # shape (n)\r\n\r\n    features = custom_fn1(X)  # shape (n, f)\r\n\r\n    y_pred = eval_tree_array(tree, features, options)\r\n    hess = batch_hessian(y_pred, X)  # shape (n, x, x)\r\n\r\n    return L(custom_fn2(X, y, hess))\r\nend\r\n```\r\n\r\nThat is, I need to do symbolic regression on`y`, but the loss function should include Hessian wrt. some parameters `X` from which input `features` are derived. Is something like this possible with PySR? I saw that `enable_autodiff` and `eval_grad_tree_array` exist, but I need Hessian and not gradient.\r\n\r\nThank you very much in advance.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hi @mirjanic,\r\n\r\nMaybe you could use `eval_grad_tree_array` to get the 1st order derivatives, and then use [finite difference](https://en.wikipedia.org/wiki/Finite_difference) to get the 2nd order differences? It might even be faster than computing the exact Hessian.\r\n\r\nAlso check through other discussions in the forums, there are a few about including derivatives in the custom loss that might be useful.\r\n\r\nCheers,\r\nMiles\r\n\r\nP.S., Another option is Enzyme.jl as explained here: https://symbolicml.org/DynamicExpressions.jl/dev/eval/#Enzyme. However, this is experimental and won't be easy. It will require you to get pretty deep into the Julia codebase. But if this is really important for you then it's worth considering. I do think that Enzyme will eventually be **the way** to do this type of thing, but it hasn't reached a level of stability where I'm comfortable recommending it as the first thing to reach for.\r\n",
                "createdAt": "2024-05-13T14:11:42Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNS0xM1QxNToxMTo0MiswMTowMM4Aj8FZ"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 673,
          "title": "[Feature]: Custom loss function implementation in python",
          "body": "### Feature Request\n\nHello. I would like to have a feature, where I can pass custom function loss implemented in python instead of in julia. I'm not able to implement it in julia as it part of bigger optimization pipeline. I believe this feature can be implemented by passing python function pointer to julia and executing it from there.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Maybe you could try PythonCall.jl? https://juliapy.github.io/PythonCall.jl/stable/pythoncall/ It lets you call Python functions from Julia. See https://astroautomata.com/PySR/examples/#7-julia-packages-and-types for an example of using an external Julia package in the loss function.",
                "createdAt": "2024-07-16T22:52:34Z"
              },
              {
                "author": {
                  "login": "xnerhu"
                },
                "body": "> Maybe you could try PythonCall.jl? https://juliapy.github.io/PythonCall.jl/stable/pythoncall/ It lets you call Python functions from Julia. See https://astroautomata.com/PySR/examples/#7-julia-packages-and-types for an example of using an external Julia package in the loss function.\r\n\r\nIt somewhat worked thanks.\r\n\r\n```\r\nimport os\r\n\r\n\r\nos.environ[\"PYTHON_JULIACALL_THREADS\"] = \"1\"\r\nfrom datetime import datetime\r\nimport numpy as np\r\nfrom pysr import PySRRegressor\r\nfrom pysr import jl\r\n\r\nfrom julia.api import Julia\r\n\r\n\r\nx = None\r\n\r\n\r\ndef custom_loss(y_true, y_pred):\r\n    # global x\r\n    # print(y_true, y_pred, x)\r\n    # if x is None:\r\n    #     x = np.random.rand()\r\n    return float((y_true - y_pred) ** 2)\r\n\r\n\r\njl.seval(\r\n    \"\"\"\r\nimport Pkg\r\nPkg.add(\"PythonCall\")\r\n\"\"\"\r\n)\r\njl.custom_loss_function = custom_loss\r\n\r\njl.seval(\r\n    \"\"\"\r\nfunction custom_loss_wrapper(y_true, y_pred)\r\n    py_obj = PythonCall.pycall(custom_loss_function, y_true, y_pred)\r\n    return PythonCall.pyconvert(Float32, py_obj)\r\nend\r\n\"\"\"\r\n)\r\n\r\nX = np.random.rand(100, 2)\r\ny = X[:, 0] * X[:, 1] + np.random.rand(100) * 0.1\r\n\r\nmodel = PySRRegressor(\r\n    niterations=40,\r\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\r\n    unary_operators=[\"cos\", \"exp\"],\r\n    elementwise_loss=\"custom_loss_wrapper\",\r\n)\r\n\r\nmodel.fit(X, y)\r\nprint(model)\r\npredictions = model.predict(X)\r\nprint(predictions)\r\n\r\n```\r\n\r\nThough I had to do `os.environ[\"PYTHON_JULIACALL_THREADS\"] = \"1\"` from https://github.com/MilesCranmer/PySR/issues/661\r\n\r\nbecause of\r\n\r\n```\r\n  Resolving package versions...\r\n  No Changes to `C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\Project.toml`\r\n  No Changes to `C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\Manifest.toml`\r\nCompiling Julia backend...\r\n\r\nPlease submit a bug report with steps to reproduce this fault, and any error messages that follow (in their entirety). Thanks.\r\nException: EXCEPTION_ACCESS_VIOLATION at 0x7ff85e1587a5 --  at 0x7ff85e1587a5 -- ION with steps to reproduce this fault, and any error messages that follow (in their entirety). Thanks.\r\nException: EXCEPTION_ACCESS_VIOLATION at 0x7ff85e163a4d -- PyObject_GC_Malloc at C:\\Python310\\python310.dll (unknown line)  \r\nnd any error messages that follow (in their entirety). Thanks.\r\nException: EXCEPTION_ACCESS_VIOLATION at 0x7ff85e1587a5 --  at 0x7ff85e1587a5 -- IONC:\\Python310\\python310.dll (unknown lin expression starting at none:0\r\n\\python310.dll (unknown line)\r\nin expression starting at none:0\r\nin expression starting at none:0\r\n\\python310.dll (unknown line)\r\nne)\r\nin expression starting at none:0\r\nin expression starting at none:0\r\n\\python310.dll (unknown line)\r\nPyEval_EvalFrameDefault at C:\\PytPyEval_EvalFrameDefault at C:\\Python310\\python310.dll (unknown line)\r\nPyFunction_Vectorcall at C:\\Python310\\python310.dll (unknown line)\r\n\r\nPyFunction_Vectorcall at C:\\Python310\\python310.dll (unknown linyFunction_Vectorcall at C:\\Python310\\python310.dll (unknown line)\r\nPyVectorcall_Call at C:\\Python310\\python310.dll (unknown line)\r\ne)\r\nPyVectorcall_Call at C:\\Python310\\python310.dll (unknown line)\r\nPyVectorcall_Call at C:\\Python310\\python310.dll (unknown line)\r\nPyObject_Call at efault at C:\\Python310\\python310.dll (unknown PyObject_Call at C:\\Python310\\python310.dll (unknown line)   \r\nwn line)\r\nPyObject_Call at C:\\Python310\\python310.dll (unknown line)\r\nPyFunction_Vectorcall at C:\\Python310\\python310.dll (unknowPyFunction_Vectorcall at C:\\Python310\\python310.dll (unknown line)\r\nPyObject_CallObject at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\C\\pointers.jl:297 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\Py.jl:132 [inlined]\r\npycallargs at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:212\r\nunknown function (ip: 00000138b4d57ddf)\r\n#pycall#21 at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:230\r\n#pycall#21 at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\#pycall#21 at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:230\r\nnlined]\r\nined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\PythonCamacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\Py.jl:132 [inlined]\r\npycallargs at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:212\r\njl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\ndo_apply at C:/workdir/src\\builtins.c:768\r\ndo_apply at C:/workdir/src\\builtins.c:76pycall at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:220 \r\npycall#21 at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:230\r\n:297 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\Py.jl:132 [inlined]\r\npycallargs at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:212\r\nunknown function (ip: 00000138b4d57ddf)\r\nages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:212\r\nunknown function (ip: 00000138b4unknown function (ip: 00000138b4d57ddf)\r\nl at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:19 [inlined]\r\n#4 at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:187 [inlined]\r\nmapreduce_impl at .\\reduce.jl:262\r\nmapreduce_impl at .\\reduce.jl:262\r\nnments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdljl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\ndo_apply at C:/workdir/src\\builtins.c:768\r\npycall at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:220\r\ncustom_loss_wrapper at .\\none:2\r\nl at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:19 [inlined]\r\n#4 at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:187 [inlined]\r\nmapreduce_impl at .\\reduce.jl:262\r\nmapreduce_impl at .\\reduce.jl:277 [inlined]\r\n_mapreduce at .\\reduce.jl:447\r\njl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\ndo_apply at C:/workdir/src\\builtins.c:768\r\npycall at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:220\r\npycall at C:\\Users\\xnerhu\\.julia\\packages\\Pytho#mapreduce#821 at .\\reducedim.jl:357 [inlined]\r\nmapreduce at .\\reducedim.jl:357 [inlined]\r\n#_sum#831 at .\\reducedim.jl:1015 [inlined]\r\n_sum at .\\reducedim.jl:1015 [inlined]\r\n#sum#829 at .\\reducedim.jl:1011 [inlined]\r\nsum at .\\reducedim.jl:1011 [inlined]\r\n_mean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:187\r\n_mean at C:\\Users\\xnerhu\\.julia\\_mean at C:\\Users\\xnerhu\\.julia\\environments_mapreduce at .\\reduce.jl:447\r\nia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\nmean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\n_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\nunknown function (ip: 00000138b4df92cf)\r\nunknown function (ip: 00000138_mapreduce_dim at .\\reducedim.jl:365 [inlined]\r\n#mapreduce#821 at .\\reducedim.jl:357 [inlined]\r\nmapreduce at .\\reducedim.jl:357 [inlined]\r\n#_sum#831 at .\\reducedim.jl:1015 [inlined]\r\n_sum at .\\reducedim.jl:1015 [inlined]\r\n#sum#829 at .\\reducedim.jl:1011 [inlined]\r\nsum at .\\reducedim.jl:1011 [inlined]\r\n_mean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:187\r\n_mean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1_mean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:186\r\n_eval_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:67\r\n#mean#1 at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\nmean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\n_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\njl:105\r\n_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\nunknown function (ip: 00000138b4df92cf)\r\nunknown function (ip: 00000138b4df92cf)\r\nSymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\njl:105\r\nStatistics\\src\\Statistics.jl:104 [inlined]\r\n]\r\nmean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\n_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\nunknown function (ip: 00000138b4df92cf)\r\nSymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\nb\\v1.10\\Stati#score_func#5 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:164 [inlined]\r\nscore_func at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:161 [inlined]\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\p#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n1#eval_loss#3 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:105\r\n_eval_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:67\r\neval_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:97 [inlined]\r\n#score_func#5 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:164 [inlined]\r\nscore_func at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:161 [inlined]\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n1 [ieval_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:97 [inlinPopMember at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\n#2 at .\\none:0 [inlined]\r\niterate at .\\generator.jl:47 [inlined]\r\nages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\nPopMember at C:\\Users\\xnerhu\\.julia\\paccollect at .\\array.jl:834\r\ncollect at .\\array.jl:834\r\nu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\n#2 at .\\none:0 [inlined]\r\niterate at .\\generator.jl:47 [inlined]\r\ncollect at .\\array.jl:834\r\n#Population#1 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:49 [inlined]#score_func#5 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:164 [inlined]\r\nscore_func at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:161 [inlined]\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n1 [inlined]Population at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:35 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SymbolicRegression.jl:765 [inlined]\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\n8 [inlined]\r\nPopMember at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\n#2 at .\\none:0 [inlined]\r\niterate at .\\generator.jl:47 [inlined]\r\ncollect at .\\array.jl:834\r\ncollect at .\\array.jl:834\r\n7 [inlined]\r\nages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\n:765Population at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:35 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SymbolicRegression.jl:765 [inlined]\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\njfptr_YY.54_21945 at C:\\Users\\xnerhu\\.julia\\compiled\\v1.10\\SymbolicRegression\\X2eIS_JE3Mg.dll (unknown line)\r\n#Population#1 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:49 [inlined]\r\nPopulation at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:35 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SymbolicRegression.jl:765 [inlined]\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\nRegression.jl:765 [injl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\nstart_task at C:/workdir/src\\task.c:1238\r\nAllocations: 17550813 (Pool: 17526745; Big: 24068); GC: 21\r\njfptr_YY.54_21945 at C:\\Users\\xnerhu\\.julia\\compiled\\v1.10\\SymbolicRegression\\X2eIS_JE3Mg.dll (unknown line)\r\njl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\nstart_task at C:/workdir/src\\task.c:1238\r\nAllocations: 17550813 (Pool: 17526745; Big: 24068); GC: 21\r\n```\r\n\r\nalso, how can I do loss_function intead of elemenetwise_loss? I think seval needs to have explicit types in args\r\n\r\n```\r\njuliacall.JuliaError: MethodError: no method matching custom_loss_wrapper(::Node{Float32}, ::Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}, ::Options{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, Node, false, false, nothing, StatsBase.Weights{Float64, Float64, Vector{Float64}}})\r\n```\r\n",
                "createdAt": "2024-07-17T09:12:07Z"
              },
              {
                "author": {
                  "login": "xnerhu"
                },
                "body": "btw is it possible to do multivaritive prediction (multiple outpouts)?",
                "createdAt": "2024-07-17T09:24:36Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "> Though I had to do `os.environ[\"PYTHON_JULIACALL_THREADS\"] = \"1\"` from https://github.com/MilesCranmer/PySR/issues/661\r\n\r\nSo if you are calling back into Python, you can't use multithreading or multiprocessing because of Python's global interpreter lock. So you will need to run PySR with `multithreading=False, procs=0` as options to PySRRegressor. For this reason, and because of the overhead of Python in general, it will be much slower than if you were to write your code as a Julia function.\r\n\r\n\r\n",
                "createdAt": "2024-07-17T11:25:21Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "> also, how can I do loss_function intead of elemenetwise_loss? I think seval needs to have explicit types in args\r\n\r\n`seval` does not need explicit types. The error is because `loss_function` takes 3 arguments whereas you have implemented a 2-argument function. If you read through the other discussion threads there are several examples of complex `loss_function` implementations: https://github.com/MilesCranmer/PySR/discussions. Also the docs have an example: https://astroautomata.com/PySR/examples/#9-custom-objectives",
                "createdAt": "2024-07-17T11:27:28Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0xN1QxMjoyNzoyOCswMTowMM4AmbH5"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 665,
          "title": "AttributeError: module 'pysr' has no attribute 'Problem'",
          "body": "I'm just getting started with PySR. Walking through the Toy Examples with Code. When I do \r\n```python\r\nfrom pysr import *\r\n```\r\nI get the error in the title:\r\n```\r\nAttributeError: module 'pysr' has no attribute 'Problem'\r\n```\r\nI am running IPython in an Apptainer container built from the `continuumio/miniconda3` Docker image.  \r\n\r\nWhen I switch to \r\n```python\r\nimport pysr\r\n```\r\nand preface things with `pysr.` as required everything works as expected.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Looks like a bug. I made an issue #666 to track it.\r\n\r\nEdit: Fixed.",
                "createdAt": "2024-07-11T01:42:44Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0xMVQwMjo0Mjo0NCswMTowMM4AmNQI"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 632,
          "title": "How to obtain tensor expression of fixed form equation by pysr?",
          "body": "Hi!\r\nI want to use pysr to train the expressions $f_{1}$, $f_{2}$, $f_{3}$ in the following fluid mechanics tensor equations：\r\n$\\tau_{\\mathrm{ij}}=\\frac{1-f_1(I_{1_{ij}},I_{2_{ij}})}3\\times T_{1_{ij}}+\\frac{f_2(I_{2_{ij}},I_{3_{ij}})}6\\times T_{2_{ij}}+\\frac{1-f_3(I_{3_{ij}},I_{4_{ij}})}3\\times T_{3_{ij}}$\r\nwhere the subscrip i=1,2,3; j=1,2,3; $\\tau,I_1,I_2,I_3,I_4,T_1,T_2,T_3$ are all in the form of a 3×3 tensor matrix.\r\nHow do I get $f_{1}$(expressions about $I_{1_{ij}}$ and $I_{2_{ij}}$) , $f_{2}$ (expressions about $I_{2_{ij}}$ and $I_{3_{ij}}$) , $f_{3}$ (expressions about $I_{3_{ij}}$ and $I_{4_{ij}}$)  from pysr?\r\nMuch appreciated!",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hi, can you please provide more info about what you've tried and what issues you are facing? Thanks",
                "createdAt": "2024-06-02T22:52:12Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNi0wMlQyMzo1MjoxMiswMTowMM4Akx07"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 628,
          "title": "Can we use GPU for accelerating SR?",
          "body": "Any chance we can use an nvidia gpu for accelerating an SR workload?\r\nI am not familiar with julia. has anyone looked into it?",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "See https://github.com/SymbolicML/DynamicExpressions.jl/pull/65. Help appreciated!",
                "createdAt": "2024-05-21T06:07:01Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNS0yMVQwNzowNzowMSswMTowMM4AkQVW"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 669,
          "title": "ValueError: Expression (loss) <= (5.050614450000001e-13) has forbidden control characters. when loss is very low, occur the error, how can i solve it?",
          "body": "when I run the demo\r\n```python\r\n# Learn equations\r\nmodel = PySRRegressor(\r\n    niterations=30,\r\n    binary_operators=[\"+\", \"*\"],\r\n    unary_operators=[\"cos\", \"exp\", \"sin\"],\r\n    early_stop_condition=(\r\n        \"stop_if(loss, complexity) = loss < 1e-10 && complexity < 12\"\r\n        # Stop early if we find a good and simple equation\r\n    ),\r\n    **default_pysr_params,\r\n)\r\n\r\nmodel.fit(X, y)\r\n```\r\nit seems the loas is lower than some threshould, but i don not know how to solve it\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\IPython\\core\\formatters.py:974, in MimeBundleFormatter.__call__(self, obj, include, exclude)\r\n    971     method = get_real_method(obj, self.print_method)\r\n    973     if method is not None:\r\n--> 974         return method(include=include, exclude=exclude)\r\n    975     return None\r\n    976 else:\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\sklearn\\base.py:669, in BaseEstimator._repr_mimebundle_(self, **kwargs)\r\n    667 def _repr_mimebundle_(self, **kwargs):\r\n    668     \"\"\"Mime bundle used by jupyter kernels to display estimator\"\"\"\r\n--> 669     output = {\"text/plain\": repr(self)}\r\n    670     if get_config()[\"display\"] == \"diagram\":\r\n    671         output[\"text/html\"] = estimator_html_repr(self)\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pysr\\sr.py:1082, in PySRRegressor.__repr__(self)\r\n   1080 for i, equations in enumerate(all_equations):\r\n   1081     selected = pd.Series([\"\"] * len(equations), index=equations.index)\r\n-> 1082     chosen_row = idx_model_selection(equations, self.model_selection)\r\n   1083     selected[chosen_row] = \">>>>\"\r\n   1084     repr_equations = pd.DataFrame(\r\n   1085         dict(\r\n   1086             pick=selected,\r\n   (...)\r\n   1091         )\r\n   1092     )\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pysr\\sr.py:2559, in idx_model_selection(equations, model_selection)\r\n   2557 elif model_selection == \"best\":\r\n   2558     threshold = 1.5 * equations[\"loss\"].min()\r\n-> 2559     filtered_equations = equations.query(f\"loss <= {threshold}\")\r\n   2560     chosen_idx = filtered_equations[\"score\"].idxmax()\r\n   2561 elif model_selection == \"score\":\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pandas\\util\\_decorators.py:331, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)\r\n    325 if len(args) > num_allow_args:\r\n    326     warnings.warn(\r\n    327         msg.format(arguments=_format_argument_list(allow_args)),\r\n    328         FutureWarning,\r\n    329         stacklevel=find_stack_level(),\r\n    330     )\r\n--> 331 return func(*args, **kwargs)\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pandas\\core\\frame.py:4474, in DataFrame.query(self, expr, inplace, **kwargs)\r\n   4472 kwargs[\"level\"] = kwargs.pop(\"level\", 0) + 2\r\n   4473 kwargs[\"target\"] = None\r\n-> 4474 res = self.eval(expr, **kwargs)\r\n   4476 try:\r\n   4477     result = self.loc[res]\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pandas\\util\\_decorators.py:331, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)\r\n    325 if len(args) > num_allow_args:\r\n    326     warnings.warn(\r\n    327         msg.format(arguments=_format_argument_list(allow_args)),\r\n    328         FutureWarning,\r\n    329         stacklevel=find_stack_level(),\r\n    330     )\r\n--> 331 return func(*args, **kwargs)\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pandas\\core\\frame.py:4612, in DataFrame.eval(self, expr, inplace, **kwargs)\r\n   4609     kwargs[\"target\"] = self\r\n   4610 kwargs[\"resolvers\"] = tuple(kwargs.get(\"resolvers\", ())) + resolvers\r\n-> 4612 return _eval(expr, inplace=inplace, **kwargs)\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pandas\\core\\computation\\eval.py:358, in eval(expr, parser, engine, truediv, local_dict, global_dict, resolvers, level, target, inplace)\r\n    356 eng = ENGINES[engine]\r\n    357 eng_inst = eng(parsed_expr)\r\n--> 358 ret = eng_inst.evaluate()\r\n    360 if parsed_expr.assigner is None:\r\n    361     if multi_line:\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pandas\\core\\computation\\engines.py:81, in AbstractEngine.evaluate(self)\r\n     78     self.result_type, self.aligned_axes = align_terms(self.expr.terms)\r\n     80 # make sure no names in resolvers and locals/globals clash\r\n---> 81 res = self._evaluate()\r\n     82 return reconstruct_object(\r\n     83     self.result_type, res, self.aligned_axes, self.expr.terms.return_type\r\n     84 )\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\pandas\\core\\computation\\engines.py:122, in NumExprEngine._evaluate(self)\r\n    120 scope = env.full_scope\r\n    121 _check_ne_builtin_clash(self.expr)\r\n--> 122 return ne.evaluate(s, local_dict=scope)\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\numexpr\\necompiler.py:975, in evaluate(ex, local_dict, global_dict, out, order, casting, sanitize, _frame_depth, **kwargs)\r\n    973     return re_evaluate(local_dict=local_dict, _frame_depth=_frame_depth)\r\n    974 else:\r\n--> 975     raise e\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\numexpr\\necompiler.py:872, in validate(ex, local_dict, global_dict, out, order, casting, _frame_depth, sanitize, **kwargs)\r\n    870 expr_key = (ex, tuple(sorted(context.items())))\r\n    871 if expr_key not in _names_cache:\r\n--> 872     _names_cache[expr_key] = getExprNames(ex, context, sanitize=sanitize)\r\n    873 names, ex_uses_vml = _names_cache[expr_key]\r\n    874 arguments = getArguments(names, local_dict, global_dict, _frame_depth=_frame_depth)\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\numexpr\\necompiler.py:721, in getExprNames(text, context, sanitize)\r\n    720 def getExprNames(text, context, sanitize: bool=True):\r\n--> 721     ex = stringToExpression(text, {}, context, sanitize)\r\n    722     ast = expressionToAST(ex)\r\n    723     input_order = getInputOrder(ast, None)\r\n\r\nFile D:\\ProgramData\\Anaconda3\\envs\\qlib\\lib\\site-packages\\numexpr\\necompiler.py:281, in stringToExpression(s, types, context, sanitize)\r\n    279     no_whitespace = re.sub(r'\\s+', '', s)\r\n    280     if _blacklist_re.search(no_whitespace) is not None:\r\n--> 281         raise ValueError(f'Expression {s} has forbidden control characters.')\r\n    283 old_ctx = expressions._context.get_current_context()\r\n    284 try:\r\n\r\nValueError: Expression (loss) <= (5.050614450000001e-13) has forbidden control characters.\r\n```",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Interesting. It seems like\r\n\r\n```python\r\nequations.query(f\"loss <= {threshold}\")\r\n```\r\n\r\nis causing the error. `equations` is a pandas dataframe and `threshold` is a float. So I am very surprised this doesn't work. \r\n\r\nCan you share the version information of everything, like your operating system, python version, pandas version, numpy version, etc.?\r\n\r\nAlso, could you try to report this as a bug to pandas? It seems like the parser isn't able to parse the string representations of floats on your machine, which seems like an issue.\r\n\r\nOn the PySR side we could look at a workaround which avoids using the `pandas.DataFrame.query` function.",
                "createdAt": "2024-07-14T14:47:03Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNy0xNFQxNTo0NzowMyswMTowMM4AmUKf"
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "discussion": {
          "number": 645,
          "title": "Modify the population.",
          "body": "Hello,\r\n\r\nThank you for your nice package, we are nicely using in real-world problems, in the academia for research.\r\n\r\nIn a previous version, we could change the population during the search, accessing by _raw_julia_state_ using a small function in julia (loaded in Python using jl.eval). Unfortunately, we have updated the version, and in the new version, even though we could change it, after we check and access to _raw_julia_state_ (or julia_state_) nothing is changed.\r\n\r\nLooking the source code, we can see that it is using jl_deserialize for each access, and so we think this is the reason it is not possible to change it.\r\n\r\nThere is a way in which we could access with writtting permissions to the populations?\r\n\r\nThank in advance.",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 661,
          "title": "[BUG]: EXCEPTION_ACCESS_VIOLATION during garbage collection in PySR",
          "body": "### What happened?\n\nThe program crashed while using PySR, with an error message indicating a memory access violation (EXCEPTION_ACCESS_VIOLATION). This error occurred during the garbage collection process.\n\n### Version\n\nv0.19.0\n\n### Operating System\n\nWindows\n\n### Package Manager\n\npip\n\n### Interface\n\nScript (i.e., `python my_script.py`)\n\n### Relevant log output\n\n```shell\n[ Info: Automatically setting `--heap-size-hint=2730M` on each Julia process. You can configure this with the `heap_size_hint_in_bytes` parameter.\r\n[ Info: Importing SymbolicRegression on workers as well as extensions Bumper, LoopVectorization.\r\n[ Info: Finished!\r\n[ Info: Copying definition of loss_fnc to workers...\r\n[ Info: Finished!\r\n[ Info: Started!\r\n32.1%┣█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                      ┫ 1.0k/3.2k [00:40<01:26, 25it/s]\r\nPlease submit a bug report with steps to reproduce this fault, and any error messages that follow (in their entirety). Thanks.\r\nException: EXCEPTION_ACCESS_VIOLATION at 0x7ffa6106a6b0 -- gc_mark_outrefs at C:/workdir/src\\gc.c:2527 [inlined]\r\ngc_mark_and_steal at C:/workdir/src\\gc.c:2746\r\nin expression starting at none:0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\ngc_mark_outrefs at C:/workdir/src\\gc.c:2527 [inlined]\r\ngc_mark_and_steal at C:/workdir/src\\gc.c:2746\r\ngc_mark_loop_parallel at C:/workdir/src\\gc.c:2885\r\njl_gc_mark_threadfun at C:/workdir/src\\partr.c:142\r\nuv__thread_start at /workspace/srcdir/libuv\\src/win\\thread.c:111\r\nbeginthreadex at C:\\Windows\\System32\\msvcrt.dll (unknown line)\r\nendthreadex at C:\\Windows\\System32\\msvcrt.dll (unknown line)\r\nBaseThreadInitThunk at C:\\Windows\\System32\\KERNEL32.DLL (unknown line)\r\nRtlUserThreadStart at C:\\Windows\\SYSTEM32\\ntdll.dll (unknown line)\r\nAllocations: 9815735891 (Pool: 9517376769; Big: 298359122); GC: 69400\n```\n\n\n### Extra Info\n\nturbo=True, bumper=True",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Can you try with `turbo=False, bumper=False`? Those options are experimental and get PySR to use libraries which are bleeding edge. When they work, they are really fast, but they can also cause crashes (especially on Windows).",
                "createdAt": "2024-07-05T09:42:43Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "Regrettably. I tried `turbo=False, bumper=False` parameter and the crash problem still occurred.",
                "createdAt": "2024-07-09T04:42:57Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "Could automatically setting `--heap-size-hint=2730M` cause this problem?",
                "createdAt": "2024-07-09T04:44:09Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hm, Can you show the rest of your code? ",
                "createdAt": "2024-07-09T06:34:58Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "```python\r\nfrom pysr import PySRRegressor\r\n\r\n# data load code\r\n\r\nX_123e = data_X_123e.to_numpy()\r\ny_123e = data_y_123e.to_numpy()\r\n\r\nsr_model = PySRRegressor(\r\n    binary_operators=[\r\n        \"*\",\r\n        \"+\",\r\n        \"-\",\r\n        \"/\",\r\n    ],\r\n    unary_operators=[\"square\", \"cube\", \"exp\", \"log\", \"sqrt\"],\r\n    maxsize=80, \r\n    maxdepth=10,  \r\n    niterations=100, \r\n    populations=32, \r\n    population_size=100, \r\n    ncycles_per_iteration=550, \r\n    constraints={\r\n        \"/\": (-1, 9),\r\n        \"^\": (-1, 5),\r\n        \"exp\": 6,\r\n        \"square\": 6,\r\n        \"cube\": 6,\r\n        \"log\": 6,\r\n        \"sqrt\": 6,\r\n        \"abs\": 9,\r\n    },\r\n    nested_constraints={\r\n        \"square\": {\"square\": 0, \"cube\": 0, \"exp\": 1},\r\n        \"cube\": {\"square\": 0, \"cube\": 0, \"exp\": 1},\r\n        \"exp\": {\"square\": 0, \"cube\": 0, \"exp\": 0},\r\n        \"sqrt\": {\"sqrt\": 0, \"log\": 0},\r\n        \"log\": {\"log\": 0},\r\n    },\r\n    complexity_of_operators={\r\n        \"square\": 2,\r\n        \"cube\": 3,\r\n        \"exp\": 3,\r\n        \"log\": 3,\r\n        \"sqrt\": 2,\r\n    },\r\n    complexity_of_constants=4,\r\n    adaptive_parsimony_scaling=150.0,\r\n    weight_add_node=0.79,\r\n    weight_insert_node=5.1,\r\n    weight_delete_node=1.7,\r\n    weight_do_nothing=0.21,\r\n    weight_mutate_constant=0.048,\r\n    weight_mutate_operator=0.47,\r\n    weight_swap_operands=0.1,\r\n    weight_randomize=0.23,\r\n    weight_simplify=0.5,\r\n    weight_optimize=0.5,\r\n    crossover_probability=0.066,\r\n    perturbation_factor=0.076,\r\n    cluster_manager=None,\r\n    precision=32,\r\n    turbo=True,\r\n    bumper=True,\r\n    progress=True,\r\n    elementwise_loss=\"\"\"\r\n    function loss_fnc(prediction, target)\r\n        percentage_error = abs((prediction - target) / target) * 100\r\n        return percentage_error\r\n    end\r\n    \"\"\",\r\n    multithreading=False,\r\n    equation_file=symbol_regression_csv_path,\r\n)\r\n\r\ncomplexity_of_variables = [] # list of complexity\r\nsr_model.fit(\r\n    X_123e, y_123e, complexity_of_variables=complexity_of_variables\r\n)\r\n```\r\nhere is the main code of the workflow.",
                "createdAt": "2024-07-09T09:01:51Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "At the same time, I will put the above code in a multi-layer loop to test different feature data sets and the stability of the symbolic regression results. A single loop takes about 2.2 minutes. The program crashes after running for 3-4 hours, running about 80-110 rounds.",
                "createdAt": "2024-07-09T09:04:39Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "That looks good. Great to see all those options being used! 🙂 \r\n\r\n(Random comment: your element wise loss divides by the target, so make sure the target > 0, otherwise one target will dominate. But I’m assuming you’re aware of that!)\r\n\r\nOther comment: can you try with `multithreading=True`? With it set to `False`, and with `procs>0` (the default), it will use multiple Julia processes. But if you just use multi-threading instead, it will start up much faster and hopefully be more stable. With multi-processing it is launching new Julia processes every single time it searches. (This is a weakness in the current codebase; I would like to eventually store the processes within PySRRegressor so multiprocessing has fast startup too.)\r\n\r\nYou can also set `multithreading=False, procs=0` to use serial mode.\r\n\r\nBut it’s curious that it crashes. Since it runs for a few hours, did you notice anything else happening, like the memory usage gradually increasing over that time and not going down?",
                "createdAt": "2024-07-09T11:15:43Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "If I use multithreading instead of multiprocessing, the calculation speed will drop from 30it/s to 7it/s on my device, which is a bit unacceptable to me. In addition, I have made sure that my y_true values ​​are all greater than 0. And the memory usage does not fluctuate when the program crashes, occupying only 30% of the total memory.",
                "createdAt": "2024-07-09T13:48:05Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Maybe try `multithreading=True` again, but this time, before loading PySR, set a larger thread count:\r\n```python\r\nimport os\r\nos.environ[\"PYTHON_JULIACALL_THREADS\"] = (num_cores) * 2\r\n```\r\nWhere `num_cores` is the number of CPU cores. The factor of 2 is so there’s some redundancy but you could try more or less depending on performance.\r\n\r\nThe default behavior of PySR is to start Julia with `--threads='auto'` which is actually fewer than the number of available cores (so it doesn’t take up the whole CPU). But for high performance you can increase the usage.\r\n\r\n\r\nThe full list of available juliacall environment variables is here: https://juliapy.github.io/PythonCall.jl/stable/juliacall/#julia-config ",
                "createdAt": "2024-07-09T21:59:28Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "I tried\r\n```python\r\nimport os\r\nos.environ[\"PYTHON_JULIACALL_THREADS\"] = \"64\"\r\n# or\r\nos.environ[\"PYTHON_JULIACALL_THREADS\"] = \"64\"\r\nos.environ[\"PYTHON_JULIACALL_PROCS\"] = \"64\"\r\n```\r\nBut it did not improve the calculation speed, the processor usage was only 20-30%, I am using a 24c32t 14900k processor.",
                "createdAt": "2024-07-10T02:07:32Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "To confirm, this was before importing PySR right? As a test, if you set it to 1, the CPU usage should only be 1 core.\r\n\r\nAlso note that the `PROCS` env variable won’t have any effect.",
                "createdAt": "2024-07-10T10:05:42Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "I had a similar problem when I gave up Windows and moved to Ubuntu 24.04 lts. I also used a tool (tm5) to test the memory. After testing for 1 hour, there was no error and the temperature was stable at 45℃. It doesn't seem to be a hardware problem. This problem is so strange.\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/home/zc/Documents/GitHub/MLPIP/notebooks/TC/S2_symbol_regression/S202_sr_123e.py\", line 192, in <module>\r\n    sr_model.fit(\r\n  File \"/home/zc/miniconda3/envs/MLPIP_ENV_PIP/lib/python3.11/site-packages/pysr/sr.py\", line 2088, in fit\r\n    self._run(X, y, runtime_params, weights=weights, seed=seed)\r\n  File \"/home/zc/miniconda3/envs/MLPIP_ENV_PIP/lib/python3.11/site-packages/pysr/sr.py\", line 1890, in _run\r\n    out = SymbolicRegression.equation_search(\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zc/.julia/packages/PythonCall/S5MOg/src/JlWrap/any.jl\", line 223, in __call__\r\n    return self._jl_callmethod($(pyjl_methodnum(pyjlany_call)), args, kwargs)\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nUnhandled Task ERROR: IOError: read: connection reset by peer (ECONNRESET)\r\nStacktrace:\r\n  [1] wait_readnb(x::Sockets.TCPSocket, nb::Int64)\r\n    @ Base ./stream.jl:410\r\n  [2] (::Base.var\"#wait_locked#739\")(s::Sockets.TCPSocket, buf::IOBuffer, nb::Int64)\r\n    @ Base ./stream.jl:949\r\n  [3] unsafe_read(s::Sockets.TCPSocket, p::Ptr{UInt8}, nb::UInt64)\r\n    @ Base ./stream.jl:955\r\n  [4] unsafe_read\r\n    @ ./io.jl:774 [inlined]\r\n  [5] unsafe_read(s::Sockets.TCPSocket, p::Base.RefValue{NTuple{4, Int64}}, n::Int64)\r\n    @ Base ./io.jl:773\r\n  [6] read!\r\n    @ ./io.jl:775 [inlined]\r\n  [7] deserialize_hdr_raw\r\n    @ ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/messages.jl:167 [inlined]\r\n  [8] message_handler_loop(r_stream::Sockets.TCPSocket, w_stream::Sockets.TCPSocket, incoming::Bool)\r\n    @ Distributed ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:172\r\n  [9] process_tcp_streams(r_stream::Sockets.TCPSocket, w_stream::Sockets.TCPSocket, incoming::Bool)\r\n    @ Distributed ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:133\r\n [10] (::Distributed.var\"#103#104\"{Sockets.TCPSocket, Sockets.TCPSocket, Bool})()\r\n    @ Distributed ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:121\r\njuliacall.JuliaError: TaskFailedException\r\nStacktrace:\r\n  [1] wait\r\n    @ ./task.jl:352 [inlined]\r\n  [2] fetch\r\n    @ ./task.jl:372 [inlined]\r\n  [3] _main_search_loop!(state::SymbolicRegression.SearchUtilsModule.SearchState{Float32, Float32, Node{Float32}, Distributed.Future, Distributed.RemoteChannel}, datasets::Vector{Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}, ropt::SymbolicRegression.SearchUtilsModule.RuntimeOptions{:multiprocessing, 1, true}, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Vector{Int64}}, DynamicExpressions.OperatorEnumModule.OperatorEnum, Node, true, true, nothing, StatsBase.Weights{Float64, Float64, Vector{Float64}}})\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:882\r\n  [4] _equation_search(datasets::Vector{Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}, ropt::SymbolicRegression.SearchUtilsModule.RuntimeOptions{:multiprocessing, 1, true}, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Vector{Int64}}, DynamicExpressions.OperatorEnumModule.OperatorEnum, Node, true, true, nothing, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, saved_state::Nothing)\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:599\r\n  [5] equation_search(datasets::Vector{Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}}; niterations::Int64, options::Options{SymbolicRegression.CoreModule.OptionsStructModule.ComplexityMapping{Int64, Vector{Int64}}, DynamicExpressions.OperatorEnumModule.OperatorEnum, Node, true, true, nothing, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, parallelism::String, numprocs::Int64, procs::Nothing, addprocs_function::Nothing, heap_size_hint_in_bytes::Nothing, runtests::Bool, saved_state::Nothing, return_state::Bool, verbosity::Int64, progress::Bool, v_dim_out::Val{1})\r\n    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:571\r\n  [6] equation_search\r\n    @ ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:449 [inlined]\r\n  [7] #equation_search#26\r\n    @ ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:412 [inlined]\r\n  [8] equation_search\r\n    @ ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:360 [inlined]\r\n  [9] #equation_search#28\r\n    @ ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:442 [inlined]\r\n [10] pyjlany_call(self::typeof(equation_search), args_::Py, kwargs_::Py)\r\n    @ PythonCall.JlWrap ~/.julia/packages/PythonCall/S5MOg/src/JlWrap/any.jl:36\r\n [11] _pyjl_callmethod(f::Any, self_::Ptr{PythonCall.C.PyObject}, args_::Ptr{PythonCall.C.PyObject}, nargs::Int64)\r\n    @ PythonCall.JlWrap ~/.julia/packages/PythonCall/S5MOg/src/JlWrap/base.jl:72\r\n [12] _pyjl_callmethod(o::Ptr{PythonCall.C.PyObject}, args::Ptr{PythonCall.C.PyObject})\r\n    @ PythonCall.JlWrap.Cjl ~/.julia/packages/PythonCall/S5MOg/src/JlWrap/C.jl:63\r\n\r\n    nested task error: Distributed.ProcessExitedException(423)\r\n    Stacktrace:\r\n      [1] try_yieldto(undo::typeof(Base.ensure_rescheduled))\r\n        @ Base ./task.jl:931\r\n      [2] wait()\r\n        @ Base ./task.jl:995\r\n      [3] wait(c::Base.GenericCondition{ReentrantLock}; first::Bool)\r\n        @ Base ./condition.jl:130\r\n      [4] wait\r\n        @ ./condition.jl:125 [inlined]\r\n      [5] take_buffered(c::Channel{Any})\r\n        @ Base ./channels.jl:477\r\n      [6] take!(c::Channel{Any})\r\n        @ Base ./channels.jl:471\r\n      [7] take!(::Distributed.RemoteValue)\r\n        @ Distributed ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:726\r\n      [8] remotecall_fetch(f::Function, w::Distributed.Worker, args::Distributed.RRID; kwargs::@Kwargs{})\r\n        @ Distributed ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:461\r\n      [9] remotecall_fetch(f::Function, w::Distributed.Worker, args::Distributed.RRID)\r\n        @ Distributed ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:454\r\n     [10] remotecall_fetch\r\n        @ ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:492 [inlined]\r\n     [11] call_on_owner\r\n        @ ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:565 [inlined]\r\n     [12] fetch(r::Distributed.Future)\r\n        @ Distributed ~/miniconda3/envs/MLPIP_ENV_PIP/julia_env/pyjuliapkg/install/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:619\r\n     [13] (::SymbolicRegression.var\"#67#72\"{SymbolicRegression.SearchUtilsModule.SearchState{Float32, Float32, Node{Float32}, Distributed.Future, Distributed.RemoteChannel}, Int64, Int64})()\r\n        @ SymbolicRegression ~/.julia/packages/SymbolicRegression/9q4ZC/src/SymbolicRegression.jl:984\r\n```",
                "createdAt": "2024-07-15T18:26:16Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Just to confirm, there is no crash now? Just that this message is printed?\r\n\r\nI see this message sometimes during testing. So far, it has seemed to be harmless, and has never caused a crash – it simply indicates that one of the worker processes has exited, due to the search returning, and the `@async fetch` call on that worker failed. \r\n\r\nHowever, if this is what is calling the error, perhaps it is not harmless, and we should close the asynchronous `fetch` tasks before the worker processes are killed.\r\n",
                "createdAt": "2024-07-15T18:55:10Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "I do think it would be better if there was a way to get multithreading to be faster, by increasing `PYTHON_JULIACALL_THREADS` before importing pysr. Windows multiprocessing seems to occasionally have issues for unknown reasons, and has been quite hard to debug, whereas multithreading has been quite stable.",
                "createdAt": "2024-07-15T18:59:00Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "This message appears when the search process reaches about 30%, and then the search process stops. I can try to reproduce it again to see if it crashes. Also, does using the slurm backend help avoid this problem?",
                "createdAt": "2024-07-15T19:01:05Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Thanks. So if this reproduces on ubuntu, it seems like a deeper issue. Can you share your data so that I can reproduce it on my machine? If there is some script I can run which reproduces the error exactly on my computer it will be easier to help debug it.\r\n\r\nAlso, the more minimal the code, the easier it will be for me to debug it. So perhaps try (1) reducing the dataset size, (2) creating conditions that cause the error to occur **earlier** during training, (3) using fewer parameters of PySR.\r\n\r\nI guess this might be hard to make a smaller MWE but (2) would be most useful.\r\n\r\n\r\n---\r\n\r\nThe Slurm backend is only if you’re using a Slurm computing cluster, but won’t be available otherwise.",
                "createdAt": "2024-07-15T19:11:31Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "> To confirm, this was before importing PySR right? As a test, if you set it to 1, the CPU usage should only be 1 core.\r\n> \r\n> Also note that the `PROCS` env variable won’t have any effect.\r\n\r\nI have confirmed this point. If I use os.environ[\"PYTHON_JULIACALL_THREADS\"] = \"1\", it will warn Warning: You are using multithreading mode, but only one thread is available. Try starting julia with `--threads=auto`.",
                "createdAt": "2024-07-15T19:14:12Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "> Thanks. So if this reproduces on ubuntu, it seems like a deeper issue. Can you share your data so that I can reproduce it on my machine? If there is some script I can run which reproduces the error exactly on my computer it will be easier to help debug it.\r\n> \r\n> Also, the more minimal the code, the easier it will be for me to debug it. So perhaps try (1) reducing the dataset size, (2) creating conditions that cause the error to occur **earlier** during training, (3) using fewer parameters of PySR.\r\n> \r\n> I guess this might be hard to make a smaller MWE but (2) would be most useful.\r\n> \r\n> The Slurm backend is only if you’re using a Slurm computing cluster, but won’t be available otherwise.\r\n\r\nThank you very much. I need to apply for the relevant code and data to be provided. In addition, I have an Ubuntu 20 server running a single-node slurm. In the preliminary test, the calculation speed is consistent with multi-process. I can test on that device to confirm whether it is a device problem.",
                "createdAt": "2024-07-15T19:19:42Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "> Just to confirm, there is no crash now? Just that this message is printed?\r\n> \r\n> I see this message sometimes during testing. So far, it has seemed to be harmless, and has never caused a crash – it simply indicates that one of the worker processes has exited, due to the search returning, and the `@async fetch` call on that worker failed.\r\n> \r\n> However, if this is what is calling the error, perhaps it is not harmless, and we should close the asynchronous `fetch` tasks before the worker processes are killed.\r\n\r\nI have confirmed that this prompt will cause the search process to be interrupted. I temporarily bypassed the crash by using ```try...except Exception...``` in the Python code, but the memory requested by Julia was not released. This caused my memory to be full after crashing 3 times. Can we use the ```try-finally ``` block in the Julia source code to improve the stability of the program?\r\n\r\n[error_log.txt](https://github.com/user-attachments/files/16248400/error_log.txt)\r\n",
                "createdAt": "2024-07-16T12:06:42Z"
              },
              {
                "author": {
                  "login": "zzccchen"
                },
                "body": "I think I have found a temporary solution for the time being, which is to manually end the julia process after each search.\r\n```python\r\nimport time, os\r\ntime.sleep(10)\r\nos.system(\"killall julia\")\r\n```",
                "createdAt": "2024-07-18T15:39:27Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Thanks. That is good to know. \r\n\r\nI do think the way SymbolicRegression.jl launches processes is a bit problematic for large-scale use-cases at the moment. The way it works is that it calls [`addprocs` from within `SymbolicRegression.equation_search`](https://github.com/MilesCranmer/SymbolicRegression.jl/blob/cd23a6e25c64d00565c3ae3905d06dc3c63033ed/src/Configure.jl#L322). This was designed for convenience of users, especially on the Python side, but as far as I can tell it's not well-supported behavior in Julia, which means it needs to do some very fragile things like [manually copying function definitions to workers](https://github.com/MilesCranmer/SymbolicRegression.jl/blob/cd23a6e25c64d00565c3ae3905d06dc3c63033ed/src/Configure.jl#L115). \r\n\r\nWhat would be better is if PySR did one of the following alternative strategies:\r\n\r\n1. For big jobs, use MPI directly, via [MPI.jl](https://github.com/JuliaParallel/MPI.jl). However, this would require the user to call `mpiexec` manually, rather than launch the multi-processor search from a single Python session. However, it is nice that MPI has support as a standard on every cluster, so we wouldn't need to rely on different cluster manager-specific scripts.\r\n2. Explore @oschulz's [ParallelProcessingTools.jl](https://github.com/oschulz/ParallelProcessingTools.jl) as an alternative. This uses an elastic manager – which is actually designed for the things PySR is doing, like adding and removing workers. (Right now PySR basically misuses Distributed.jl to start new processes, send code to them, and finally kill them at the end of a search. It works and it's convenient, but I'm not sure it is a sustainable solution)\r\n3. Start the workers from the Python side, rather than within Julia. Basically, the `PySRRegressor` object itself would call `addprocs`, and store the processes as an attribute of the regressor object. It can pass these to `equation_search` via the `procs` keyword argument, in which case SymbolicRegression.jl will simply use them.\r\n    - However, this would require rewriting some of the Python side of things so that each `jl.seval` is called with an [`@everywhere`](https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.@everywhere) in front of it – thus executing each Julia snippet on all processes. This also means that it would be harder for users to use `jl.seval` themselves.\r\n    - This approach would also mean that we could wrap PySR in a Julia module, rather than the current approach of running everything in Julia's `Main` context – which might interfere with other Python+Julia packages in the future.\r\n  \r\nI'm not sure how much work each of these options would be. They might be fairly easy to get working though. But it would definitely require some Julia coding (if you are up for it).",
                "createdAt": "2024-07-18T18:25:24Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Just going to keep this open until there's a better solution than a manual workaround. Ideally the workaround shouldn't be needed",
                "createdAt": "2024-07-19T14:04:38Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOhXhggQ=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 636,
          "title": "[BUG]: libpcre2-8 bug on Windows",
          "body": "### What happened?\r\n\r\nI have been using the Julia1.9 version, and pysr can run normally\r\nHowever, when I add the dimension calculation, the following error occurs：\r\n\r\n```python\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[32], line 28\r\n     21 X = pd.DataFrame(dict(\r\n     22     M=M.to(\"M_sun\").value,\r\n     23     m=m.to(\"kg\").value,\r\n     24     r=r.to(\"R_earth\").value,\r\n     25 ))\r\n     26 y = F.value\r\n---> 28 model.fit(\r\n     29     X,\r\n     30     y,\r\n     31     X_units=[\"Constants.M_sun\", \"kg\", \"Constants.R_earth\"],\r\n     32     y_units=\"kg * m / s^2\"\r\n     33 )\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pysr\\sr.py:1970, in PySRRegressor.fit(self, X, y, Xresampled, weights, variable_names, X_units, y_units)\r\n   1967     self._checkpoint()\r\n   1969 # Perform the search:\r\n-> 1970 self._run(X, y, mutated_params, weights=weights, seed=seed)\r\n   1972 # Then, after fit, we save again, so the pickle file contains\r\n   1973 # the equations:\r\n   1974 if not self.temp_equation_file:\r\n\r\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pysr\\sr.py:1800, in PySRRegressor._run(self, X, y, mutated_params, weights, seed)\r\n   1796     y_variable_names = [f\"y{_subscriptify(i)}\" for i in range(y.shape[1])]\r\n   1798 # Call to Julia backend.\r\n   1799 # See https://github.com/MilesCranmer/SymbolicRegression.jl/blob/master/src/SymbolicRegression.jl\r\n-> 1800 self.raw_julia_state_ = SymbolicRegression.equation_search(\r\n   1801     Main.X,\r\n   1802     Main.y,\r\n   1803     weights=Main.weights,\r\n   1804     niterations=int(self.niterations),\r\n   1805     variable_names=self.feature_names_in_.tolist(),\r\n   1806     display_variable_names=self.display_feature_names_in_.tolist(),\r\n   1807     y_variable_names=y_variable_names,\r\n   1808     X_units=self.X_units_,\r\n   1809     y_units=self.y_units_,\r\n   1810     options=options,\r\n   1811     numprocs=cprocs,\r\n   1812     parallelism=parallelism,\r\n   1813     saved_state=self.raw_julia_state_,\r\n   1814     return_state=True,\r\n   1815     addprocs_function=cluster_manager,\r\n   1816     progress=progress and self.verbosity > 0 and len(y.shape) == 1,\r\n   1817     verbosity=int(self.verbosity),\r\n   1818 )\r\n   1820 # Set attributes\r\n   1821 self.equations_ = self.get_hof()\r\n\r\nRuntimeError: <PyCall.jlwrap (in a Julia function called from Python)\r\nJULIA: MethodError: no method matching _method_instances(::Type{typeof(*)}, ::Type{Tuple{SymbolicRegression.DimensionalAnalysisModule.WildcardQuantity{DynamicQuantities.Quantity{Float32, DynamicQuantities.Dimensions{DynamicQuantities.FixedRational{Int32, 25200}}}}, SymbolicRegression.DimensionalAnalysisModule.WildcardQuantity{DynamicQuantities.Quantity{Float32, DynamicQuantities.Dimensions{DynamicQuantities.FixedRational{Int32, 25200}}}}}})\r\nThe applicable method may be too new: running in world age 45094, while current world is 54814.\r\n```\r\n\r\nAs #420  says, I replaced the julia version with 1.10.0 and the latest 1.10.3. However, the following errors occurred :\r\n\r\n```julia\r\nfatal: error thrown and no exception handler available.\r\nInitError(mod=:Sys, error=ErrorException(\"could not load library \"libpcre2-8\"\r\nThe specified module could not be found. \"))\r\nijl_errorf at C:/workdir/src\\rtutils.c:77\r\n```\r\n\r\njust like #566 . I want to know which version of julia I should adjust when I want to introduce the dimension into pysr for calculation ? Thank you.\r\n\r\n### Version\r\n\r\n0.16.3\r\n\r\n### Operating System\r\n\r\nWindows\r\n\r\n### Package Manager\r\n\r\nNone\r\n\r\n### Interface\r\n\r\nJupyter Notebook\r\n\r\n### Relevant log output\r\n\r\n_No response_\r\n\r\n### Extra Info\r\n\r\n_No response_",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Ah, this issue. This issue has been driving me crazy for a while. It only happens on Windows, and it's only for a few users. \r\n\r\nIt is a reported bug on Julia but with no solution: https://github.com/JuliaLang/julia/issues/52205. Only a few users have reported it, and I really haven't been able to figure out what is special about those windows installations which causes this bug.\r\n\r\n@tbuckworth were you ever able to solve this?\r\n\r\n@zhuyi-bjut my advice is to try installing Julia a different way. My recommendation is to use JuliaUp: https://github.com/JuliaLang/juliaup. \r\n\r\nYou should make sure that juliacall is actually using the Julia version installed by JuliaUp. To do that you can use\r\n\r\n```python\r\nimport os\r\nos.environ[\"PYTHON_JULIACALL_BINDIR\"] = \"/path/to/your/julia/bin/folder\"\r\n```\r\n\r\nmake sure to do this before you import PySR.\r\n\r\nOne thing I think might be happening is juliacall is installing a version without this particular dll for some reason. But if you can install the version yourself using juliaup I think it should carry it.\r\n\r\n@cjdoris do you have any idea what this is? Sometimes PySR users on Windows will run into it. But only sometimes. I haven't been able to reproduce this once so I figure it must be something missing in the system libraries they have installed. Maybe juliacall is installing a julia binary without some dll?\r\n",
                "createdAt": "2024-06-02T23:08:11Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Is this related at all I wonder: https://github.com/JuliaLang/julia/issues/52007",
                "createdAt": "2024-06-02T23:12:54Z"
              },
              {
                "author": {
                  "login": "cjdoris"
                },
                "body": "The OP's error message is from PyCall not PythonCall - does that mean they are on an old version of PySR?",
                "createdAt": "2024-06-03T12:36:27Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Oh I didn't even notice that, good catch!! Yeah @zhuyi-bjut the latest version of PySR is 0.18.4 but you are on 0.16.3. Can you please try upgrading?",
                "createdAt": "2024-06-03T13:43:51Z"
              },
              {
                "author": {
                  "login": "zhuyi-bjut"
                },
                "body": "I am very happy to tell you that I have solved this problem by updating the version of pysr ! Obviously I made a stupid mistake before. Anyway, thank you for your help @MilesCranmer @cjdoris \r\n",
                "createdAt": "2024-06-03T14:12:47Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Awesome :) ",
                "createdAt": "2024-06-03T16:40:30Z"
              },
              {
                "author": {
                  "login": "tbuckworth"
                },
                "body": "> @tbuckworth were you ever able to solve this?\r\n\r\nNo, I ended up installing Ubuntu instead",
                "createdAt": "2024-06-03T17:29:33Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "@tbuckworth if you try installing the latest PySR it might just work? (On Windows too)",
                "createdAt": "2024-06-03T17:52:10Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Seems like @Angelld23 found a workaround on https://github.com/JuliaLang/julia/issues/52007#issuecomment-2173945505. Copying here for visibility:\r\n\r\n```python\r\nimport os import ctypes import glob\r\n\r\n# Path to the bin directory of your Julia installation\r\njulia_bin_path = \"C:\\path\\to\\your\\dll_directory\" # (which is the same as the julia.exe)\r\n\r\n# Add the bin directory to PATH\r\nos.environ[\"PATH\"] += \";\" + julia_bin_path\r\n\r\n# Load each DLL file in the bin directory\r\nfor dll_path in glob.glob(os.path.join(julia_bin_path, \"*.dll\")):\r\n    try:\r\n        ctypes.CDLL(dll_path)\r\n        print(f\"Loaded {dll_path} successfully.\")\r\n    except OSError as e:\r\n        print(f\"Could not load {dll_path}: {e}\")\r\n\r\nfrom pysr import PySRRegressor\r\n```\r\n",
                "createdAt": "2024-06-17T18:08:38Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOgZTsxw=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 682,
          "title": "[BUG]: Hard crash on import from MacOS System Integrity Protection (SIP)",
          "body": "### What happened?\n\nupon pip installing pysr into a virtual environment, making sure my PATH variable has the bin, exporting LD_LIBRARY_PATH as specified in github readme, and even removing quarantine status for the environment, importing pysr still results in python quitting\r\n\r\njulia version supports arch64 (silicon)\n\n### Version\n\nAny version of PySR\n\n### Operating System\n\nmacOS\n\n### Package Manager\n\npip\n\n### Interface\n\nJupyter Notebook\n\n### Relevant log output\n\n```shell\n-------------------------------------\r\nTranslated Report (Full Report Below)\r\n-------------------------------------\r\n\r\nProcess:               Python [40891]\r\nPath:                  /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/Resources/Python.app/Contents/MacOS/Python\r\nIdentifier:            com.apple.python3\r\nVersion:               3.9.6 (3.9.6)\r\nBuild Info:            python3-141000000000000~1415\r\nCode Type:             ARM-64 (Native)\r\nParent Process:        python [40765]\r\nResponsible:           pycharm [40727]\r\nUser ID:               501\r\n\r\nDate/Time:             2024-07-27 21:46:46.1280 -0700\r\nOS Version:            macOS 14.5 (23F79)\r\nReport Version:        12\r\nAnonymous UUID:        6F31D97B-2A3B-8D95-FA9E-B1FE5CB86DF1\r\n\r\nSleep/Wake UUID:       404515B4-A7B3-4531-A2F4-F7C17B16EC40\r\n\r\nTime Awake Since Boot: 240000 seconds\r\nTime Since Wake:       27250 seconds\r\n\r\nSystem Integrity Protection: enabled\r\n\r\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\r\n\r\nException Type:        EXC_GUARD (SIGKILL)\r\nException Codes:       GUARD_TYPE_MACH_PORT\r\nException Codes:       0x0000000000012740, 0x0000000000000000\r\n\r\nTermination Reason:    Namespace GUARD, Code 2305843035917854528\n```\n\n\n### Extra Info\n\ntried all sorts of PySR and Julia versions, this seems to be independent of that, id prefer a solution that doesnt involve me booting in RecoveryOS and disabling SIP, although this is what I have done in the meantime",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "ev-watson"
                },
                "body": "just to be clear, booting into RecoveryOS and disabling the security protocol DOES solve this issue (I was able to fully regress an equation), so surely there is a fix that can be made, maybe permissions can be set somewhere or something",
                "createdAt": "2024-07-28T08:01:41Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "1. Can you launch Julia by itself?\r\n2. Then, can you import `juliacall` by itself, or does that trigger the error too?\r\n3. Do you have any corporate security tools that might be causing this? Note that PySR attempts to install Julia the first time you import it. \r\n\r\nIf (1) is okay, but (2) is not, perhaps you could try forcing the version of Julia it is loading? You can do this with the environment variables here: https://juliapy.github.io/PythonCall.jl/stable/juliacall/#julia-config",
                "createdAt": "2024-07-28T13:36:15Z"
              },
              {
                "author": {
                  "login": "ev-watson"
                },
                "body": "I can launch julia just fine, i can also import juliacall by itself, once I did this, i then imported pysr and everything worked just fine\r\nIf i restart my jupyter server and import pysr alone it crashes, but if i import juliacall before pysr everything works and i can regress an equation. For clarity this works:\r\n```\r\nimport juliacall\r\nimport pysr\r\n```\r\n-----------------------\r\nBut this causes the security protocol to be triggered:\r\n```\r\nimport pysr\r\n```\r\ni do not have any corporate security tools, this was just Apple's own disk System Integrity Protection force killing python. So obviously it is because pysr cant install julia, or at least right now it just does so in way that triggers SIP,\r\n\r\nThanks for the help,",
                "createdAt": "2024-07-29T03:06:50Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "> but if i import juliacall before pysr everything works and i can regress an equation\r\n\r\nInteresting. So the only things that this could change are the environment variables set within `pysr/julia_import.py`. I wonder if you set those manually, if any environment variable in particular triggers the error? Perhaps the multithreaded setting is causing it?\r\n\r\ne.g., https://github.com/MilesCranmer/PySR/blob/3aee19e38ceb3e0e1617d357a831400e01204658/pysr/julia_import.py#L32-L37\r\n\r\nBasically if you import juliacall beforehand, these environment variables in this file will not be used. So maybe one of the variables is causing the issue.",
                "createdAt": "2024-07-29T03:12:52Z"
              },
              {
                "author": {
                  "login": "ev-watson"
                },
                "body": "Setting manually does not trigger an error, I ran the code provided above without importing pysr and it ran, but then upon importing PySR, even with the environment variables set, it still triggered security protocol",
                "createdAt": "2024-07-29T04:02:58Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Can you set the environment variables, and then import juliacall? (i.e., no PySR)",
                "createdAt": "2024-07-29T05:43:32Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Another thing to try is using the `trace` module to see exactly where it gets killed:\r\n```\r\npython -m trace -t myscript.py \r\n```\r\nIt should print every line that gets executed. So the last line printed is the one that triggered the system integrity protection",
                "createdAt": "2024-07-29T05:46:58Z"
              },
              {
                "author": {
                  "login": "ev-watson"
                },
                "body": "Setting environment variables and then importing juliacall did cause a crash, in particular the HANDLE_SIGNALS variable:\r\n\r\nSo this works (commented out first line):\r\n```\r\nimport os\r\n\r\nfor k, default in (\r\n        #(\"PYTHON_JULIACALL_HANDLE_SIGNALS\", \"yes\"),\r\n        (\"PYTHON_JULIACALL_THREADS\", \"auto\"),\r\n        (\"PYTHON_JULIACALL_OPTLEVEL\", \"3\"),\r\n):\r\n    os.environ[k] = os.environ.get(k, default)\r\n\r\nimport juliacall\r\n```\r\n------------------------------------\r\nWhereas this triggers security protocol (uncommented first line):\r\n```\r\nimport os\r\n\r\nfor k, default in (\r\n        (\"PYTHON_JULIACALL_HANDLE_SIGNALS\", \"yes\"),\r\n        (\"PYTHON_JULIACALL_THREADS\", \"auto\"),\r\n        (\"PYTHON_JULIACALL_OPTLEVEL\", \"3\"),\r\n):\r\n    os.environ[k] = os.environ.get(k, default)\r\n\r\nimport juliacall\r\n```\r\n\r\nHere is the traceback of the crash:\r\n```\r\n --- modulename: __init__, funcname: __init__\r\n__init__.py(336):         self._name = name\r\n__init__.py(337):         flags = self._func_flags_\r\n__init__.py(338):         if use_errno:\r\n__init__.py(340):         if use_last_error:\r\n__init__.py(342):         if _sys.platform.startswith(\"aix\"):\r\n__init__.py(350):         if _os.name == \"nt\":\r\n__init__.py(360):         class _FuncPtr(_CFuncPtr):\r\n --- modulename: __init__, funcname: _FuncPtr\r\n__init__.py(360):         class _FuncPtr(_CFuncPtr):\r\n__init__.py(361):             _flags_ = flags\r\n__init__.py(362):             _restype_ = self._func_restype_\r\n__init__.py(363):         self._FuncPtr = _FuncPtr\r\n__init__.py(365):         if handle is None:\r\n__init__.py(366):             self._handle = _dlopen(self._name, mode)\r\n__init__.py(183):     argc, argv = args_from_config()\r\n --- modulename: __init__, funcname: args_from_config\r\n__init__.py(112):         argv = [CONFIG['exepath']]\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(120):                 argv.append('--' + opt[4:].replace('_', '-') + '=' + val)\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(120):                 argv.append('--' + opt[4:].replace('_', '-') + '=' + val)\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(115):                 if val is None:\r\n__init__.py(116):                     if opt == 'opt_handle_signals':\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(114):             if opt.startswith('opt_'):\r\n__init__.py(113):         for opt, val in CONFIG.items():\r\n__init__.py(121):         argv = [s.encode(\"utf-8\") for s in argv]\r\n --- modulename: __init__, funcname: <listcomp>\r\n__init__.py(121):         argv = [s.encode(\"utf-8\") for s in argv]\r\n__init__.py(121):         argv = [s.encode(\"utf-8\") for s in argv]\r\n__init__.py(121):         argv = [s.encode(\"utf-8\") for s in argv]\r\n__init__.py(121):         argv = [s.encode(\"utf-8\") for s in argv]\r\n__init__.py(123):         argc = len(argv)\r\n__init__.py(124):         argc = c.c_int(argc)\r\n__init__.py(125):         argv = c.POINTER(c.c_char_p)((c.c_char_p * len(argv))(*argv))\r\n__init__.py(126):         return argc, argv\r\n__init__.py(184):     jl_parse_opts = lib.jl_parse_opts\r\n --- modulename: __init__, funcname: __getattr__\r\n__init__.py(377):         if name.startswith('__') and name.endswith('__'):\r\n__init__.py(379):         func = self.__getitem__(name)\r\n --- modulename: __init__, funcname: __getitem__\r\n__init__.py(384):         func = self._FuncPtr((name_or_ordinal, self))\r\n__init__.py(385):         if not isinstance(name_or_ordinal, int):\r\n__init__.py(386):             func.__name__ = name_or_ordinal\r\n__init__.py(387):         return func\r\n__init__.py(380):         setattr(self, name, func)\r\n__init__.py(381):         return func\r\n__init__.py(185):     jl_parse_opts.argtypes = [c.c_void_p, c.c_void_p]\r\n__init__.py(186):     jl_parse_opts.restype = None\r\n__init__.py(187):     jl_parse_opts(c.pointer(argc), c.pointer(argv))\r\n__init__.py(188):     assert argc.value == 0\r\n__init__.py(191):     try:\r\n__init__.py(192):         jl_init = lib.jl_init_with_image__threading\r\n --- modulename: __init__, funcname: __getattr__\r\n__init__.py(377):         if name.startswith('__') and name.endswith('__'):\r\n__init__.py(379):         func = self.__getitem__(name)\r\n --- modulename: __init__, funcname: __getitem__\r\n__init__.py(384):         func = self._FuncPtr((name_or_ordinal, self))\r\n__init__.py(385):         if not isinstance(name_or_ordinal, int):\r\n__init__.py(386):             func.__name__ = name_or_ordinal\r\n__init__.py(387):         return func\r\n__init__.py(380):         setattr(self, name, func)\r\n__init__.py(381):         return func\r\n__init__.py(195):     jl_init.argtypes = [c.c_char_p, c.c_char_p]\r\n__init__.py(196):     jl_init.restype = None\r\n__init__.py(197):     jl_init(\r\n__init__.py(198):         (default_bindir if bindir is None else bindir).encode('utf8'),\r\n__init__.py(199):         None if sysimg is None else sysimg.encode('utf8'),\r\n__init__.py(197):     jl_init(\r\nzsh: killed     python -m trace -t testing.py\r\n```",
                "createdAt": "2024-07-29T06:13:21Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Thanks, that is super useful! So it sounds like that one environment variable is the issue here. (You can ignore the other two, they seem to not affect it).\r\n\r\nWould it be possible for you to make an issue on PythonCall.jl (same thing as juliacall)? Here: https://github.com/JuliaPy/PythonCall.jl/issues\r\n\r\nIt sounds like an issue is larger than just PySR so we can solve it over there instead. I would just copy the minimal example you mentioned to an issue there (without the other two env variables), and explain how it is triggering system integrity protection issues.",
                "createdAt": "2024-07-29T06:29:15Z"
              },
              {
                "author": {
                  "login": "ev-watson"
                },
                "body": "Done, thanks for all the help",
                "createdAt": "2024-07-29T07:29:34Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOhmssaQ=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 640,
          "title": "[Feature]:  Multi-Class Classification",
          "body": "### Feature Request\n\nHow can we use this library to get an equation for multiclass classification ?",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "xylhal"
                },
                "body": "I'm also looking into this. Is it possible to define cross entropy as a custom loss function?",
                "createdAt": "2024-06-06T20:13:44Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Right now the labels have to be scalars, so only binary classification and regression is possible. But I have an intern working with me this summer on adding vector capabilities to PySR so in principle it should be eventually doable.\r\n\r\nIn principle there's nothing standing in the way of this as the backend allows vectors/tensors/whatever other type you want: https://github.com/SymbolicML/DynamicExpressions.jl?tab=readme-ov-file#tensors. Just need to get it all integrated.",
                "createdAt": "2024-06-06T20:47:35Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Actually I guess you could get it working as a custom loss function: https://astroautomata.com/PySR/examples/#9-custom-objectives\r\n\r\nMaybe you could split a single tree into multiple expressions. And each of those expressions could act as a single logit – then compute the multi-class classification on top!",
                "createdAt": "2024-06-06T20:48:38Z"
              },
              {
                "author": {
                  "login": "xylhal"
                },
                "body": "> Right now the labels have to be scalars, so only binary classification and regression is possible. But I have an intern working with me this summer on adding vector capabilities to PySR so in principle it should be eventually doable.\r\n> \r\n> In principle there's nothing standing in the way of this as the backend allows vectors/tensors/whatever other type you want: https://github.com/SymbolicML/DynamicExpressions.jl?tab=readme-ov-file#tensors. Just need to get it all integrated.\r\n\r\nInteresting, what do you mean by vector capabilities and how will you envision the outputs to be different? What I had in mind was the output equation mimicking a classification process, where the labels are scalar integers. That probably counter intuitive to the definition of regression, but I'm wondering if generating an equation that ultimately computes an integer is possible? Maybe naively a bunch of indicator functions of each variable? If so, would just using a custom loss function help?\r\n\r\n> Actually I guess you could get it working as a custom loss function: https://astroautomata.com/PySR/examples/#9-custom-objectives\r\n> \r\n> Maybe you could split a single tree into multiple expressions. And each of those expressions could act as a single logit – then compute the multi-class classification on top!\r\n\r\nStill pretty new to this, will take a deeper dive into it.",
                "createdAt": "2024-06-06T22:19:11Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOgFu21w=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 656,
          "title": "[BUG]: torch export fails for expressions with constant inputs e.g. exp(2)",
          "body": "### What happened?\n\nsympy2torch produces a module that fails when called if a function of a constant is present in the expression.\r\n\r\nFor example:\r\n```\r\nfrom sympy import symbols, exp\r\nfrom pysr import sympy2torch\r\nimport torch\r\n\r\nx, y = symbols(\"x y\")\r\n\r\nexpression = exp(2)\r\n\r\nmodule = sympy2torch(expression, [x, y])\r\n\r\nX = torch.rand(100, 2).float() * 10\r\n\r\ntorch_out = module(X)\r\n```\r\nproduces this error\r\n> TypeError: exp(): argument 'input' (position 1) must be Tensor, not float\r\n\r\nI've tried other expressions like log(4), which produces the same problem.\r\n\r\nThe current mapping in `export_torch.py` is `sympy.exp: torch.exp`.\r\n\r\nI believe that \r\n\r\n```\r\ndef exp(x):\r\n    return torch.exp(torch.FloatTensor(x))\r\n```\r\nthen using the mapping `sympy.exp: exp` might work, but I have been unable to test it (adding to extra_sympy_mappings doesn't work, I think because it is chained to the end of the existing mappings and doesn't override the original one).\r\n\r\nAlternatively, perhaps simplifying all expressions to constants where possible might solve the problem for all expressions e.g. `exp(2)` becomes `7.38905609893`.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Version\n\n0.18.4\n\n### Operating System\n\nLinux\n\n### Package Manager\n\npip\n\n### Interface\n\nScript (i.e., `python my_script.py`)\n\n### Relevant log output\n\n_No response_\n\n### Extra Info\n\n_No response_",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Thanks for the bug report! One thing I am surprised about is that I thought this would already happen? See this code:\r\nhttps://github.com/MilesCranmer/PySR/blob/06ca0e376e63d563aa063028a5f9bc7fa7d849c5/pysr/export_torch.py#L94-L97\r\n\r\nThe code `torch.tensor(float(expr))` should already map any constant into a torch tensor. \r\n\r\nMaybe the issue is that you are explicitly passing a Python integer, rather than a SymPy integer? \r\n\r\nFor example, we can see these are actually different classes:\r\n\r\n```python\r\nIn [4]: isinstance(1, sympy.Integer)\r\nOut[4]: False\r\n```\r\n\r\nDid you see this error from a PySR export, or are you trying to use `sympy2torch` manually and putting in the integers explicitly?",
                "createdAt": "2024-06-20T13:22:44Z"
              },
              {
                "author": {
                  "login": "tbuckworth"
                },
                "body": "This came about using `PySRRegressor.fit()`, which produced an expression containing `square(exp(sign(0.44796443)))`, which seems to simplify to `exp(2)`.\r\n\r\nI recreated the issue using `expression = exp(sign(0.44796443))*exp(sign(0.44796443))` originally, but wrote `exp(2)` here as a minimal example.\r\n\r\n",
                "createdAt": "2024-06-20T16:07:53Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Do you know the original error message? It could be the MWE is actually a different thing. The `exp(2)` should never actually occur, it should (I think) be `exp(sympy.Integer(2))`. At least it should be.\r\n\r\nperhaps because it was `sign(..)` it is some kind of floating point number PySR don’t account for",
                "createdAt": "2024-06-20T16:48:45Z"
              },
              {
                "author": {
                  "login": "tbuckworth"
                },
                "body": "Oh, apologies if this is my fault, but I was using `extra_torch_mappings` that included:\r\n```\r\nsympy.core.numbers.Exp1: exp1\r\n```\r\nwhere\r\n```\r\ndef exp1():\r\n    return torch.exp(torch.FloatTensor([1]))\r\n```\r\nI believe I added this due to an error arising when trying to export to torch an expression containing `exp(sign(0.1...))`, but I don't remember exactly.\r\n\r\nIn terms of the original error in this issue, the `PySRRegressor.fit` function learned this expression:\r\n>(square(x2 / 0.10893087) * exp(x3)) - square(exp(sign(0.44796443)))\r\n\r\nI then called `model.pytorch()`, which resulted in this error:\r\n\r\n```python\r\n> 22 Traceback (most recent call last):\r\n> 23   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/pysr/export_torch.py\", line 151, in forward\r\n> 24     arg_ = memodict[arg]\r\n> 25 KeyError: _Node(\r\n> 26   (_args): ModuleList(\r\n\r\n> 27     (0): _Node()\r\n> 28     (1): _Node(\r\n> 29       (_args): ModuleList(\r\n> 30         (0): _Node()\r\n> 31       )\r\n> 32     )\r\n> 33   )\r\n> 34 )\r\n> 35 During handling of the above exception, another exception occurred:\r\n> 36 Traceback (most recent call last):\r\n> 37   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/pysr/export_torch.py\", line 151, in forward\r\n> 38     arg_ = memodict[arg]\r\n> 39 KeyError: _Node(\r\n> 40   (_args): ModuleList(\r\n> 41     (0): _Node()\r\n> 42   )\r\n> 43 )\r\n> 44 During handling of the above exception, another exception occurred:\r\n> 45 Traceback (most recent call last):\r\n> 46   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/hyperparameter_optimization.py\", line 300, in <module>\r\n> 47     optimize_hyperparams(bounds, fixed, project, id_tag, run_graph_hyperparameters)\r\n> 48   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/hyperparameter_optimization.py\", line 141, in optimize_hyperparams\r\n> 49     run_next(hparams)\r\n> 50   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/hyperparameter_optimization.py\", line 116, in run_graph_hyperparameters\r\n> 51     run_graph_neurosymbolic_search(args)\r\n> 52   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/graph_sr.py\", line 503, in run_graph_neurosymbolic_search\r\n\r\n> 53     fine_tuned_policy = fine_tune(ns_agent.policy, logdir, symbdir, hp_override)\r\n> 54   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/graph_sr.py\", line 397, in fine_tune\r\n> 55     agent.train(args.num_timesteps)\r\n> 56   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/agents/ppo_model.py\", line 213, in train\r\n> 57     act, value = self.predict(obs)\r\n> 58   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/agents/ppo_model.py\", line 107, in predict\r\n> 59     dist, value = self.policy(obs)\r\n> 60   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n> 61     return self._call_impl(*args, **kwargs)\r\n> 62   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n> 63     return forward_call(*args, **kwargs)\r\n> 64   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/common/policy.py\", line 124, in forward\r\n> 65     d, r = self.all_dones_rewards(s)\r\n> 66   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/common/policy.py\", line 156, in all_dones_rewards\r\n> 67     dones, rew = self.dr(sa)\r\n> 68   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/common/policy.py\", line 102, in dr\r\n> 69     d = self.done_model(sa)\r\n> 70   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n> 71     return self._call_impl(*args, **kwargs)\r\n> 72   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n> 73     return forward_call(*args, **kwargs)\r\n> 74   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/common/model.py\", line 1065, in forward\r\n> 75     return self.fwd(X)\r\n> 76   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/common/model.py\", line 1061, in fwd\r\n> 77     return self.model._node(symbols)\r\n> 78   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n> 79     return self._call_impl(*args, **kwargs)\r\n> 80   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n> 81     return forward_call(*args, **kwargs)\r\n> 82   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/pysr/export_torch.py\", line 153, in forward\r\n> 83     arg_ = arg(memodict)\r\n> 84   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n> 85     return self._call_impl(*args, **kwargs)\r\n> 86   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n> 87     return forward_call(*args, **kwargs)\r\n> 88   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/pysr/export_torch.py\", line 153, in forward\r\n> 89     arg_ = arg(memodict)\r\n> 90   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n> 91     return self._call_impl(*args, **kwargs)\r\n> 92   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n> 93     return forward_call(*args, **kwargs)\r\n> 94   File \"/vol/bitbucket/tfb115/train-procgen-pytorch/venvcartpole/lib/python3.8/site-packages/pysr/export_torch.py\", line 156, in forward\r\n> 95     return self._torch_func(*args)\r\n> 96 TypeError: exp(): argument 'input' (position 1) must be Tensor, not float\r\n\r\n```\r\n",
                "createdAt": "2024-06-21T11:38:48Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "It might be because your function `exp1` is returning `torch.exp(torch.FloatTensor([1]))` rather than `torch.exp(torch.tensor(1))`? (note the difference in shape)\r\n\r\n```diff\r\n- torch.exp(torch.FloatTensor([1]))\r\n+ torch.exp(torch.tensor(1))\r\n```\r\n\r\nBut normally I would just do\r\n\r\n```python\r\nextra_torch_mappings={sympy.core.numbers.Exp1: (lambda: math.exp(1.0))}\r\n```\r\n\r\nwhich is similar to the definitions for `sympy.core.numbers.Half` and `sympy.core.numbers.One`.\r\n\r\n",
                "createdAt": "2024-06-21T22:43:11Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Ok, weird, I can actually reproduce it with the following, which sounds to be the same as you saw originally:\r\n\r\n```python\r\nimport math\r\nimport pysr\r\nimport sympy\r\nimport torch\r\n\r\nex = pysr.export_sympy.pysr2sympy(\r\n    \"square(exp(sign(0.44796443))) + 1.5 * x1\",\r\n    feature_names_in=[\"x1\"],\r\n    extra_sympy_mappings={\"square\": lambda x: x**2},\r\n)\r\n\r\n\r\ndef exp1():\r\n    return torch.exp(torch.FloatTensor([1]))\r\n\r\n\r\nm = pysr.export_torch.sympy2torch(\r\n    ex, [\"x1\"], extra_torch_mappings={sympy.core.numbers.Exp1: exp1}\r\n)\r\nm(torch.randn(10, 1))  # Errors\r\n\r\n\r\nm2 = pysr.export_torch.sympy2torch(\r\n    ex, [\"x1\"], extra_torch_mappings={sympy.core.numbers.Exp1: (lambda: math.exp(1))}\r\n)\r\nm2(torch.randn(10, 1))  # Also errors\r\n\r\n```",
                "createdAt": "2024-06-21T22:57:23Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Ah, I got it! It's because we don't have a branch for `sympy.core.numbers.NumberSymbol`. Argh...\r\n\r\nhttps://github.com/MilesCranmer/PySR/blob/06ca0e376e63d563aa063028a5f9bc7fa7d849c5/pysr/export_torch.py#L94-L122\r\n\r\nWill also need to get added to the sympy2jax code I guess.",
                "createdAt": "2024-06-21T23:00:32Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOgiaAyw=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 683,
          "title": "[Feature]: Easier bug reporting/automated error reports",
          "body": "### Feature Request\n\nIt would be nice if there was an easier way for users to submit bugs, so we can get more reports, and ensure PySR is made more and more robust. Perhaps there could be an automatic report generated with all relevant info, and all the user would have to do is click a link, and it would get sent to the core devs? (I am thinking it would be safer to have it sent to the core devs directly, rather than posted publicly, just in case the report accidentally includes tokens/passwords. We would then upload the redacted version to the issues page). ",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 699,
          "title": "[Feature]: PySR interface should support ClusterManager keyword arguments",
          "body": "### Feature Request\n\nWhen using a ClusterManager, the PySRRegressor interface should allow the user to specify associated keyword arguments. There are a couple of use cases for this:\r\n\r\n1. when the user wants to pass additional flags to the workload manager designated by the `cluster_manager` argument. For example when `cluster_manager=\"slurm\"` the user may want to specify `--ntasks-per-node` or `--cpus-per-task`, etc.\r\n2. in an Apptainer container the user needs to specify the `exename` and `exeflags` keyword arguments if `cluster_manager=\"slurm\"`.",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 630,
          "title": "[Feature]: How to get the value of a constant in a fixed expression",
          "body": "### Feature Request\n\nHi, thanks for developing the helpful tool.\r\nI want to use pysr to get the values of the constants C1 and C2 in the following fixed expression:\r\ny=(1-C1)*x1/(x2+1)+x3/(C2+1)\r\nx1,x2,x3,y are known data sets.\r\nHow should I use pysr to get the constant C1 and C2?\r\nThanks！",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 626,
          "title": "Unable to run 2nd last cell",
          "body": "<img width=\"482\" alt=\"issue\" src=\"https://github.com/MilesCranmer/PySR/assets/58954022/7d02af4f-866f-41ad-b318-7bf8fb404196\">\r\n\r\nI was trying to run this segment at the end. It first gave me a warning \"It seems you are running in jupyter......\". Then I skipped the warning and resumed it after which it gave me the error shown in the given figure. Can you please help me to solve this issue.\r\n\r\nThanks in advance\r\nShubham",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "`g_input` needs to be transposed ",
                "createdAt": "2024-05-19T22:39:28Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOflM1IA=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 627,
          "title": "[BUG]: model.fit works but there's an error at the end in parse_expr",
          "body": "### What happened?\n\nWhen training a model using:\r\n`model = PySRRegressor(niterations=1000,maxsize=30,maxdepth=15,ncycles_per_iteration=100,populations=10,binary_operators=[\"+\", \"*\",\"/\"],\r\n    unary_operators=[\r\n        \"exp\",\r\n        \"inv(x) = 1/x\",\r\n        \"square\",\r\n        \"cube\",\r\n        \"sqrt\",\r\n        \"log\",\r\n        \"logistic(x) = 1 / (1 + exp(-x))\", \r\n        # ^ Custom operator (julia syntax)\r\n    ],\r\n    extra_sympy_mappings={\"inv\": lambda x: 1 / x,\"logistic\": lambda x: 1.0 / (1.0 + np.exp(-x))},)\r\n`\r\nit correctly finds and expression but then there is an error when trying to convert to an expression `loop of ufunc does not support argument 0 of type Mul which has no callable exp method`.\r\n\r\nI've tried different ways to introduce new mappings but have had no sucess. I would have expected that the error would be thrown out at the start of fitting and not at the end.\r\n\n\n### Version\n\n0.18.1\n\n### Operating System\n\nLinux\n\n### Package Manager\n\npip\n\n### Interface\n\nJupyter Notebook\n\n### Relevant log output\n\n```shell\n{\r\n\t\"name\": \"TypeError\",\r\n\t\"message\": \"loop of ufunc does not support argument 0 of type Mul which has no callable exp method\",\r\n\t\"stack\": \"---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nValueError: Error from parse_expr with transformed code: \\\"logistic (x0 )+(((x1 +-Float ('1.3587815' ))*square (x0 ))*Float ('0.00025213166' ))\\\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[9], line 1\r\n----> 1 model.fit(x, y)\r\n\r\nFile ~/.local/lib/python3.8/site-packages/pysr/sr.py:1929, in PySRRegressor.fit(self, X, y, Xresampled, weights, variable_names, X_units, y_units)\r\n   1926     self._checkpoint()\r\n   1928 # Perform the search:\r\n-> 1929 self._run(X, y, mutated_params, weights=weights, seed=seed)\r\n   1931 # Then, after fit, we save again, so the pickle file contains\r\n   1932 # the equations:\r\n   1933 if not self.temp_equation_file:\r\n\r\nFile ~/.local/lib/python3.8/site-packages/pysr/sr.py:1779, in PySRRegressor._run(self, X, y, mutated_params, weights, seed)\r\n   1776 self.julia_state_stream_ = jl_serialize(out)\r\n   1778 # Set attributes\r\n-> 1779 self.equations_ = self.get_hof()\r\n   1781 if self.delete_tempfiles:\r\n   1782     shutil.rmtree(self.tempdir_)\r\n\r\nFile ~/.local/lib/python3.8/site-packages/pysr/sr.py:2249, in PySRRegressor.get_hof(self)\r\n   2246     torch_format = []\r\n   2248 for _, eqn_row in output.iterrows():\r\n-> 2249     eqn = pysr2sympy(\r\n   2250         eqn_row[\\\"equation\\\"],\r\n   2251         feature_names_in=self.feature_names_in_,\r\n   2252         extra_sympy_mappings=self.extra_sympy_mappings,\r\n   2253     )\r\n   2254     sympy_format.append(eqn)\r\n   2256     # NumPy:\r\n\r\nFile ~/.local/lib/python3.8/site-packages/pysr/export_sympy.py:86, in pysr2sympy(equation, feature_names_in, extra_sympy_mappings)\r\n     79     feature_names_in = []\r\n     80 local_sympy_mappings = {\r\n     81     **create_sympy_symbols_map(feature_names_in),\r\n     82     **(extra_sympy_mappings if extra_sympy_mappings is not None else {}),\r\n     83     **sympy_mappings,\r\n     84 }\r\n---> 86 return sympify(equation, locals=local_sympy_mappings)\r\n\r\nFile ~/.local/lib/python3.8/site-packages/sympy/core/sympify.py:496, in sympify(a, locals, convert_xor, strict, rational, evaluate)\r\n    494 try:\r\n    495     a = a.replace('\\\r\n', '')\r\n--> 496     expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\r\n    497 except (TokenError, SyntaxError) as exc:\r\n    498     raise SympifyError('could not parse %r' % a, exc)\r\n\r\nFile ~/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py:1101, in parse_expr(s, local_dict, transformations, global_dict, evaluate)\r\n   1099 for i in local_dict.pop(null, ()):\r\n   1100     local_dict[i] = null\r\n-> 1101 raise e from ValueError(f\\\"Error from parse_expr with transformed code: {code!r}\\\")\r\n\r\nFile ~/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py:1092, in parse_expr(s, local_dict, transformations, global_dict, evaluate)\r\n   1089     code = compile(evaluateFalse(code), '<string>', 'eval')\r\n   1091 try:\r\n-> 1092     rv = eval_expr(code, local_dict, global_dict)\r\n   1093     # restore neutral definitions for names\r\n   1094     for i in local_dict.pop(null, ()):\r\n\r\nFile ~/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py:907, in eval_expr(code, local_dict, global_dict)\r\n    901 def eval_expr(code, local_dict: DICT, global_dict: DICT):\r\n    902     \\\"\\\"\\\"\r\n    903     Evaluate Python code generated by ``stringify_expr``.\r\n    904 \r\n    905     Generally, ``parse_expr`` should be used.\r\n    906     \\\"\\\"\\\"\r\n--> 907     expr = eval(\r\n    908         code, global_dict, local_dict)  # take local objects in preference\r\n    909     return expr\r\n\r\nFile <string>:1\r\n\r\nCell In[8], line 12, in <lambda>(x)\r\n      1 model = PySRRegressor(niterations=1000,maxsize=30,maxdepth=15,ncycles_per_iteration=100,populations=10,binary_operators=[\\\"+\\\", \\\"*\\\",\\\"/\\\"],\r\n      2     unary_operators=[\r\n      3         \\\"exp\\\",\r\n      4         \\\"inv(x) = 1/x\\\",\r\n      5         \\\"square\\\",\r\n      6         \\\"cube\\\",\r\n      7         \\\"sqrt\\\",\r\n      8         \\\"log\\\",\r\n      9         \\\"logistic(x) = 1 / (1 + exp(-x))\\\", \r\n     10         # ^ Custom operator (julia syntax)\r\n     11     ],\r\n---> 12     extra_sympy_mappings={\\\"inv\\\": lambda x: 1 / x,\\\"logistic\\\": lambda x: 1.0 / (1.0 + np.exp(-x))},)\r\n     14 x = np.array([merged_df[\\\"N\\\"].values,merged_df[\\\"voltage\\\"].values]).T\r\n     15 y = merged_df[\\\"P1\\\"].values*10\r\n\r\nTypeError: loop of ufunc does not support argument 0 of type Mul which has no callable exp method\"\r\n}\n```\n\n\n### Extra Info\n\n_No response_",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "It’s because `np.exp` is not a sympy function, it’s a numpy function",
                "createdAt": "2024-05-20T12:00:34Z"
              },
              {
                "author": {
                  "login": "SrGonao"
                },
                "body": "For some reason, I thought that using sympy.exp() had not worked. \r\nTrying it now and everything seems ok. ~\r\nThanks for the quick reply.",
                "createdAt": "2024-05-20T12:36:34Z"
              },
              {
                "author": {
                  "login": "SrGonao"
                },
                "body": "Sorry I remember why I used np.exp(). I'm still getting the error when having sympy.exp() and also get this extra warning:\r\n```p1_fitting_no_ions.py:57: DeprecationWarning: scipy.exp is deprecated and will be removed in SciPy 2.0.0, use numpy.exp instead\r\n  extra_sympy_mappings={\"inv\": lambda x: 1 / x,\"logistic\": lambda x: 1.0 / (1.0 + sp.exp(-x))},\r\nValueError: Error from parse_expr with transformed code: \"(-Float ('0.68147457' )+logistic (x1 ))*x0 \"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"p1_fitting_no_ions.py\", line 73, in <module>\r\n    model.fit(x, y)\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/pysr/sr.py\", line 1929, in fit\r\n    self._run(X, y, mutated_params, weights=weights, seed=seed)\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/pysr/sr.py\", line 1779, in _run\r\n    self.equations_ = self.get_hof()\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/pysr/sr.py\", line 2249, in get_hof\r\n    eqn = pysr2sympy(\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/pysr/export_sympy.py\", line 86, in pysr2sympy\r\n    return sympify(equation, locals=local_sympy_mappings)\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/sympy/core/sympify.py\", line 496, in sympify\r\n    expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py\", line 1101, in parse_expr\r\n    raise e from ValueError(f\"Error from parse_expr with transformed code: {code!r}\")\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py\", line 1092, in parse_expr\r\n    rv = eval_expr(code, local_dict, global_dict)\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py\", line 907, in eval_expr\r\n    expr = eval(\r\n  File \"<string>\", line 1, in <module>\r\n  File \"p1_fitting_no_ions.py\", line 57, in <lambda>\r\n    extra_sympy_mappings={\"inv\": lambda x: 1 / x,\"logistic\": lambda x: 1.0 / (1.0 + sp.exp(-x))},\r\n  File \"/home/gpaulo/.local/lib/python3.8/site-packages/scipy/_lib/deprecation.py\", line 20, in call\r\n    return fun(*args, **kwargs)\r\nTypeError: loop of ufunc does not support argument 0 of type Mul which has no callable exp method```\r\n",
                "createdAt": "2024-05-22T09:03:37Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Are you calling scipy.exp? It should be sympy.exp. (The error message is from scipy it looks like)",
                "createdAt": "2024-05-22T09:11:27Z"
              },
              {
                "author": {
                  "login": "SrGonao"
                },
                "body": "Now that I'm reading the error it is strange that it does say scipy but I'm doing `import sympy as sp` and then sp.exp. \r\n",
                "createdAt": "2024-05-22T09:27:40Z"
              },
              {
                "author": {
                  "login": "SrGonao"
                },
                "body": "I'm sorry for wasting your time. I was indeed using scipy in this script. I had fixed it in other scripts but not this one. ",
                "createdAt": "2024-05-22T09:34:33Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOfp6LGA=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 666,
          "title": "AttributeError: module 'pysr' has no attribute 'Problem'",
          "body": "### Discussed in https://github.com/MilesCranmer/PySR/discussions/665\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **wkharold** July 10, 2024</sup>\r\nI'm just getting started with PySR. Walking through the Toy Examples with Code. When I do \r\n```python\r\nfrom pysr import *\r\n```\r\nI get the error in the title:\r\n```\r\nAttributeError: module 'pysr' has no attribute 'Problem'\r\n```\r\nI am running IPython in an Apptainer container built from the `continuumio/miniconda3` Docker image.  \r\n\r\nWhen I switch to \r\n```python\r\nimport pysr\r\n```\r\nand preface things with `pysr.` as required everything works as expected.</div>",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Thanks @wkharold for the bug report, seems to be a stale `__all__`: https://github.com/MilesCranmer/PySR/blob/db449385ad189ac150c7f198ea3edfa302a60f52/pysr/__init__.py#L21",
                "createdAt": "2024-07-11T01:41:57Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOhG5lDA=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 690,
          "title": "Replace spaces with underscores in column names also for the predict function",
          "body": "### Discussed in https://github.com/MilesCranmer/PySR/discussions/689\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **@GoldenGoldy** August  3, 2024</sup>\r\nI found that PySR warns about spaces in column names when passing the .fit function data where this occurs. It then replaces the spaces in the column names with underscores and prints a warning about this. You can then proceed with fitting the data as per normal.\r\nWhen later calling the .predict function, this does not attempt to make the same replacement of spaces with underscores in the column names.\r\nSo, if we have a fitted model and want to use it to make predictions, and we pass data to the .predict function in the same format that we used for the .fit function, we can run into the following issue:\r\nThe predict function (in sr.py) contains the following code line \"X = X.reindex(columns=self.feature_names_in_)\". This results in NaN values in case the column names have spaces, because now it tries to match the column names (with spaces) with the feature names of the model, but in the latter the spaces were replaced by underscores.\r\nWe then get the somewhat confusing message \"ValueError: Input X contains NaN.\", which leads one to believe that there are NaN values in the data even while there are none, they only get introduced by the reindex which can't match the column names.\r\n\r\nAll this can be avoided of course, once you are aware of the problem and avoid using spaces in the column names from the beginning. However, it might be more consistent, and allow for a better user experience, if the .predict function also replaces spaces in the column names with underscores?\r\n\r\n</div>",
          "comments": {
            "nodes": [],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": null
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 686,
          "title": "[Feature]: Apptainer support",
          "body": "### Feature Request\n\nHPC System admins generally don't install or run Docker in their clusters. Apptainer / Singularity has become the de facto container platform. PySR should include an Apptainer definition file which researchers can use, or extend, to build PySR Apptainer containers.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Closed with #687 ",
                "createdAt": "2024-08-12T03:34:05Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "P.S., let me know how things work with multi-node for you. Currently it relies on ClusterManagers.jl + Distributed.jl. It works OK but I've had some issues with ClusterManagers.jl before in the past. Eventually I'd like to switch to using MPIClusterManagers.jl which relies on MPI.jl which is probably much more compatible across clusters. https://github.com/JuliaParallel/MPIClusterManagers.jl",
                "createdAt": "2024-08-13T08:10:18Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOiDvpaA=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 672,
          "title": "[Feature]: Custom loss function implementation in python ",
          "body": "### Feature Request\n\nHello. I would like to have a feature, where I can pass custom function loss implemented in python instead of in julia. I'm not able to implement it in julia as it part of bigger optimization pipeline. I believe this feature can be implemented by passing python function pointer to julia and executing it from there.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Maybe you could try PythonCall.jl? https://juliapy.github.io/PythonCall.jl/stable/pythoncall/ It lets you call Python functions from Julia. See https://astroautomata.com/PySR/examples/#7-julia-packages-and-types for an example of using an external Julia package in the loss function.",
                "createdAt": "2024-07-16T22:52:34Z"
              },
              {
                "author": {
                  "login": "xnerhu"
                },
                "body": "> Maybe you could try PythonCall.jl? https://juliapy.github.io/PythonCall.jl/stable/pythoncall/ It lets you call Python functions from Julia. See https://astroautomata.com/PySR/examples/#7-julia-packages-and-types for an example of using an external Julia package in the loss function.\r\n\r\nIt somewhat worked thanks.\r\n\r\n```\r\nimport os\r\n\r\n\r\nos.environ[\"PYTHON_JULIACALL_THREADS\"] = \"1\"\r\nfrom datetime import datetime\r\nimport numpy as np\r\nfrom pysr import PySRRegressor\r\nfrom pysr import jl\r\n\r\nfrom julia.api import Julia\r\n\r\n\r\nx = None\r\n\r\n\r\ndef custom_loss(y_true, y_pred):\r\n    # global x\r\n    # print(y_true, y_pred, x)\r\n    # if x is None:\r\n    #     x = np.random.rand()\r\n    return float((y_true - y_pred) ** 2)\r\n\r\n\r\njl.seval(\r\n    \"\"\"\r\nimport Pkg\r\nPkg.add(\"PythonCall\")\r\n\"\"\"\r\n)\r\njl.custom_loss_function = custom_loss\r\n\r\njl.seval(\r\n    \"\"\"\r\nfunction custom_loss_wrapper(y_true, y_pred)\r\n    py_obj = PythonCall.pycall(custom_loss_function, y_true, y_pred)\r\n    return PythonCall.pyconvert(Float32, py_obj)\r\nend\r\n\"\"\"\r\n)\r\n\r\nX = np.random.rand(100, 2)\r\ny = X[:, 0] * X[:, 1] + np.random.rand(100) * 0.1\r\n\r\nmodel = PySRRegressor(\r\n    niterations=40,\r\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\r\n    unary_operators=[\"cos\", \"exp\"],\r\n    elementwise_loss=\"custom_loss_wrapper\",\r\n)\r\n\r\nmodel.fit(X, y)\r\nprint(model)\r\npredictions = model.predict(X)\r\nprint(predictions)\r\n\r\n```\r\n\r\nThough I had to do `os.environ[\"PYTHON_JULIACALL_THREADS\"] = \"1\"` from https://github.com/MilesCranmer/PySR/issues/661\r\n\r\nbecause of\r\n\r\n```\r\n  Resolving package versions...\r\n  No Changes to `C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\Project.toml`\r\n  No Changes to `C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\Manifest.toml`\r\nCompiling Julia backend...\r\n\r\nPlease submit a bug report with steps to reproduce this fault, and any error messages that follow (in their entirety). Thanks.\r\nException: EXCEPTION_ACCESS_VIOLATION at 0x7ff85e1587a5 --  at 0x7ff85e1587a5 -- ION with steps to reproduce this fault, and any error messages that follow (in their entirety). Thanks.\r\nException: EXCEPTION_ACCESS_VIOLATION at 0x7ff85e163a4d -- PyObject_GC_Malloc at C:\\Python310\\python310.dll (unknown line)  \r\nnd any error messages that follow (in their entirety). Thanks.\r\nException: EXCEPTION_ACCESS_VIOLATION at 0x7ff85e1587a5 --  at 0x7ff85e1587a5 -- IONC:\\Python310\\python310.dll (unknown lin expression starting at none:0\r\n\\python310.dll (unknown line)\r\nin expression starting at none:0\r\nin expression starting at none:0\r\n\\python310.dll (unknown line)\r\nne)\r\nin expression starting at none:0\r\nin expression starting at none:0\r\n\\python310.dll (unknown line)\r\nPyEval_EvalFrameDefault at C:\\PytPyEval_EvalFrameDefault at C:\\Python310\\python310.dll (unknown line)\r\nPyFunction_Vectorcall at C:\\Python310\\python310.dll (unknown line)\r\n\r\nPyFunction_Vectorcall at C:\\Python310\\python310.dll (unknown linyFunction_Vectorcall at C:\\Python310\\python310.dll (unknown line)\r\nPyVectorcall_Call at C:\\Python310\\python310.dll (unknown line)\r\ne)\r\nPyVectorcall_Call at C:\\Python310\\python310.dll (unknown line)\r\nPyVectorcall_Call at C:\\Python310\\python310.dll (unknown line)\r\nPyObject_Call at efault at C:\\Python310\\python310.dll (unknown PyObject_Call at C:\\Python310\\python310.dll (unknown line)   \r\nwn line)\r\nPyObject_Call at C:\\Python310\\python310.dll (unknown line)\r\nPyFunction_Vectorcall at C:\\Python310\\python310.dll (unknowPyFunction_Vectorcall at C:\\Python310\\python310.dll (unknown line)\r\nPyObject_CallObject at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\C\\pointers.jl:297 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\Py.jl:132 [inlined]\r\npycallargs at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:212\r\nunknown function (ip: 00000138b4d57ddf)\r\n#pycall#21 at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:230\r\n#pycall#21 at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\#pycall#21 at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:230\r\nnlined]\r\nined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\PythonCamacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\Py.jl:132 [inlined]\r\npycallargs at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:212\r\njl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\ndo_apply at C:/workdir/src\\builtins.c:768\r\ndo_apply at C:/workdir/src\\builtins.c:76pycall at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:220 \r\npycall#21 at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:230\r\n:297 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\Py.jl:132 [inlined]\r\npycallargs at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:212\r\nunknown function (ip: 00000138b4d57ddf)\r\nages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:212\r\nunknown function (ip: 00000138b4unknown function (ip: 00000138b4d57ddf)\r\nl at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:19 [inlined]\r\n#4 at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:187 [inlined]\r\nmapreduce_impl at .\\reduce.jl:262\r\nmapreduce_impl at .\\reduce.jl:262\r\nnments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdljl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\ndo_apply at C:/workdir/src\\builtins.c:768\r\npycall at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:220\r\ncustom_loss_wrapper at .\\none:2\r\nl at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:19 [inlined]\r\n#4 at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:187 [inlined]\r\nmapreduce_impl at .\\reduce.jl:262\r\nmapreduce_impl at .\\reduce.jl:277 [inlined]\r\n_mapreduce at .\\reduce.jl:447\r\njl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\ndo_apply at C:/workdir/src\\builtins.c:768\r\npycall at C:\\Users\\xnerhu\\.julia\\packages\\PythonCall\\S5MOg\\src\\Core\\builtins.jl:220\r\npycall at C:\\Users\\xnerhu\\.julia\\packages\\Pytho#mapreduce#821 at .\\reducedim.jl:357 [inlined]\r\nmapreduce at .\\reducedim.jl:357 [inlined]\r\n#_sum#831 at .\\reducedim.jl:1015 [inlined]\r\n_sum at .\\reducedim.jl:1015 [inlined]\r\n#sum#829 at .\\reducedim.jl:1011 [inlined]\r\nsum at .\\reducedim.jl:1011 [inlined]\r\n_mean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:187\r\n_mean at C:\\Users\\xnerhu\\.julia\\_mean at C:\\Users\\xnerhu\\.julia\\environments_mapreduce at .\\reduce.jl:447\r\nia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\nmean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\n_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\nunknown function (ip: 00000138b4df92cf)\r\nunknown function (ip: 00000138_mapreduce_dim at .\\reducedim.jl:365 [inlined]\r\n#mapreduce#821 at .\\reducedim.jl:357 [inlined]\r\nmapreduce at .\\reducedim.jl:357 [inlined]\r\n#_sum#831 at .\\reducedim.jl:1015 [inlined]\r\n_sum at .\\reducedim.jl:1015 [inlined]\r\n#sum#829 at .\\reducedim.jl:1011 [inlined]\r\nsum at .\\reducedim.jl:1011 [inlined]\r\n_mean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:187\r\n_mean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1_mean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:186\r\n_eval_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:67\r\n#mean#1 at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\nmean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\n_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\njl:105\r\n_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\nunknown function (ip: 00000138b4df92cf)\r\nunknown function (ip: 00000138b4df92cf)\r\nSymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\njl:105\r\nStatistics\\src\\Statistics.jl:104 [inlined]\r\n]\r\nmean at C:\\Users\\xnerhu\\.julia\\environments\\pyjuliapkg\\pyjuliapkg\\install\\share\\julia\\stdlib\\v1.10\\Statistics\\src\\Statistics.jl:104 [inlined]\r\n_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\nunknown function (ip: 00000138b4df92cf)\r\nSymbolicRegression\\FtJSD\\src\\LossFunctions.jl:20\r\nb\\v1.10\\Stati#score_func#5 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:164 [inlined]\r\nscore_func at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:161 [inlined]\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\p#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n1#eval_loss#3 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:105\r\n_eval_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:67\r\neval_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:97 [inlined]\r\n#score_func#5 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:164 [inlined]\r\nscore_func at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:161 [inlined]\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n1 [ieval_loss at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:97 [inlinPopMember at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\n#2 at .\\none:0 [inlined]\r\niterate at .\\generator.jl:47 [inlined]\r\nages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\nPopMember at C:\\Users\\xnerhu\\.julia\\paccollect at .\\array.jl:834\r\ncollect at .\\array.jl:834\r\nu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\n#2 at .\\none:0 [inlined]\r\niterate at .\\generator.jl:47 [inlined]\r\ncollect at .\\array.jl:834\r\n#Population#1 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:49 [inlined]#score_func#5 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:164 [inlined]\r\nscore_func at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\LossFunctions.jl:161 [inlined]\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n#PopMember#2 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:99\r\n1 [inlined]Population at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:35 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SymbolicRegression.jl:765 [inlined]\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\n8 [inlined]\r\nPopMember at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\n#2 at .\\none:0 [inlined]\r\niterate at .\\generator.jl:47 [inlined]\r\ncollect at .\\array.jl:834\r\ncollect at .\\array.jl:834\r\n7 [inlined]\r\nages\\SymbolicRegression\\FtJSD\\src\\PopMember.jl:88 [inlined]\r\n:765Population at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:35 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SymbolicRegression.jl:765 [inlined]\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\njfptr_YY.54_21945 at C:\\Users\\xnerhu\\.julia\\compiled\\v1.10\\SymbolicRegression\\X2eIS_JE3Mg.dll (unknown line)\r\n#Population#1 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:49 [inlined]\r\nPopulation at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\Population.jl:35 [inlined]\r\nmacro expansion at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SymbolicRegression.jl:765 [inlined]\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\n#54 at C:\\Users\\xnerhu\\.julia\\packages\\SymbolicRegression\\FtJSD\\src\\SearchUtils.jl:116\r\nRegression.jl:765 [injl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\nstart_task at C:/workdir/src\\task.c:1238\r\nAllocations: 17550813 (Pool: 17526745; Big: 24068); GC: 21\r\njfptr_YY.54_21945 at C:\\Users\\xnerhu\\.julia\\compiled\\v1.10\\SymbolicRegression\\X2eIS_JE3Mg.dll (unknown line)\r\njl_apply at C:/workdir/src\\julia.h:1982 [inlined]\r\nstart_task at C:/workdir/src\\task.c:1238\r\nAllocations: 17550813 (Pool: 17526745; Big: 24068); GC: 21\r\n```\r\n\r\nalso, how can I do loss_function intead of elemenetwise_loss? I think seval needs to have explicit types in args\r\n\r\n```\r\njuliacall.JuliaError: MethodError: no method matching custom_loss_wrapper(::Node{Float32}, ::Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, @NamedTuple{}, Nothing, Nothing, Nothing, Nothing}, ::Options{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, Node, false, false, nothing, StatsBase.Weights{Float64, Float64, Vector{Float64}}})\r\n```\r\n",
                "createdAt": "2024-07-17T09:12:07Z"
              },
              {
                "author": {
                  "login": "xnerhu"
                },
                "body": "btw is it possible to do multivaritive prediction (multiple outpouts)?",
                "createdAt": "2024-07-17T09:24:36Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOhRaS2Q=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 629,
          "title": "[Feature]: Unary operator to pick an item at index from current X",
          "body": "### Feature Request\r\n\r\nIs there a way to define a custom unary operator `getXAt` that takes an integer `i` as a parameter and returns `X_i`? This could possibly allow creating a similar mechanism to attention.",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Not yet but all we need to do is make SymbolicRegression.jl compatible with `GenericOperatorEnum` – see https://github.com/SymbolicML/DynamicExpressions.jl?tab=readme-ov-file#tensors. Then the operators could do anything at all – even operations on non-numerical data.\r\n\r\nRight now it is fixed to only use `OperatorEnum` by this line: https://github.com/MilesCranmer/SymbolicRegression.jl/blob/71447ee7bee18fc836a7d7bb4c8ce5d249bc0e7c/src/Options.jl#L718-L722. \r\n\r\nWe would need to change that to allow the user to specify the `AbstractOperatorEnum` type, and then basically fix up all the error messages that will show up. But I feel like it wouldn't be too bad.\r\n\r\n(There are also n-ary operators planned which someone is working on, those could be useful here as well)",
                "createdAt": "2024-05-22T14:26:45Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOfqgKmA=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 691,
          "title": "[Feature]: Random Seed for Reproducibility",
          "body": "### Feature Request\r\n\r\nFor a paper, I am trying to make my code reproducible. I have set python random seed. Still every time it runs it provides different results. \r\n\r\nIs it posslbe to set a random seed? I cant find it in the docs.\r\n\r\n```python\r\n# %%\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom pysr import PySRRegressor\r\n\r\n# Random seed\r\nnp.random.seed(42)\r\n\r\ndata_dict = {\r\n    100: {\r\n        \"x_data\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\r\n        \"means\": [0.0, 0.2625, 0.7275, 0.86, 0.9125, 0.94, 0.9325, 0.95, 0.9624999999999999, 0.9775] \r\n    },\r\n    1000: {\r\n        \"x_data\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180],\r\n        \"means\": [0.0, 0.0, 0.115, 0.6116666666666667, 0.855, 0.9550000000000001, 0.97, 0.9525, 0.9624999999999999, 0.94, 0.9724999999999999, 0.975, 0.985, 0.9875] \r\n    },\r\n    3162: {\r\n        \"x_data\": [10, 20, 30, 40, 50, 60, 70, 80, 90],\r\n        \"means\": [0.0, 0.0, 0.0025, 0.6375, 0.8925000000000001, 0.8925000000000001, 0.935, 0.9425, 0.98] \r\n    },\r\n    10000: {\r\n        \"x_data\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180],\r\n        \"means\": [0.0, 0.0, 0.0075, 0.28, 0.755, 0.9199999999999999, 0.8825000000000001, 0.9299999999999999, 0.955, 0.9624999999999999, 0.9775, 0.95, 0.9775, 0.985]\r\n    },\r\n    100000: {\r\n        \"x_data\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180],\r\n        \"means\": [0.0, 0.0, 0.0, 0.2525, 0.5575, 0.7050000000000001, 0.7525, 0.8175, 0.9325, 0.83, 0.93, 0.85, 0.94, 0.97] \r\n    }\r\n}\r\n\r\n# Creating and concatenating DataFrames\r\ndata_frames = []\r\nfor n, data in data_dict.items():\r\n    df = pd.DataFrame(data)\r\n    df[\"n\"] = n\r\n    data_frames.append(df)\r\n\r\n# Concatenate all DataFrames\r\ndf = pd.concat(data_frames)\r\n\r\ndf = df[df['x_data']>19]\r\ndf = df[df['x_data']<101]\r\n# %%\r\nX = df[[\"x_data\", \"n\"]]\r\ny = df[\"means\"]\r\n\r\n# %%\r\n# Initialize and fit the symbolic regressor\r\nmodel = PySRRegressor(\r\n    binary_operators=[\"*\", \"^\",\"+\", \"/\"],\r\n    unary_operators=['exp'],\r\n    # dimensional_constraint_penalty=10**5,\r\n)\r\n# %%\r\n# Fit the model\r\nmodel.fit(X, y)\r\nmodel\r\n```\r\n",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "cmougan"
                },
                "body": "Also, often the algorithm gets stuck in an iteration. Sometimes it takes 2 secs",
                "createdAt": "2024-08-03T09:25:06Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "You can use `deterministic=True` in the PySRRegressor. See the docs here: https://astroautomata.com/PySR/api/",
                "createdAt": "2024-08-03T15:56:09Z"
              },
              {
                "author": {
                  "login": "cmougan"
                },
                "body": "Thanks! I see its in the API, but its not in the detailed example https://astroautomata.com/PySR/",
                "createdAt": "2024-08-03T16:36:04Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Yeah there are too many features to have them all in the detailed example and still have it be useful. But maybe in the Toy examples page it could be good? https://astroautomata.com/PySR/examples/\r\n\r\nIf you are interested it becoming a PySR contributor I'd welcome a PR adding a toy example to that page demonstrating deterministic settings :)",
                "createdAt": "2024-08-03T16:46:50Z"
              },
              {
                "author": {
                  "login": "cmougan"
                },
                "body": "Thanks! We are writting a paper about finding scaling laws of LLMs. I am trying to use your work here. If everything goes all right I will be happy to write an example if you find it meaningful. ",
                "createdAt": "2024-08-03T16:51:15Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOhyAWPA=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 655,
          "title": "[BUG]: Failed to customize and let PySR use the modified backend.",
          "body": "### What happened?\n\nI'm trying to customize and let PySR use the modified backend according to instructions in https://astroautomata.com/PySR/backend/.\r\nBut getting errors as shown follows:\r\nERROR: Ignored the following yanked versions: 0.4.1, 0.5.0\r\nERROR: Ignored the following versions that require a different python version: 0.9.20 Requires-Python ~=3.8\r\nERROR: Could not find a version that satisfies the requirement juliacall==0.9.20 (from pysr) (from versions: 0.4.2, 0.4.3, 0.5.1, 0.6.0, 0.6.1, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.9.5, 0.9.6, 0.9.7, 0.9.8, 0.9.9, 0.9.10, 0.9.11, 0.9.12, 0.9.13, 0.9.14, 0.9.15, 0.9.16, 0.9.17, 0.9.18, 0.9.19)\r\nERROR: No matching distribution found for juliacall==0.9.20\r\n\n\n### Version\n\npysr version: 0.18.3\n\n### Operating System\n\nLinux\n\n### Package Manager\n\npip\n\n### Interface\n\nScript (i.e., `python my_script.py`)\n\n### Relevant log output\n\n```shell\nProcessing /home/perception/Jun/PySR\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Installing backend dependencies ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nRequirement already satisfied: sympy<2.0.0,>=1.0.0 in /home/perception/anaconda3/envs/CAVperception/lib/python3.7/site-packages (from pysr==0.18.5) (1.10.1)\r\nRequirement already satisfied: pandas<3.0.0,>=0.21.0 in /home/perception/anaconda3/envs/CAVperception/lib/python3.7/site-packages (from pysr==0.18.5) (1.3.5)\r\nRequirement already satisfied: numpy<2.0.0,>=1.13.0 in /home/perception/anaconda3/envs/CAVperception/lib/python3.7/site-packages (from pysr==0.18.5) (1.21.5)\r\nRequirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /home/perception/anaconda3/envs/CAVperception/lib/python3.7/site-packages (from pysr==0.18.5) (1.0.2)\r\nINFO: pip is looking at multiple versions of pysr to determine which version is compatible with other requirements. This could take a while.\r\nERROR: Ignored the following yanked versions: 0.4.1, 0.5.0\r\nERROR: Ignored the following versions that require a different python version: 0.9.20 Requires-Python ~=3.8\r\nERROR: Could not find a version that satisfies the requirement juliacall==0.9.20 (from pysr) (from versions: 0.4.2, 0.4.3, 0.5.1, 0.6.0, 0.6.1, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.9.5, 0.9.6, 0.9.7, 0.9.8, 0.9.9, 0.9.10, 0.9.11, 0.9.12, 0.9.13, 0.9.14, 0.9.15, 0.9.16, 0.9.17, 0.9.18, 0.9.19)\r\nERROR: No matching distribution found for juliacall==0.9.20\n```\n\n\n### Extra Info\n\n_No response_",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Perhaps you can try uninstalling everything and then reinstalling?",
                "createdAt": "2024-06-19T06:04:31Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOgc7jqw=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 639,
          "title": "[BUG]: Piecewise not in torch_mappings",
          "body": "### What happened?\n\nafter fitting a pysr module with \"greater\" as a binary operator, exporting to torch failed with the following error:\r\n\r\n`KeyError: 'Function Piecewise was not found in Torch function mappings.Please add it to extra_torch_mappings in the format, e.g., {sympy.sqrt: torch.sqrt}.'\r\n`\r\n\r\nI've seen that in #433 Piecewise was added to the mappings, so I'm surprised to see this error.\r\n\r\n\r\nI did attempt to fix myself, but it didn't work out:\r\nI've tried adding mappings such as: \r\n\r\n`{sympy.Piecewise: lambda x, y: torch.where(x[1], x[0], y[0])}`\r\n\r\nbut then the same error arises for `sympy.functions.elementary.piecewise.ExprCondPair` and then `sympy.logic.boolalg.BooleanTrue`\r\n\r\nin the end, I added\r\n```\r\nextra_torch_mappings = {\r\n        sympy.Piecewise: lambda x, y: torch.where(x[1], x[0], y[0]),\r\n        sympy.functions.elementary.piecewise.ExprCondPair: tuple,\r\n        sympy.logic.boolalg.BooleanTrue: torch.BoolTensor,\r\n        \"greater\": lambda x, y: torch.where(x > y, 1.0, 0.0),\r\n    }\r\n```\r\n\r\nBut even this produced the following error:\r\n\r\n`KeyError: 'Function ITE was not found in Torch function mappings.Please add it to extra_torch_mappings in the format, e.g., {sympy.sqrt: torch.sqrt}.'\r\n`\r\n\r\nHopefully, I am missing something obvious?\n\n### Version\n\n0.18.4\n\n### Operating System\n\nLinux\n\n### Package Manager\n\npip\n\n### Interface\n\nScript (i.e., `python my_script.py`)\n\n### Relevant log output\n\n_No response_\n\n### Extra Info\n\n_No response_",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "tbuckworth"
                },
                "body": "I just realised that #433 is a pull request, so I copied the code and used it to add the mappings manually. \r\nHowever, I'm still getting the error:\r\n`KeyError: 'Function ITE was not found in Torch function mappings.Please add it to extra_torch_mappings in the format, e.g., {sympy.sqrt: torch.sqrt}.'`",
                "createdAt": "2024-06-03T09:49:53Z"
              },
              {
                "author": {
                  "login": "tbuckworth"
                },
                "body": "I've added this mapping, which seems to circumvent the error, but I haven't fully tested it yet:\r\n```\r\ndef if_then_else(*conds):\r\n    a, b, c = conds\r\n    return torch.where(a, torch.where(b, True, False), torch.where(c, True, False))\r\n```\r\n\r\n`extra_torch_mappings = {sympy.logic.boolalg.ITE: if_then_else}`\r\n",
                "createdAt": "2024-06-03T10:19:43Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Nice! Yeah that should be added to the GitHub pull request. Feel free to suggest that on the PR via the review system and you will be credited as a coauthor of the PR.",
                "createdAt": "2024-06-03T11:12:19Z"
              },
              {
                "author": {
                  "login": "tbuckworth"
                },
                "body": "Thanks! I'll add a review comment on the PR.\r\n\r\nThere was another error with `piecewise`, when cond is a float (1.), but I fixed it by replacing `cond` with `cond.bool()`:\r\n\r\n```\r\noutput += torch.where(\r\n                    cond.bool() & ~already_used, expr, torch.zeros_like(expr)\r\n                )\r\n                already_used = already_used | cond.bool()\r\n```\r\n\r\nNow, as long as I use a single batch dimension, it works, but multiple batch dimensions fail.\r\n\r\nI believe this is due to export_torch.py, where _SingleSymPyModule.forward is:\r\n\r\n```\r\n            def forward(self, X):\r\n                if self._selection is not None:\r\n                    X = X[:, self._selection]\r\n                symbols = {symbol: X[:, i] for i, symbol in enumerate(self.symbols_in)}\r\n                return self._node(symbols)\r\n```\r\n\r\nif `X[:,` is replaced with `X[...,` then i believe it will work. This is a separate issue though, I suppose\r\n\r\n\r\n",
                "createdAt": "2024-06-03T11:53:10Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "(Just leaving it open until that PR is closed, since there are still some TODO items)",
                "createdAt": "2024-06-03T16:15:48Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOf-OL9A=="
            }
          }
        }
      }
    }
  },
  {
    "data": {
      "repository": {
        "issue": {
          "number": 662,
          "title": "[BUG]: Freezes after launching pysr in streamlit app",
          "body": "### What happened?\n\nI am currently trying to launch pysr from streamlit app and after running model.fit(x_train, y_train) \r\nI see only this and nothing else:\r\n**Compiling Julia backend...\r\n[ Info: Started!**\r\nrunning it just as a python script or in jupiter notebook works\r\nI also trying calling pysr using ThreadPoolExecutor and same happens\r\n\r\nWould appreciate any suggestions\r\n\n\n### Version\n\n0.19.0\n\n### Operating System\n\nmacOS\n\n### Package Manager\n\nConda\n\n### Interface\n\nOther (specify below)\n\n### Relevant log output\n\n```shell\nCompiling Julia backend...\r\n[ Info: Started!\n```\n\n\n### Extra Info\n\n_No response_",
          "comments": {
            "nodes": [
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Hi @bahonya,\r\nCan you give a full code example so I can reproduce it? I am not familiar with streamlit.\r\nThanks,\r\nMiles",
                "createdAt": "2024-07-06T13:58:31Z"
              },
              {
                "author": {
                  "login": "bahonya"
                },
                "body": "Hi @MilesCranmer , thanks for the quick response\r\n\r\n\r\nI got it, it happens because I change directory during the run, if I remove os.chdir in example below it works. I have to change the directory durung the run. I will look for a workaround. I think I will close this ticket then \r\n\r\n\r\n`\r\nimport streamlit as st\r\nimport numpy as np\r\nimport pandas as pd\r\nimport os\r\nfrom pysr import PySRRegressor\r\n\r\n\r\ndef generate_data(n_samples):\r\n    X = np.random.rand(n_samples, 1) * 10\r\n    y = 2 * X.squeeze() + np.random.randn(n_samples) * 2\r\n    return X, y\r\nst.title(\"example app\")\r\nst.sidebar.header(\"parameters to generate data\")\r\nn_samples = st.sidebar.slider(\"number of samples\", min_value=10, max_value=1000, value=100)\r\nX, y = generate_data(n_samples)\r\nst.subheader(\"generated data\")\r\ndata = pd.DataFrame(data={'X': X.squeeze(), 'y': y})\r\nst.write(data)\r\nmodel = PySRRegressor(\r\n    populations=5,\r\n    niterations=100,\r\n    binary_operators=[\"+\", \"*\"],\r\n    unary_operators=[\"sin\", \"cos\"],\r\n)\r\nif st.button(\"Fit the model\"):\r\n    os.chdir('subfolder')\r\n    model.fit(X, y)\r\n    os.chdir('..')\r\n    st.subheader(\"found equations\")\r\n    st.write(model.equations_)\r\n    st.subheader(\"predictions\")\r\n    y_pred = model.predict(X)\r\n    st.write(\"predicted values\", y_pred)\r\n    import matplotlib.pyplot as plt\r\n    plt.scatter(X, y, label='ground truth', color='blue')\r\n    plt.scatter(X, y_pred, label='predicted values', color='red')\r\n    plt.legend()\r\n    st.pyplot(plt)\r\n    `",
                "createdAt": "2024-07-07T08:18:12Z"
              },
              {
                "author": {
                  "login": "MilesCranmer"
                },
                "body": "Cool!\r\n\r\nIf you are interested, maybe we could work together on a GUI? I have a start on this PR: https://github.com/MilesCranmer/PySR/pull/589. It's with gradio rather than streamlit but parts are similar. The goal is to have it be compatible with HuggingFace spaces so you can have web version of PySR.",
                "createdAt": "2024-07-08T22:20:51Z"
              }
            ],
            "pageInfo": {
              "hasNextPage": false,
              "endCursor": "Y3Vyc29yOnYyOpHOhA0PXA=="
            }
          }
        }
      }
    }
  }
]