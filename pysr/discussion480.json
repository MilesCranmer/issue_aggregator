{
  "data": {
    "repository": {
      "discussion": {
        "number": 480,
        "title": "How to speed up custom loss function?",
        "body": "I have custom loss function ( https://gist.github.com/DenisSvirin/64952e83496968cfce9d117328670c04 ).\r\n\r\n<details><summary>(edit - pasted below)</summary>\r\n\r\n```julia\r\ninclude(\"data/fcc_coords.jl\")\r\ninclude(\"data/potential.jl\")\r\ninclude(\"data/Training_r.jl\")\r\ninclude(\"data/Target_properties.jl\")\r\ninclude(\"data/Training_E.jl\")\r\nusing Symbolics\r\n\r\n@variables x1 x2 x3\r\n\r\nfunction distance(v1::AbstractVector{T}, v2::AbstractVector{T})::Float64 where\r\n{T<:Real}\r\n    # v1: Cartisean coordinates of atom 1\r\n    # v2: Cartisean coordinates of atom 2\r\n    # -----\r\n    # returns: Euqlidian distance between atom 1 and atom 2\r\n\r\n    return sqrt(sum((v1 .- v2).^2))\r\nend\r\n\r\n# Define the smoothness penalty using mixed partial derivatives\r\nfunction smoothness_penalty(x1, x2, x3, tree, options, values)\r\n    # x1, x2, x3: variables of a tree\r\n    # tree: tree of a function\r\n    # options: options\r\n    # values: Dictionary of values of variables\r\n    # -----\r\n    # returns: sum of squares of partial derivatives\r\n\r\n    # Transform function from a tree to an experssion\r\n    f_str = string_tree(tree, options)\r\n\r\n    # Calculate penalty catching imaginary results, like: (-2) ^ 0.5\r\n    penalty = 0\r\n    try\r\n        f = eval(Meta.parse(f_str))\r\n\r\n        # Calculate Hessian of the function\r\n        H = Symbolics.hessian(f, [x1, x2, x3])\r\n\r\n        # Calcute final penalty as sum of squares of partial derivatives\r\n        penalty = sum(substitute(H, values) .^ 2).val\r\n    catch\r\n        penalty = 1e6\r\n    end\r\n\r\n    return penalty\r\nend\r\n\r\nfunction count_non_zero_values(atom::AbstractVector{T})::Int8 where {T<:Real}\r\n    # atom: Cartisean coordinates of the atom\r\n    # -----\r\n    # returns: Count of non zero coordinates\r\n\r\n    return count(!iszero, atom)\r\nend\r\n\r\nmutable struct atom_env\r\n    # n: number of atoms with similar coordinates\r\n    # coords: coordinates of an atom [x, y, z]\r\n\r\n    n::Float64\r\n    coords::Array{Float64,1}\r\nend\r\n\r\nfunction add_r(d, v::AbstractVector{T}, key::Float64) where {K<:String, V<:Real,\r\nT<:Real}\r\n    # d: dictionary (key: distance to the atom, value: number of\r\n    # atoms with the same distance)\r\n    # key: distance to the atom\r\n    # v: number of atoms with the same distance\r\n\r\n    d_keys = keys(d)\r\n    if key in keys(d)\r\n        d[key].n += 2 ^ count_non_zero_values(v)\r\n    else\r\n        d[key] = atom_env(2 ^ count_non_zero_values(v), v)\r\n    end\r\nend\r\n\r\nfunction get_distances(scale_factor::Union{T, Vector{T}}, r_cutoff::T = 7.0) where\r\n{T<:Float64}\r\n    # scale_factor: array of scaling coefficents for each axis ([x, y, z])\r\n    # r_cutoff: Cutoff distance\r\n    # -----\r\n    # returns: dictionary of distances\r\n\r\n    distance_dict = Dict()\r\n    atomic_coordinates = relative_coordinates\r\n    if scale_factor isa Number\r\n         scale_factor = [scale_factor, scale_factor, scale_factor]\r\n    end\r\n\r\n    for atom_i in atomic_coordinates\r\n        atom = atom_i .* scale_factor\r\n        distance_ij = distance([0., 0., 0.], atom)\r\n\r\n        if distance_ij != 0 && distance_ij <= r_cutoff\r\n            add_r(distance_dict, atom, distance_ij)\r\n        end\r\n    end\r\n    return distance_dict\r\nend\r\n\r\n# Creating array of distance dictionaries for each r in train_r\r\ndist_arr = []\r\nfor i in train_r\r\n    distances_i = get_distances(i, 7.0)\r\n    push!(dist_arr, distances_i)\r\nend\r\n\r\nfunction my_custom_objective(tree, dataset::Dataset{T,L}, options) where {T,L}\r\n    # tree: evaluation tree\r\n    # dataset: training dataset\r\n    # options: options\r\n\r\n    r_cutoff = 7.0\r\n    tree.degree != 2 && return L(Inf)\r\n\r\n    # Two body part\r\n    E_two_body = tree.l\r\n    E_two_body.degree != 2 && return L(Inf)\r\n\r\n    # Three body part\r\n    E_three_body = tree.r\r\n\r\n    has_x1_in_E_three_body = any(node -> node.degree==0 && node.constant==false && node.feature==1, E_three_body)\r\n    has_x2_in_E_three_body = any(node -> node.degree==0 && node.constant==false && node.feature==2, E_three_body)\r\n    has_x3_in_E_three_body = any(node -> node.degree==0 && node.constant==false && node.feature==3, E_three_body)\r\n\r\n    # Return huge loss if E_three_body doesn't have all three variables\r\n    if !has_x1_in_E_three_body || !has_x2_in_E_three_body || !has_x3_in_E_three_body\r\n        return L(Inf)\r\n    end\r\n\r\n    # E repulsion\r\n    tree_rep = E_two_body.l\r\n\r\n    # Return huge loss if tree_rep doesn't have at least one variable\r\n    has_variable = any(node -> node.degree==0 && node.constant==false, tree_rep)\r\n    if !has_variable\r\n         return L(Inf)\r\n    end\r\n\r\n    # E attraction, power\r\n    tree_att_p = E_two_body.r\r\n    tree_att_p.degree != 2 && return L(Inf)\r\n\r\n    # E attraction\r\n    tree_att = tree_att_p.l\r\n\r\n    # Return huge loss if tree_att doesn't have at least one variable\r\n    has_variable = any(node -> node.degree==0 && node.constant==false, tree_att)\r\n    if !has_variable\r\n         return L(Inf)\r\n    end\r\n\r\n    # power\r\n    power = tree_att_p.r\r\n    !any(node -> node.degree==0 && node.constant==true, power) && return L(Inf)\r\n\r\n    total_E_two_body = zeros(length(dist_arr))      # predicted total energy for two body part\r\n    total_E_three_body = zeros(length(dist_arr))    # predicted total energy for three body part\r\n    calculate_two_body_energy = true # flag to calculate two body part only 1 time\r\n    flag = true                      # flag to ensure tree evaluation\r\n    smooth_penalty_E_three_body = 0  # penalty for too high second derivatives\r\n\r\n    for (i, dict_i) in enumerate(dist_arr)\r\n        if calculate_two_body_energy\r\n            N = []\r\n            r = [[0, 0, 0]]\r\n            for i in keys(dict_i)\r\n                N = [N; dict_i[i].n]\r\n                r = vcat(r, [[i,i,i]])\r\n            end\r\n\r\n            r = T.(hcat(r[2:end, :]...))\r\n\r\n            eval_tree_att, finished_tree_att =  eval_tree_array(tree_att, r, options)\r\n            eval_tree_rep, finished_tree_rep =  eval_tree_array(tree_rep, r, options)\r\n            eval_power, finished_power =  eval_tree_array(power, r, options)\r\n\r\n            total_E_two_body[i] = sum(N .* eval_tree_rep) - sum(N .* (eval_tree_att.^ 2) .^ eval_power)\r\n            flag *= finished_tree_rep * finished_tree_att * finished_power\r\n            calculate_two_body_energy = false\r\n        end\r\n        triplets = []\r\n        N = []\r\n        for atom_2 in range(start=1, stop=length(dict_i), step=1)\r\n            N_x = []\r\n            distances_x = [[0, 0, 0]]\r\n            for atom_3 in range(start=atom_2, stop=length(dict_i), step=1)\r\n                atom_12 = Float64.(keys(dict_i))[atom_2]\r\n                atom_13 = Float64.(keys(dict_i))[atom_3]\r\n                atom_23 = distance(dict_i[atom_12].coords, dict_i[atom_13].coords)\r\n\r\n                if atom_23 != 0  && atom_12 <= r_cutoff && atom_13 <= r_cutoff && atom_23 <= r_cutoff\r\n                    distances_x = vcat(distances_x, [[atom_12, atom_13, atom_23]])\r\n                    N_x = [N_x; dict_i[atom_12].n * dict_i[atom_13].n]\r\n                    vals = Dict(x1 => atom_12, x2 => atom_13, x3 => atom_23)\r\n                    smooth_penalty_E_three_body += dict_i[atom_12].n * dict_i[atom_13].n * smoothness_penalty(x1, x2, x3, E_three_body, options, vals)\r\n                end\r\n            end # end of loop for atom_3\r\n\r\n            triplets = vcat(triplets, distances_x[2:end, :])\r\n            N = [N; N_x]\r\n\r\n        end # end of loop for atom_2\r\n\r\n        r = T.(hcat(triplets...))\r\n        if length(r) != 0\r\n            eval_E_three_body, finished_E_three_body = eval_tree_array(E_three_body, r, options)\r\n            total_E_three_body[i] = sum(N .* eval_E_three_body)\r\n            flag *= finished_E_three_body\r\n        end\r\n\r\n        calculate_two_body_energy = true\r\n    end # end of loop for dict_i\r\n\r\n    !flag && return L(Inf)\r\n\r\n    diffs = total_E_two_body .+ total_E_three_body .- dataset.y\r\n\r\n  return sum((diffs .^ 2)) / length(diffs ) + sum((diffs_dimer .^ 2) .* dimer_weights) / length(diffs_dimer) + smooth_penalty_E_three_body\r\nend\r\n```\r\n\r\n</details>\r\n\r\nI was following tips on your website, but still I had only 4 iterations in 22 hours. Is there way to speed it up? \r\nHere is my model: \r\n```python\r\nmodel = PySRRegressor(\r\n    model_selection=\"accuracy\",\r\n    niterations=30000,\r\n    maxsize=45,\r\n    populations=32*3,\r\n    ncyclesperiteration=4500,\r\n    binary_operators=[\"+\", \"-\",\"*\", \"/\", \"^\"],\r\n    constraints={'^': (4, 4), '*':(5, 5), '/':(2,2)},\r\n    nested_constraints={\"^\":{\"^\":1}},\r\n    full_objective=objective,\r\n    weight_optimize=0.001,\r\n    parsimony=0.0001,\r\n   #procs=32,\r\n   #cluster_manager=\"slurm\",\r\n    turbo = True,\r\n    use_frequency=False,\r\n    warm_start=True)\r\n```\r\nI have 23 training points and each cycle shouldn't be over 200 iterations.",
        "comments": {
          "nodes": [
            {
              "author": {
                "login": "MilesCranmer"
              },
              "body": "I would try to benchmark it with BenchmarkTools.jl (https://github.com/JuliaCI/BenchmarkTools.jl), in pure Julia, with SymbolicRegression.jl. You can do this with, for example:\r\n\r\n```julia\r\nusing SymbolicRegression\r\nusing BenchmarkTools\r\n\r\n# For generating random trees:\r\nusing SymbolicRegression.MutationFunctionsModule: gen_random_tree_fixed_size\r\n\r\n#= define objective function =#\r\n\r\n# Set up dataset and options\r\noptions = Options(binary_operators=[+, *, -, /], unary_operators=[cos, exp])\r\nnrows = 23\r\nnfeatures = 5\r\nT = Float64\r\ndataset = Dataset(\r\n    randn(T, nfeatures, nrows),\r\n    randn(T, nrows)\r\n)\r\n\r\n# Benchmark:\r\n@benchmark(\r\n    my_custom_objective(tree, $dataset, $options),\r\n    setup=(\r\n        tree_size = 30;\r\n        tree = gen_random_tree_fixed_size(tree_size, options, nfeatures, T)\r\n    )\r\n)\r\n```\r\n\r\nThis will give you a sense of how fast it is. For this dataset size and tree size, the baseline median performance for the default objective is around 1.5 microseconds on my machine:\r\n\r\n```julia\r\njulia> function default_objective(tree, dataset::Dataset{T,L}, options) where {T,L}\r\n           pred, completed = eval_tree_array(tree, dataset.X, options)\r\n           !completed && return L(Inf)\r\n           return sum(i -> (pred[i] - dataset.y[i])^2, eachindex(pred))\r\n       end\r\nmy_custom_objective (generic function with 1 method)\r\n\r\njulia> # Benchmark:\r\n       @benchmark(\r\n           default_objective(tree, $dataset, $options),\r\n           setup=(\r\n               tree_size = 30;\r\n               tree = gen_random_tree_fixed_size(tree_size, options, nfeatures, T)\r\n           )\r\n       )\r\nBenchmarkTools.Trial: 10000 samples with 262 evaluations.\r\n Range (min … max):  267.813 ns … 18.438 μs  ┊ GC (min … max): 0.00% … 90.47%\r\n Time  (median):       1.507 μs              ┊ GC (median):    0.00%\r\n Time  (mean ± σ):     1.632 μs ±  1.382 μs  ┊ GC (mean ± σ):  7.27% ±  7.91%\r\n\r\n                              ▁▂▄▅▅▆█▇▆▇▆▆▄▃▂▁\r\n  ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▄▅▆▇█████████████████▇▆▅▄▄▃▃▂▂▂▂▂▁▁▁ ▄\r\n  268 ns          Histogram: frequency by time         2.31 μs <\r\n\r\n Memory estimate: 960 bytes, allocs estimate: 4.\r\n```\r\n\r\n So if you are way larger than this (which is what it sounds like) that there is likely some bottleneck in the code. You can continue to use `@benchmark` to see what changes improve performance.\r\n\r\nThen my second comment is you should check out the page https://docs.julialang.org/en/v1/manual/performance-tips/ for common performance \"gotchas\" in Julia. In particular it seems like some of your code does not declare global variables as `const` such as `@variables x y z`, so this is probably one of the things slowing the code down (because Julia can't compile those variables into the function, as they might change in the future). So for example I would replace\r\n\r\n```julia\r\n@variables x y z\r\n```\r\n\r\nwith (you can see what `@variables` is doing with `@macroexpand` – it returns `[x, y, z]` with some definitions for each)\r\n\r\n```julia\r\nconst (x, y, z) = @variables x y z\r\n```\r\n\r\nThe Julia discourse is also super helpful for optimizing code snippets: https://discourse.julialang.org/.\r\n\r\nCheers,\r\nMiles",
              "createdAt": "2023-12-03T17:46:56Z"
            }
          ],
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0xMi0wM1QxNzo0Njo1NiswMDowMM4Adi4F"
          }
        }
      }
    }
  }
}