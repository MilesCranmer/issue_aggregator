{
  "data":
  {
    "repository":
    {
      "issue":
      {
        "number": 41,
        "title": "Running in float32 or float64",
        "body": "Hi Miles,\r\n\r\nIs there a way for this to run in `float64` or `float32` at run time, rather than modding the code?  I am writing some code that uses PySR, and the changes we made to make it work previously will likely not apply cleanly if we pull the changes to PySR up, in it's current state.  If not, do you have any advice on making this work?    ",
        "comments":
        {
          "nodes":
          [
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "Hi,\r\n\r\nGood question. For now, no, but I will add this change to PySR later this week. The backend fully supports it, just haven't enabled it in PySR yet.\r\n\r\nThe mod to the code right now is to replace float32 with float64 and Float32 with Float64 in these lines: https://github.com/MilesCranmer/PySR/blob/2309acf16b025b50b423514d2e6557c0ca1bbca9/pysr/sr.py#L445-L454\r\n\r\nCheers,\r\nMiles",
              "createdAt": "2021-04-01T05:04:11Z"
            },
            {
              "author":
              {
                "login": "Sean-Reilly"
              },
              "body": "Thank you Much for your quick Response!",
              "createdAt": "2021-04-05T20:51:18Z"
            },
            {
              "author":
              {
                "login": "Sean-Reilly"
              },
              "body": "Hello Miles,\r\n\r\nWanted to check in on this issue since we're still having some trouble with it.  To begin I've tried making extensive changes, including the one you mentioned, to get the precision to be where I want it to be.  It was quite a bit more extensive change than just changing that section you mentioned, with there being many instances of `Float32` in the Julia backend as well, and have been trying to enable selecting `Float32` or `Float64` in the code at runtime rather than having to find all the instances and change them manually.  \r\n\r\nTo this end I've hit a wall, with the dependency the way it is I'm having trouble calling in the selected precision into `ProgramConstants.jl`, since the `precision` variable is set through the `hyperparams.jl` which are generated by `sr.py`.  Originally I had it just adjust the values in `Options.jl`, which works fine, but isn't achieving the true `Float64` precision without changing every instance of `Float32` in both `Equation.jl` and `ProgramConstants.jl`, and I can't bring in the `precision` variable into those files because of the dependency tree.  \r\n\r\nMy goal is to enable the selection of floating point precision as a parameter of `sr.py` that can be set when running the code, so that you can maintain the precision others need while also allowing for higher precision calculations, eventually to be included hopefully as a pull for the repo.  Do you have any further insight into this?  Are there any changes you would be willing to make to make this easier?  ",
              "createdAt": "2021-06-18T19:56:33Z"
            },
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "Hey,\r\nsorry for forgetting to do this. I'm working on it now before I forget again. Just pushed to `master` branch. The `precision=32` parameter to `pysr` now controls the precision (\\in [16, 32, 64])\r\n\r\nThe constants in each equation are hardcoded to 32-bit precision. Just so I understand, why do you need 64-bit precision in the constants? I wouldn't expect it to actually reach that level of precision in optimizing them (since it simultaneously is searching over equations, it's hard to optimize both); even Float32 is probably too much for the constants and Float16 might be good enough. To reach 64-bit level of precision, I would probably do post-processing and re-optimize the constants in the equation with a normal optimizer. (both `export_torch_format` and `export_jax_format` give you a representation with tunable parameters).\r\n\r\nCheers,\r\nMiles",
              "createdAt": "2021-06-19T18:35:21Z"
            },
            {
              "author":
              {
                "login": "Sean-Reilly"
              },
              "body": "Hi again,\r\n\r\nThank you so much for updating to support this, if I have any other issues/concerns regarding this I'll post them here.  To answer your question, I'm trying to reproduce a theoretical model for a mathematical representation of molecular dynamics, so it has to be incredibly high precision to pass the MSE 'bar' we're looking for.  I'll pass along your suggestions to my PI and we'll see how feasible that is for the project.  \r\n\r\nThanks again!",
              "createdAt": "2021-06-22T20:48:42Z"
            },
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "Awesome! Sounds like a really interesting project. \r\nAnother thing you might consider trying is: after learning an equation to approximately fit your system, if itâ€™s not enough precision, then learn the remaining error with a much more flexible model like a NNet. That way, the majority of your model is analytic, but it gets to a high level of precision using a NNet to pick up the slack. ",
              "createdAt": "2021-06-23T15:11:10Z"
            },
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "Closed with the `precision` argument in PySR, although constants are still fixed in Float32. I think arbitrary complexity could be added for the constants fairly easily, but that is not yet done. If needed, one can change `CONST_TYPE` in the SymbolicRegression.jl source code, and specify the custom source with `julia_project` in pysr.",
              "createdAt": "2022-05-20T15:05:31Z"
            },
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "Hi @Sean-Reilly,\r\nIf you're still interested in this, I have a draft PR here: https://github.com/MilesCranmer/SymbolicRegression.jl/pull/119 which adds support for arbitrary types of constants in the expressions.\r\nCheers,\r\nMiles",
              "createdAt": "2022-08-30T17:57:56Z"
            }
          ],
          "pageInfo":
          {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpHOSW6iqg=="
          }
        }
      }
    }
  }
}