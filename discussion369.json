{
  "data":
  {
    "repository":
    {
      "discussion":
      {
        "number": 369,
        "title": "Reduce the running-time with custom objective",
        "body": "Hey,\r\n\r\nHere is my custom objective function:\r\n```\r\nobjective = \"\"\"\r\nimport Pkg\r\nPkg.add(\"Roots\")\r\nPkg.add(\"ForwardDiff\")\r\nPkg.add(\"QuadGK\")\r\n\r\nusing Roots\r\nusing ForwardDiff\r\nusing QuadGK\r\nfunction eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\r\n    MAX_THRESHOLD_ALLOWED = 10000\r\n    \r\n\r\n    prediction, flag = eval_tree_array(tree, dataset.X, options)\r\n    !flag && return L(Inf)\r\n    \r\n    # NOTE: reshape([x], 1, 1) should give same dimensions\r\n    # as reshape([x;], 1, 1).\r\n    f(x) = 1 / ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])\r\n    \r\n    fp(x) = ( -1 / ((eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1])^2 * (eval_diff_tree_array(tree, reshape([Float32(x)], 1, 1), options, 1))[2][1])\r\n    \r\n    found_a_root = true\r\n    roots = -3\r\n    try\r\n        global roots = find_zero((f, fp), 0.3, Roots.Newton(), maxevals=100)\r\n    catch\r\n        global found_a_root = false\r\n        # This means that \"Convergence Failed\"\r\n    end\r\n    \r\n    # If the function f(x) represented by the tree \"tree\"\r\n    # has a value f(x) close to positive infinity or negative infinity\r\n    # when x is in [0, 1], then return L(Inf).\r\n    if found_a_root\r\n        if (roots >= 0 && roots <= 1)\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    global integral = 1\r\n    err = -3\r\n    try\r\n        my_tuple = quadgk(x -> (eval_tree_array(tree, reshape([Float32(x)], 1, 1), options))[1][1], 0, 1, rtol=1e-8)\r\n        global integral = my_tuple[1]\r\n        global err = my_tuple[2]\r\n    catch\r\n        return L(Inf)\r\n    end\r\n    \r\n    \r\n    \r\n    num_points = length(prediction)\r\n    \r\n    for i in 1:num_points\r\n        cur_expr_val = prediction[i]\r\n        if (cur_expr_val > MAX_THRESHOLD_ALLOWED)\r\n            return L(Inf)\r\n        end\r\n        if (cur_expr_val < 0)\r\n            return L(Inf)\r\n        end\r\n    end\r\n    \r\n    norm_constant = integral\r\n    \r\n    actual_probs = (prediction) / norm_constant\r\n    \r\n    final_loss = exp(-1 * sum(log.(actual_probs)) / (length(prediction)))\r\n    return final_loss\r\nend\r\n\"\"\"\r\n\r\n```\r\n\r\nHere is how I defined my model:\r\n```\r\nmodel = PySRRegressor(\r\n    niterations=2,  # < Increase me for better results\r\n    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\r\n    full_objective=objective,\r\n    enable_autodiff=True\r\n)\r\n```\r\n\r\nFirst, I verified that this objective function works correctly using the julia helper object to evaluate this objective function on two example trees, given one example dataset X.\r\n```\r\njl = pysr.julia_helpers.init_julia(julia_kwargs={\"threads\": 8, \"optimize\": 3})\r\n```\r\n\r\nThen, I ran model.fit(X, y):\r\n\r\nHowever, it's been over 25 minutes and the only thing printed out to my terminal is this (which was printed like 1 or 2 minutes after I ran \"model.fit(X, y)\"):\r\n```\r\nCompiling Julia backend...\r\n\r\n/opt/anaconda3/envs/pysr_env/lib/python3.10/site-packages/pysr/julia_helpers.py:208: UserWarning: Your system's Python library is static (e.g., conda), so precompilation will be turned off. For a dynamic library, try using `pyenv` and installing with `--enable-shared`: https://github.com/pyenv/pyenv/blob/master/plugins/python-build/README.md#building-with---enable-shared.\r\n  warnings.warn(\r\n   Resolving package versions...\r\n  No Changes to `~/.julia/environments/pysr-0.14.3/Project.toml`\r\n  No Changes to `~/.julia/environments/pysr-0.14.3/Manifest.toml`\r\n   Resolving package versions...\r\n  No Changes to `~/.julia/environments/pysr-0.14.3/Project.toml`\r\n  No Changes to `~/.julia/environments/pysr-0.14.3/Manifest.toml`\r\n   Resolving package versions...\r\n  No Changes to `~/.julia/environments/pysr-0.14.3/Project.toml`\r\n  No Changes to `~/.julia/environments/pysr-0.14.3/Manifest.toml`\r\nStarted!\r\n```\r\n\r\nOn my past runnings of model.fit(X, y), it did finish within 5 minutes or less.\r\nI think the only thing I changed was that I changed \"integral = 1\" to \"global integral = 1\", which means that now, I am actually updating the value of \"integral\" (and \"norm_constant\") correctly.\r\nBefore, the value of \"integral\" (and \"norm_constant\") was always 1, because I was not actually updating the value of integral.\r\n\r\nSo, my main question is:\r\n1. What parameters in the construction of the PySRRegressor model can I change to reduce runtime of model.fit(X, y)?\r\n2. When does model.fit(X, y) know when to stop? Is it when the loss of the guessed function becomes less than a arbitrary small number like less than 0.02 ?\r\n3. Is there some useful metric that I can force model.fit(X, y) to print out while it is running, so that I can know how much more time it is going to take to finish running?",
        "comments":
        {
          "nodes":
          [
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "1. In general it sounds the objective is slowing things down, rather than anything in the hyperparameters. i.e., you should try to speed up the objective, or, if not possible, use more compute. To speed up the objective I recommend reading https://docs.julialang.org/en/v1/manual/performance-tips/.\r\n    - I also note your `rtol` is super high in quadgk. I would not expect to reach that number if you still want it to be fast at evaluation. I would try to limit the number of steps and return a big loss if it can't reach a moderate rtol.\r\n3. By default this is determined by `niterations`, which is an outer loop in the evolutionary code. It does not quit, regardless of the loss, until it reaches those number of iterations. For other stopping methods you can see the stopping criteria parameters: https://astroautomata.com/PySR/api/#stopping-criteria\r\n4. It should do this by default – it displays a progress bar when you are running with a time estimate for completion. Not sure why this isn't visible to you – maybe are you running in Jupyter? Unfortunately the progress bar is not yet working in Jupyter, so it instead prints updated expressions every 5 seconds (once the first iteration has passed). Maybe try running in IPython, with `progress=true` to see the progress bar?\r\n\r\nSince you are doing a lot of editing Julia code, it might even make sense to work directly in Julia. At least until you speed things up and can move back to Python.\r\n\r\nAs it happens I just released an improved API for the backend which makes it much easier to use via Julia sklearn equivalent: [MLJ](https://github.com/alan-turing-institute/MLJ.jl). More info here: https://github.com/MilesCranmer/SymbolicRegression.jl/#mlj-interface\r\n\r\nExample:\r\n```julia\r\nimport SymbolicRegression: SRRegressor\r\nimport MLJ: machine, fit!, predict, report\r\n\r\ndata = (x=randn(100), y=randn(100))\r\ny = @. 2 * cos(data.x * 12) + data.y ^ 2 - 2\r\n# This also works: data = randn(100, 2)\r\n\r\nmodel = SRRegressor(\r\n    niterations=100,\r\n    binary_operators=[+, *, /, -],\r\n    unary_operators=[exp, cos],\r\n    parallelism=:multithreading,\r\n)\r\n\r\nmach = machine(model, data, y)\r\n\r\n# Train model\r\nfit!(mach)\r\n\r\n# View expressions\r\nreport(mach)\r\n```\r\n\r\nJust note that the `full_objective` in `PySRRegressor` is called `loss_function` in `SRRegressor` (sorry; will eventually make this consistent).",
              "createdAt": "2023-07-05T18:22:09Z"
            }
          ],
          "pageInfo":
          {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wNy0wNVQxOToyMjowOSswMTowMM4AYSOG"
          }
        }
      }
    }
  }
}