{
  "data":
  {
    "repository":
    {
      "discussion":
      {
        "number": 540,
        "title": "Can PySR handle recursive functions?",
        "body": "I am very impressed by the capabilities of PySR. I am just starting with it and it appears very useful to me!\r\nI am planning to use it for deriving formulas from experimental data in the Physics domain amongst others.\r\n\r\nBut I have a question.\r\nCan PySR handle recursive functions?\r\nSuppose I have dataset that was generated by using a recursive function (such as the data representing the famous Mandelbrot set). Is PySR able to output as a solution a recursive function?\r\n\r\nIf this is not possible directly, how could I formulate the problem so that PySR could solve it, perhaps even partially or approximately?\r\n\r\nThe main goal is to show that the Mandelbrot set, although it looks infinitely complex to a human observer because it shows infinitely new and complex structure if you zoom into it infinitely wide, in reality it’s structure has a very low (algoritmic) complexity because of the fact that it’s generating (recursive) function is so simple: Zn+1 = Zn^2 + C.\r\nI hope to be able to prove that by using PySR  finding a relatively small formula that can (approximately ?) generate data that can represent the Mandelbrot set. But the real generating formula is recursive in nature, so I am stuck for the moment because I am not sure on 2 things:\r\n\r\n1) how exactly to present this problem to as input to PySR (how to structure the dataset that represents the output of the generating recursive function so that can be effectively used by PySR to solve the problem, and what other inputs and parameters to set)\r\n\r\n2) Can PySR output a recursive function as a solution? Perhaps in a indirect form or format?\r\n\r\nI hope someone can help me overcome this hurdle! \r\n\r\n\r\n",
        "comments":
        {
          "nodes":
          [
            {
              "author":
              {
                "login": "MilesCranmer"
              },
              "body": "Hi @jabogithub,\r\n\r\nReally interesting question! I spent some time on this out of interest and managed to get it to work. This lets you rediscover the Mandelbrot set recursive relation from random samples of the Mandelbrot set.\r\n\r\n<details>\r\n<summary>\r\nFirst, here's a Julia version:\r\n</summary>\r\n\r\n```julia\r\nusing SymbolicRegression\r\nusing MLJBase: machine, fit!, predict\r\nusing Random: Random\r\n\r\n# Mandelbrot set\r\nfunction in_mandelbrot(c; max_iteration = 1000)\r\n    z = zero(c)\r\n    iteration = 0\r\n    while abs(z) <= 2.0 && iteration < max_iteration\r\n        z = z^2 + c\r\n        iteration += 1\r\n    end\r\n    return (abs(z) <= 2.0 && iteration == max_iteration)\r\nend\r\n\r\n\"\"\"\r\n    my_loss(tree, dataset, options)\r\n\r\nCustom loss function for sets defined by recursive functions.\r\n\r\n- `tree`: The expression tree defining the recursive function. The first feature\r\n    will be used to store the output of the function.\r\n- `dataset`: The dataset to evaluate the function on, of shape (n_features, n_samples).\r\n- `options`: The options for the symbolic regression model.\r\n\"\"\"\r\nfunction my_loss(tree, dataset::Dataset{T,L}, options::Options) where {T,L}\r\n    num_rows = size(dataset.X, 2)\r\n    max_iteration = 1000\r\n    boundary = 2.0\r\n\r\n    X = copy(dataset.X)  # Copy dataset for modification\r\n    for i in axes(X, 2)\r\n        X[1, i] = zero(T)  # Initialize z to 0.0 + 0.0im\r\n    end\r\n    in_bounds = [true for _ = 1:num_rows]\r\n\r\n    for iteration in 1:max_iteration\r\n        # Only work on subset of data that is in-bounds\r\n        sliced_X = @view X[:, in_bounds]\r\n        sliced_in_bounds = @view in_bounds[in_bounds]\r\n        # (The @view macro is used to avoid copying the data)\r\n\r\n        sliced_out, completed = eval_tree_array(tree, sliced_X, options)\r\n        if !completed\r\n            # Penalty if incomplete evaluation due to NaNs,\r\n            #  but reduce penalty if it got further along\r\n            return L(1000000 * (1 + max_iteration - iteration))\r\n        end\r\n\r\n        # Update z\r\n        for i in axes(sliced_X, 2)\r\n            sliced_in_bounds[i] &= abs(sliced_out[i]) <= boundary\r\n            if sliced_in_bounds[i]\r\n                # Only update if still in bounds\r\n                sliced_X[1, i] = sliced_out[i]\r\n            end\r\n        end\r\n\r\n        if !any(in_bounds)\r\n            break\r\n        end\r\n    end\r\n    return L(sum(i -> abs(dataset.y[i] - in_bounds[i]), 1:num_rows) / num_rows)\r\nend\r\n\r\nN = 300\r\n\r\nz = [zero(ComplexF32) for _ = 1:N]\r\nc = let RNG = Random.MersenneTwister(0)\r\n    [rand(RNG, ComplexF32) * 2 - 1 for _ = 1:N]\r\nend\r\n\r\nX = (; z, c)\r\ny = ComplexF32.(in_mandelbrot.(c))\r\n\r\nprintln(\"Proportion of dataset in Mandelbrot set: \", Float64(sum(y) / N))\r\n\r\nmodel = SRRegressor(\r\n    binary_operators=(+, -, *),\r\n    unary_operators=(cos,),\r\n    niterations=100,\r\n    loss_function=my_loss,\r\n)\r\n\r\nmach = machine(model, X, y)\r\nfit!(mach)\r\n```\r\n\r\n</details>\r\n\r\nWe can see that this recovers the correct Mandelbrot set relation!\r\n![Screenshot 2024-02-03 at 17 07 02](https://github.com/MilesCranmer/PySR/assets/7593028/907d474c-1ffa-4f50-8a97-7d8a5553aded)\r\n\r\nNote that this code is written to be pedagogical so does not use all the Julia tricks out there. Also, it is currently quite expensive to run for some reason. Perhaps that is unavoidable or maybe there is a better way to structure things. Disabling some things like constant optimization, if not needed, might speed things up.\r\n\r\nFinally, note that the order of features input matters – it is assumed in the code that the first feature is `z` which gets initialized as 0 and is used to store the output at each step.\r\n\r\nThe key part here is the definition of `my_loss` which defines a loss that performs recursive evaluation up to 1000 iterations (to estimate if a value is bounded). See, e.g., https://astroautomata.com/PySR/examples/#9-custom-objectives for some discussion about custom losses. There are also several other discussion threads which might be enlightening for the kinds of things you can do here (note that differentiability is not required).\r\n\r\nThe PySR equivalent would be as follows:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom pysr import PySRRegressor\r\n\r\ndef in_mandelbrot(c, max_iteration=1000):\r\n    z = 0.0 + 0.0j\r\n    iteration = 0\r\n    while abs(z) <= 2.0 and iteration < max_iteration:\r\n        z = z**2 + c\r\n        iteration += 1\r\n    return abs(z) <= 2.0 and iteration == max_iteration\r\n\r\nfull_objective = \"\"\"\r\n    function my_loss(tree, dataset::Dataset{T,L}, options::Options) where {T,L}\r\n        num_rows = size(dataset.X, 2)\r\n        max_iteration = 1000\r\n        boundary = 2.0\r\n\r\n        X = copy(dataset.X)  # Copy dataset for modification\r\n        for i in axes(X, 2)\r\n            X[1, i] = zero(T)  # Initialize z to 0.0 + 0.0im\r\n        end\r\n        in_bounds = [true for _ = 1:num_rows]\r\n\r\n        for iteration = 1:max_iteration\r\n            # Only work on subset of data that is in-bounds\r\n            sliced_X = @view X[:, in_bounds]\r\n            sliced_in_bounds = @view in_bounds[in_bounds]\r\n            # (The @view macro is used to avoid copying the data)\r\n\r\n            sliced_out, completed = eval_tree_array(tree, sliced_X, options)\r\n            if !completed\r\n                # Penalty if incomplete evaluation due to NaNs,\r\n                #  but reduce penalty if it got further along\r\n                return L(1000000 * (1 + max_iteration - iteration))\r\n            end\r\n\r\n            # Update z\r\n            for i in axes(sliced_X, 2)\r\n                sliced_in_bounds[i] &= abs(sliced_out[i]) <= boundary\r\n                if sliced_in_bounds[i]\r\n                    # Only update if still in bounds\r\n                    sliced_X[1, i] = sliced_out[i]\r\n                end\r\n            end\r\n\r\n            if !any(in_bounds)\r\n                break\r\n            end\r\n        end\r\n        return L(sum(i -> abs(dataset.y[i] - in_bounds[i]), 1:num_rows) / num_rows)\r\n    end\r\n\"\"\"\r\n\r\nN = 300\r\n\r\nrstate = np.random.RandomState(0)\r\n\r\nz = np.zeros(N, dtype=np.complex128)\r\nc = rstate.uniform(-1, 1, N) + 1j * rstate.uniform(-1, 1, N)\r\n\r\nX = pd.DataFrame({\"z\": z, \"c\": c})\r\ny = (np.vectorize(in_mandelbrot)(c)).astype(np.complex128)\r\n\r\n\r\nmodel = PySRRegressor(\r\n    binary_operators=[\"+\", \"-\", \"*\"],\r\n    unary_operators=[\"cos\"],\r\n    niterations=100,\r\n    full_objective=full_objective,\r\n)\r\n\r\nmodel.fit(X, y)\r\n```\r\n\r\nHere's part way through the search:\r\n\r\n![Screenshot 2024-02-03 at 17 19 45](https://github.com/MilesCranmer/PySR/assets/7593028/e0b5fbf6-0ddf-4318-a90f-a3615f640f59)\r\n\r\na little bit farther (with the correct relation now showing up):\r\n\r\n![Screenshot 2024-02-03 at 17 20 43](https://github.com/MilesCranmer/PySR/assets/7593028/a9862ae5-d42a-4e39-83b8-96f0e854faac)\r\n\r\nand here it is once it finds it:\r\n\r\n![Screenshot 2024-02-03 at 17 21 24](https://github.com/MilesCranmer/PySR/assets/7593028/1a67c7fc-1a29-4ed7-9301-a0575235f1a9)\r\n\r\n\r\nCheers,\r\nMiles\r\n",
              "createdAt": "2024-02-03T17:23:24Z"
            },
            {
              "author":
              {
                "login": "jbdatascience"
              },
              "body": "Thank you for your answer ! Amazing work, which goes to show the power of Symbolic Regression in general, but more specific the power of PySR . \r\n\r\nAlso this proves that PySR is able to uncover the simplicity behind seemingly overly complex data! (Although we know beforehand the ground truth in this use case: the Mandelbrot data generating recursive function, but it had to be seen if PySR has the power to uncover that, which it has according to your effort!). That was my main goal: getting to know if PySR is up to this challenging problem. \r\n\r\nIt strengthens my trust that PySR in principle can solve difficult problems, just from raw (experimental) data alone.\r\n\r\nI still have to go into your code, because it is not so straightforward as it seems, to me as totally inexperienced PySR practitioner at least. In particular the construction of the loss function looks not so trivial at first sight! \r\n\r\nI will study your solution and of course try it out next week! Very exciting I must say. Perhaps I come back to you here in this space …",
              "createdAt": "2024-02-10T21:53:09Z"
            }
          ],
          "pageInfo":
          {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wMi0xMFQyMTo1MzowOSswMDowMM4AgKE7"
          }
        }
      }
    }
  }
}